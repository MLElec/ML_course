{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "##Ridge regression \n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scripts.implementations as lib  # Add personal library\n",
    "import scripts.proj1_helpers as helper  # Add personal library\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DATA_TRAIN = os.path.join(DATA_FOLDER, 'train.csv')\n",
    "DATA_TEST = os.path.join(DATA_FOLDER, 'test.csv')\n",
    "\n",
    "y, x, ids, header = helper.load_csv_data(DATA_TRAIN)\n",
    "y_train, x_train,  y_validation, x_validation = lib.sep_valid_train_data(x,y, 0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[x_train == -999] = np.nan\n",
    "x_validation[x_validation == -999] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 - DER_mass_MMC has range: [9.0440, 1192.0260]\n",
      "Feature 2 - DER_mass_transverse_met_lep has range: [0.0000, 595.8190]\n",
      "Feature 3 - DER_mass_vis has range: [6.4620, 1329.9130]\n",
      "Feature 4 - DER_pt_h has range: [0.0000, 1053.8070]\n",
      "Feature 5 - DER_deltaeta_jet_jet has range: [0.0000, 8.5030]\n",
      "Feature 6 - DER_mass_jet_jet has range: [13.6020, 4974.9790]\n",
      "Feature 7 - DER_prodeta_jet_jet has range: [-18.0660, 16.6900]\n",
      "Feature 8 - DER_deltar_tau_lep has range: [0.2080, 5.6840]\n",
      "Feature 9 - DER_pt_tot has range: [0.0000, 513.6590]\n",
      "Feature 10 - DER_sum_pt has range: [46.1040, 1852.4620]\n",
      "Feature 11 - DER_pt_ratio_lep_tau has range: [0.0470, 19.7730]\n",
      "Feature 12 - DER_met_phi_centrality has range: [-1.4140, 1.4140]\n",
      "Feature 13 - DER_lep_eta_centrality has range: [0.0000, 1.0000]\n",
      "Feature 14 - PRI_tau_pt has range: [20.0000, 622.8620]\n",
      "Feature 15 - PRI_tau_eta has range: [-2.4990, 2.4970]\n",
      "Feature 16 - PRI_tau_phi has range: [-3.1420, 3.1420]\n",
      "Feature 17 - PRI_lep_pt has range: [26.0000, 461.8960]\n",
      "Feature 18 - PRI_lep_eta has range: [-2.5050, 2.5020]\n",
      "Feature 19 - PRI_lep_phi has range: [-3.1420, 3.1420]\n",
      "Feature 20 - PRI_met has range: [0.1090, 951.3630]\n",
      "Feature 21 - PRI_met_phi has range: [-3.1420, 3.1420]\n",
      "Feature 22 - PRI_met_sumet has range: [13.6780, 2003.9760]\n",
      "Feature 23 - PRI_jet_num has range: [0.0000, 3.0000]\n",
      "Feature 24 - PRI_jet_leading_pt has range: [30.0000, 1120.5730]\n",
      "Feature 25 - PRI_jet_leading_eta has range: [-4.4990, 4.4990]\n",
      "Feature 26 - PRI_jet_leading_phi has range: [-3.1420, 3.1410]\n",
      "Feature 27 - PRI_jet_subleading_pt has range: [30.0010, 721.4560]\n",
      "Feature 28 - PRI_jet_subleading_eta has range: [-4.5000, 4.5000]\n",
      "Feature 29 - PRI_jet_subleading_phi has range: [-3.1420, 3.1420]\n",
      "Feature 30 - PRI_jet_all_pt has range: [0.0000, 1633.4330]\n"
     ]
    }
   ],
   "source": [
    "for i, feature in enumerate(x_train.T):\n",
    "    print('Feature {} - {} has range: [{:.4f}, {:.4f}]'.format(\n",
    "        i+1, header[i], np.nanmin(feature), np.nanmax(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"ridge_regression\")\n",
    "    \n",
    "def test_ridge_regression(x, y, x_val, y_val, degrees, lambdas):\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_degree = 0\n",
    "    best_lambda = 0\n",
    "    best_rmse_tr = []\n",
    "    best_rmse_te = []\n",
    "    best_weights = []\n",
    "    for degree in degrees:\n",
    "        degree = int(degree)\n",
    "        #lambdas = np.logspace(-7, 2, 20)\n",
    "\n",
    "        # Split sets\n",
    "        #x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "        # Get ploynomial\n",
    "        phi_train = lib.build_poly(x, degree)\n",
    "        phi_test = lib.build_poly(x_val, degree)\n",
    "\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        update_rmse = False\n",
    "\n",
    "        for ind, lambda_ in enumerate(lambdas):\n",
    "\n",
    "            mse_tr, weights = lib.ridge_regression(y, phi_train, lambda_)\n",
    "            mse_te = lib.compute_loss(y_val, phi_test.dot(weights))\n",
    "            rmse_tr.append(np.sqrt(2*mse_tr))\n",
    "            rmse_te.append(np.sqrt(2*mse_te))\n",
    "\n",
    "            print(\"degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "                    d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "            print('train acc : ', lib.accuracy(y, phi_train.dot(weights)))\n",
    "            val_acc = lib.accuracy(y_val, phi_test.dot(weights))\n",
    "            print('validation acc : ', val_acc)\n",
    "\n",
    "            if(val_acc > best_acc):\n",
    "                best_acc = val_acc\n",
    "                best_degree = degree\n",
    "                best_lambda = lambda_\n",
    "                best_weights = weights\n",
    "                update_rmse = True\n",
    "        \n",
    "        if(update_rmse):\n",
    "            best_rmse_tr = rmse_tr\n",
    "            best_rmse_te = rmse_te\n",
    "\n",
    "        # Plot the best obtained results\n",
    "    plot_train_test(best_rmse_tr, best_rmse_te, lambdas, best_degree)\n",
    "\n",
    "    print('Best params for Ridge regression : degree = ',best_degree, ', lambda = ',best_lambda,', accuracy = ', best_acc)\n",
    "    \n",
    "    return best_weights, best_degree, best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (200000, 35)\n",
      "\n",
      "Std: [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "\n",
      "Std: [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "from scripts.ml import augmented_feat_angle\n",
    "\n",
    "id_angle_feat = np.array([15, 18, 20, 25, 28])\n",
    "id_left = [ i for i in range(x_train.shape[1]) if i not in id_angle_feat]\n",
    "\n",
    "# Augment features\n",
    "x_aug = augmented_feat_angle(x_train, id_angle_feat)\n",
    "x_aug = np.concatenate((x_train[:, id_left], x_aug), axis=1)\n",
    "x_aug_val = augmented_feat_angle(x_validation, id_angle_feat)\n",
    "x_aug_val = np.concatenate((x_validation[:, id_left], x_aug_val), axis=1)\n",
    "print('Shape:', x_aug.shape)\n",
    "\n",
    "# normalize features\n",
    "x_no_nan = x_aug.copy()\n",
    "x_no_nan = (x_no_nan - np.nanmean(x_no_nan, axis=0)) # /np.nanstd(x_no_nan, axis=0)\n",
    "x_no_nan = np.nan_to_num(x_no_nan)\n",
    "x_no_nan /= np.nanstd(x_no_nan, axis=0)\n",
    "print('\\nStd:', np.std(x_no_nan, axis=0))\n",
    "\n",
    "# normalize features\n",
    "x_no_nan_val = x_aug_val.copy()\n",
    "x_no_nan_val = (x_no_nan_val - np.nanmean(x_no_nan_val, axis=0)) #/np.nanstd(x_no_nan_val, axis=0)\n",
    "x_no_nan_val = np.nan_to_num(x_no_nan_val)\n",
    "x_no_nan_val /= np.nanstd(x_no_nan_val, axis=0)\n",
    "\n",
    "print('\\nStd:', np.std(x_no_nan_val, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51572439.979\n",
      "train acc :  0.815455\n",
      "validation acc :  0.48628\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51572834.409\n",
      "train acc :  0.815405\n",
      "validation acc :  0.52002\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51573952.782\n",
      "train acc :  0.81539\n",
      "validation acc :  0.58068\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51577021.539\n",
      "train acc :  0.81543\n",
      "validation acc :  0.6405\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51584743.166\n",
      "train acc :  0.815405\n",
      "validation acc :  0.8096\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51600068.689\n",
      "train acc :  0.815435\n",
      "validation acc :  0.81344\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51616941.205\n",
      "train acc :  0.81542\n",
      "validation acc :  0.81512\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51613923.388\n",
      "train acc :  0.815365\n",
      "validation acc :  0.81486\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51578486.809\n",
      "train acc :  0.815365\n",
      "validation acc :  0.81502\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51498189.501\n",
      "train acc :  0.815305\n",
      "validation acc :  0.81492\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=51243488.744\n",
      "train acc :  0.815245\n",
      "validation acc :  0.81496\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=50321829.261\n",
      "train acc :  0.815145\n",
      "validation acc :  0.81526\n",
      "degree=9, lambda=0.000, Training RMSE=0.746, Testing RMSE=47556477.051\n",
      "train acc :  0.814815\n",
      "validation acc :  0.8154\n",
      "degree=9, lambda=0.000, Training RMSE=0.746, Testing RMSE=40999662.672\n",
      "train acc :  0.81427\n",
      "validation acc :  0.81442\n",
      "degree=9, lambda=0.000, Training RMSE=0.749, Testing RMSE=29229408.372\n",
      "train acc :  0.81277\n",
      "validation acc :  0.81296\n",
      "degree=9, lambda=0.000, Training RMSE=0.755, Testing RMSE=14225277.676\n",
      "train acc :  0.80763\n",
      "validation acc :  0.8091\n",
      "degree=9, lambda=0.000, Training RMSE=0.766, Testing RMSE=1807865.893\n",
      "train acc :  0.79669\n",
      "validation acc :  0.7987\n",
      "degree=9, lambda=0.001, Training RMSE=0.783, Testing RMSE=957499.627\n",
      "train acc :  0.78076\n",
      "validation acc :  0.78354\n",
      "degree=9, lambda=0.001, Training RMSE=0.805, Testing RMSE=12588674.469\n",
      "train acc :  0.763095\n",
      "validation acc :  0.768\n",
      "degree=9, lambda=0.002, Training RMSE=0.829, Testing RMSE=23404921.807\n",
      "train acc :  0.74675\n",
      "validation acc :  0.75204\n",
      "degree=9, lambda=0.004, Training RMSE=0.851, Testing RMSE=24992174.298\n",
      "train acc :  0.732335\n",
      "validation acc :  0.7364\n",
      "degree=9, lambda=0.007, Training RMSE=0.870, Testing RMSE=19108266.501\n",
      "train acc :  0.720685\n",
      "validation acc :  0.72586\n",
      "degree=9, lambda=0.012, Training RMSE=0.889, Testing RMSE=11235912.703\n",
      "train acc :  0.713265\n",
      "validation acc :  0.71894\n",
      "degree=9, lambda=0.020, Training RMSE=0.907, Testing RMSE=5163647.546\n",
      "train acc :  0.708955\n",
      "validation acc :  0.71496\n",
      "degree=9, lambda=0.035, Training RMSE=0.921, Testing RMSE=1548508.205\n",
      "train acc :  0.706065\n",
      "validation acc :  0.7117\n",
      "degree=9, lambda=0.059, Training RMSE=0.934, Testing RMSE=366019.642\n",
      "train acc :  0.70479\n",
      "validation acc :  0.71056\n",
      "degree=9, lambda=0.100, Training RMSE=0.948, Testing RMSE=831242.102\n",
      "train acc :  0.702875\n",
      "validation acc :  0.70864\n",
      "degree=9, lambda=0.170, Training RMSE=0.959, Testing RMSE=35350.551\n",
      "train acc :  0.70067\n",
      "validation acc :  0.70616\n",
      "degree=9, lambda=0.289, Training RMSE=0.968, Testing RMSE=1758516.998\n",
      "train acc :  0.698435\n",
      "validation acc :  0.7037\n",
      "degree=9, lambda=0.492, Training RMSE=0.976, Testing RMSE=3597977.556\n",
      "train acc :  0.69781\n",
      "validation acc :  0.70316\n",
      "degree=9, lambda=0.838, Training RMSE=0.983, Testing RMSE=4584568.269\n",
      "train acc :  0.698255\n",
      "validation acc :  0.7025\n",
      "degree=9, lambda=1.425, Training RMSE=0.988, Testing RMSE=4002208.952\n",
      "train acc :  0.697925\n",
      "validation acc :  0.7029\n",
      "degree=9, lambda=2.424, Training RMSE=0.991, Testing RMSE=2291817.317\n",
      "train acc :  0.696815\n",
      "validation acc :  0.70138\n",
      "degree=9, lambda=4.125, Training RMSE=0.993, Testing RMSE=700291.907\n",
      "train acc :  0.664315\n",
      "validation acc :  0.66798\n",
      "degree=9, lambda=7.017, Training RMSE=0.995, Testing RMSE=195845.541\n",
      "train acc :  0.65378\n",
      "validation acc :  0.65694\n",
      "degree=9, lambda=11.938, Training RMSE=0.995, Testing RMSE=610730.662\n",
      "train acc :  0.653865\n",
      "validation acc :  0.65642\n",
      "degree=9, lambda=20.309, Training RMSE=0.996, Testing RMSE=805692.754\n",
      "train acc :  0.64308\n",
      "validation acc :  0.64776\n",
      "degree=9, lambda=34.551, Training RMSE=0.996, Testing RMSE=873147.218\n",
      "train acc :  0.62981\n",
      "validation acc :  0.6353\n",
      "degree=9, lambda=58.780, Training RMSE=0.997, Testing RMSE=802090.496\n",
      "train acc :  0.6194\n",
      "validation acc :  0.62472\n",
      "degree=9, lambda=100.000, Training RMSE=0.997, Testing RMSE=571815.348\n",
      "train acc :  0.612545\n",
      "validation acc :  0.61856\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=676483333.131\n",
      "train acc :  0.816515\n",
      "validation acc :  0.53968\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=676500856.261\n",
      "train acc :  0.816405\n",
      "validation acc :  0.48428\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=676544462.725\n",
      "train acc :  0.816465\n",
      "validation acc :  0.4595\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=676660693.757\n",
      "train acc :  0.816445\n",
      "validation acc :  0.56982\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=676913614.641\n",
      "train acc :  0.81638\n",
      "validation acc :  0.5357\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=677324027.013\n",
      "train acc :  0.8165\n",
      "validation acc :  0.61458\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=677714984.004\n",
      "train acc :  0.816385\n",
      "validation acc :  0.69606\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=677538237.264\n",
      "train acc :  0.816505\n",
      "validation acc :  0.79322\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=675696015.137\n",
      "train acc :  0.816515\n",
      "validation acc :  0.7668\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=669844727.173\n",
      "train acc :  0.816445\n",
      "validation acc :  0.81068\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=654915829.318\n",
      "train acc :  0.816515\n",
      "validation acc :  0.81596\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=617254138.277\n",
      "train acc :  0.816395\n",
      "validation acc :  0.78712\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=522016065.059\n",
      "train acc :  0.8163\n",
      "validation acc :  0.81652\n",
      "degree=10, lambda=0.000, Training RMSE=0.744, Testing RMSE=323555050.973\n",
      "train acc :  0.81568\n",
      "validation acc :  0.8129\n",
      "degree=10, lambda=0.000, Training RMSE=0.746, Testing RMSE=34944121.647\n",
      "train acc :  0.814075\n",
      "validation acc :  0.81266\n",
      "degree=10, lambda=0.000, Training RMSE=0.752, Testing RMSE=231857543.215\n",
      "train acc :  0.80956\n",
      "validation acc :  0.8091\n",
      "degree=10, lambda=0.000, Training RMSE=0.764, Testing RMSE=348615337.539\n",
      "train acc :  0.798895\n",
      "validation acc :  0.7996\n",
      "degree=10, lambda=0.001, Training RMSE=0.782, Testing RMSE=220518030.909\n",
      "train acc :  0.781765\n",
      "validation acc :  0.78496\n",
      "degree=10, lambda=0.001, Training RMSE=0.804, Testing RMSE=29692814.386\n",
      "train acc :  0.763155\n",
      "validation acc :  0.76784\n",
      "degree=10, lambda=0.002, Training RMSE=0.828, Testing RMSE=121006379.134\n",
      "train acc :  0.74858\n",
      "validation acc :  0.75304\n",
      "degree=10, lambda=0.004, Training RMSE=0.848, Testing RMSE=21388113.097\n",
      "train acc :  0.734805\n",
      "validation acc :  0.73818\n",
      "degree=10, lambda=0.007, Training RMSE=0.866, Testing RMSE=92782299.336\n",
      "train acc :  0.722535\n",
      "validation acc :  0.72746\n",
      "degree=10, lambda=0.012, Training RMSE=0.882, Testing RMSE=149024393.458\n",
      "train acc :  0.713785\n",
      "validation acc :  0.7203\n",
      "degree=10, lambda=0.020, Training RMSE=0.898, Testing RMSE=159772013.416\n",
      "train acc :  0.70859\n",
      "validation acc :  0.7161\n",
      "degree=10, lambda=0.035, Training RMSE=0.914, Testing RMSE=137196732.924\n",
      "train acc :  0.706035\n",
      "validation acc :  0.71366\n",
      "degree=10, lambda=0.059, Training RMSE=0.925, Testing RMSE=102222553.367\n",
      "train acc :  0.70448\n",
      "validation acc :  0.71154\n",
      "degree=10, lambda=0.100, Training RMSE=0.932, Testing RMSE=74316834.526\n",
      "train acc :  0.703235\n",
      "validation acc :  0.70976\n",
      "degree=10, lambda=0.170, Training RMSE=0.940, Testing RMSE=48931608.418\n",
      "train acc :  0.702215\n",
      "validation acc :  0.70848\n",
      "degree=10, lambda=0.289, Training RMSE=0.952, Testing RMSE=15485174.240\n",
      "train acc :  0.70034\n",
      "validation acc :  0.70654\n",
      "degree=10, lambda=0.492, Training RMSE=0.965, Testing RMSE=16534028.691\n",
      "train acc :  0.697565\n",
      "validation acc :  0.70326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=10, lambda=0.838, Training RMSE=0.976, Testing RMSE=19608363.586\n",
      "train acc :  0.694495\n",
      "validation acc :  0.69976\n",
      "degree=10, lambda=1.425, Training RMSE=0.983, Testing RMSE=244767.886\n",
      "train acc :  0.69177\n",
      "validation acc :  0.69766\n",
      "degree=10, lambda=2.424, Training RMSE=0.987, Testing RMSE=18864003.383\n",
      "train acc :  0.690055\n",
      "validation acc :  0.69538\n",
      "degree=10, lambda=4.125, Training RMSE=0.989, Testing RMSE=29933381.008\n",
      "train acc :  0.688115\n",
      "validation acc :  0.69406\n",
      "degree=10, lambda=7.017, Training RMSE=0.991, Testing RMSE=33485561.374\n",
      "train acc :  0.68561\n",
      "validation acc :  0.69176\n",
      "degree=10, lambda=11.938, Training RMSE=0.993, Testing RMSE=29614450.492\n",
      "train acc :  0.6835\n",
      "validation acc :  0.68836\n",
      "degree=10, lambda=20.309, Training RMSE=0.994, Testing RMSE=20039625.535\n",
      "train acc :  0.682385\n",
      "validation acc :  0.68778\n",
      "degree=10, lambda=34.551, Training RMSE=0.995, Testing RMSE=9054941.498\n",
      "train acc :  0.683455\n",
      "validation acc :  0.689\n",
      "degree=10, lambda=58.780, Training RMSE=0.996, Testing RMSE=1376970.168\n",
      "train acc :  0.681775\n",
      "validation acc :  0.68724\n",
      "degree=10, lambda=100.000, Training RMSE=0.997, Testing RMSE=2401295.905\n",
      "train acc :  0.675345\n",
      "validation acc :  0.67928\n",
      "degree=11, lambda=0.000, Training RMSE=0.751, Testing RMSE=31304253390.694\n",
      "train acc :  0.81029\n",
      "validation acc :  0.48974\n",
      "degree=11, lambda=0.000, Training RMSE=0.750, Testing RMSE=31302652742.883\n",
      "train acc :  0.812275\n",
      "validation acc :  0.48736\n",
      "degree=11, lambda=0.000, Training RMSE=0.746, Testing RMSE=31302169363.624\n",
      "train acc :  0.814415\n",
      "validation acc :  0.40008\n",
      "degree=11, lambda=0.000, Training RMSE=0.761, Testing RMSE=31299697438.637\n",
      "train acc :  0.80579\n",
      "validation acc :  0.4699\n",
      "degree=11, lambda=0.000, Training RMSE=0.789, Testing RMSE=31298054341.649\n",
      "train acc :  0.79415\n",
      "validation acc :  0.51604\n",
      "degree=11, lambda=0.000, Training RMSE=0.746, Testing RMSE=31296188436.911\n",
      "train acc :  0.81465\n",
      "validation acc :  0.45292\n",
      "degree=11, lambda=0.000, Training RMSE=0.753, Testing RMSE=31294989576.883\n",
      "train acc :  0.81265\n",
      "validation acc :  0.47628\n",
      "degree=11, lambda=0.000, Training RMSE=0.746, Testing RMSE=31290285463.470\n",
      "train acc :  0.81502\n",
      "validation acc :  0.45942\n",
      "degree=11, lambda=0.000, Training RMSE=0.765, Testing RMSE=31270992319.997\n",
      "train acc :  0.801905\n",
      "validation acc :  0.5697\n",
      "degree=11, lambda=0.000, Training RMSE=0.755, Testing RMSE=31195601964.367\n",
      "train acc :  0.81067\n",
      "validation acc :  0.52024\n",
      "degree=11, lambda=0.000, Training RMSE=0.755, Testing RMSE=30918230041.030\n",
      "train acc :  0.81019\n",
      "validation acc :  0.54588\n",
      "degree=11, lambda=0.000, Training RMSE=0.747, Testing RMSE=29978571646.482\n",
      "train acc :  0.81368\n",
      "validation acc :  0.60288\n",
      "degree=11, lambda=0.000, Training RMSE=0.757, Testing RMSE=27348865669.044\n",
      "train acc :  0.809015\n",
      "validation acc :  0.57808\n",
      "degree=11, lambda=0.000, Training RMSE=0.752, Testing RMSE=22013933801.979\n",
      "train acc :  0.808735\n",
      "validation acc :  0.50952\n",
      "degree=11, lambda=0.000, Training RMSE=0.747, Testing RMSE=15177153818.641\n",
      "train acc :  0.81419\n",
      "validation acc :  0.61618\n",
      "degree=11, lambda=0.000, Training RMSE=0.752, Testing RMSE=9853801198.948\n",
      "train acc :  0.811975\n",
      "validation acc :  0.63186\n",
      "degree=11, lambda=0.000, Training RMSE=0.765, Testing RMSE=7481324345.050\n",
      "train acc :  0.79708\n",
      "validation acc :  0.76974\n",
      "degree=11, lambda=0.001, Training RMSE=0.783, Testing RMSE=7600447560.204\n",
      "train acc :  0.778855\n",
      "validation acc :  0.7151\n",
      "degree=11, lambda=0.001, Training RMSE=0.804, Testing RMSE=6416907422.671\n",
      "train acc :  0.7646\n",
      "validation acc :  0.77246\n",
      "degree=11, lambda=0.002, Training RMSE=0.825, Testing RMSE=2440279918.887\n",
      "train acc :  0.749805\n",
      "validation acc :  0.75648\n",
      "degree=11, lambda=0.004, Training RMSE=0.846, Testing RMSE=1024805434.589\n",
      "train acc :  0.73427\n",
      "validation acc :  0.73944\n",
      "degree=11, lambda=0.007, Training RMSE=0.863, Testing RMSE=2332814603.333\n",
      "train acc :  0.726075\n",
      "validation acc :  0.7306\n",
      "degree=11, lambda=0.012, Training RMSE=0.880, Testing RMSE=2020862655.344\n",
      "train acc :  0.71508\n",
      "validation acc :  0.72066\n",
      "degree=11, lambda=0.020, Training RMSE=0.896, Testing RMSE=925440974.963\n",
      "train acc :  0.709975\n",
      "validation acc :  0.71684\n",
      "degree=11, lambda=0.035, Training RMSE=0.910, Testing RMSE=223238189.931\n",
      "train acc :  0.70688\n",
      "validation acc :  0.71358\n",
      "degree=11, lambda=0.059, Training RMSE=0.922, Testing RMSE=968824248.376\n",
      "train acc :  0.70565\n",
      "validation acc :  0.71316\n",
      "degree=11, lambda=0.100, Training RMSE=0.931, Testing RMSE=1357817630.556\n",
      "train acc :  0.704765\n",
      "validation acc :  0.71096\n",
      "degree=11, lambda=0.170, Training RMSE=0.940, Testing RMSE=1547100015.411\n",
      "train acc :  0.703505\n",
      "validation acc :  0.70976\n",
      "degree=11, lambda=0.289, Training RMSE=0.951, Testing RMSE=1466238925.018\n",
      "train acc :  0.70204\n",
      "validation acc :  0.70798\n",
      "degree=11, lambda=0.492, Training RMSE=0.961, Testing RMSE=1143301563.025\n",
      "train acc :  0.701185\n",
      "validation acc :  0.70732\n",
      "degree=11, lambda=0.838, Training RMSE=0.968, Testing RMSE=691189431.424\n",
      "train acc :  0.699585\n",
      "validation acc :  0.70484\n",
      "degree=11, lambda=1.425, Training RMSE=0.973, Testing RMSE=199919627.403\n",
      "train acc :  0.699\n",
      "validation acc :  0.70466\n",
      "degree=11, lambda=2.424, Training RMSE=0.979, Testing RMSE=188477659.079\n",
      "train acc :  0.69591\n",
      "validation acc :  0.70106\n",
      "degree=11, lambda=4.125, Training RMSE=0.984, Testing RMSE=359822363.300\n",
      "train acc :  0.67679\n",
      "validation acc :  0.6828\n",
      "degree=11, lambda=7.017, Training RMSE=0.988, Testing RMSE=249240806.673\n",
      "train acc :  0.682725\n",
      "validation acc :  0.68762\n",
      "degree=11, lambda=11.938, Training RMSE=0.991, Testing RMSE=8613145.217\n",
      "train acc :  0.621575\n",
      "validation acc :  0.62668\n",
      "degree=11, lambda=20.309, Training RMSE=0.993, Testing RMSE=173764892.278\n",
      "train acc :  0.626405\n",
      "validation acc :  0.63116\n",
      "degree=11, lambda=34.551, Training RMSE=0.994, Testing RMSE=262614110.382\n",
      "train acc :  0.657365\n",
      "validation acc :  0.66154\n",
      "degree=11, lambda=58.780, Training RMSE=0.995, Testing RMSE=285166019.490\n",
      "train acc :  0.622335\n",
      "validation acc :  0.62452\n",
      "degree=11, lambda=100.000, Training RMSE=0.996, Testing RMSE=264518920.422\n",
      "train acc :  0.639765\n",
      "validation acc :  0.64194\n",
      "Best params for Ridge regression : degree =  10 , lambda =  5.87801607227e-05 , accuracy =  0.81652\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEaCAYAAAASSuyNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMRnCAr4xxmQIC/jGGJMhLOAbY0yG+P82Tjw/6ednNQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e36c89a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_no_nan, degree_no_nan, lambda_no_nan = test_ridge_regression(\n",
    "    x_no_nan, y_train, x_no_nan_val, y_validation, \n",
    "    degrees = np.linspace(9,11,3), lambdas=np.logspace(-7,2,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.ml import build_poly, least_squares\n",
    "\n",
    "# Build polynomial matrix\n",
    "_phi_train = build_poly(x_naive, 3)\n",
    "loss_tr, weights = least_squares(y_train, _phi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "weights[0:len(header)]\n",
    "w = weights[1:].reshape((-1, x_naive.shape[1]))\n",
    "print(np.sum(np.max(np.abs(w), axis=0) > 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_pt_h',\n",
       "       'DER_deltar_tau_lep', 'DER_sum_pt', 'DER_pt_ratio_lep_tau',\n",
       "       'PRI_tau_pt', 'PRI_lep_pt', 'PRI_jet_num', 'PRI_jet_all_pt'],\n",
       "      dtype='<U27')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_keep = np.max(np.abs(w), axis=0) > 0.1\n",
    "head_naive[feat_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 Least square deg 1 with acc 0.7355\n",
      "2/6 Least square deg 2 with acc 0.7479\n",
      "3/6 Least square deg 3 with acc 0.7544\n",
      "4/6 Least square deg 4 with acc 0.7534\n",
      "5/6 Least square deg 5 with acc 0.7553\n",
      "6/6 Least square deg 6 with acc 0.7295\n"
     ]
    }
   ],
   "source": [
    "from scripts.ml import build_poly, least_squares\n",
    "\n",
    "degrees = np.linspace(1, 6, 6).astype(int)\n",
    "for i, degree in enumerate(degrees):\n",
    "    acc, _, _ = cross_validation_ls(y_train, x_naive[:, feat_keep], degree=degree)\n",
    "    print('{}/{} Least square deg {} with acc {:.4f}'.format(i+1, len(degrees), degree, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 Least square deg 1 with acc 0.7357\n",
      "2/6 Least square deg 2 with acc 0.6631\n",
      "3/6 Least square deg 3 with acc 0.6215\n",
      "4/6 Least square deg 4 with acc 0.5946\n",
      "5/6 Least square deg 5 with acc 0.6269\n",
      "6/6 Least square deg 6 with acc 0.5415\n"
     ]
    }
   ],
   "source": [
    "from scripts.ml import augmented_feat_angle\n",
    "\n",
    "id_angle_feat = np.array([10, 13, 15])\n",
    "x_aug = augmented_feat_angle(x_naive, id_angle_feat)\n",
    "\n",
    "x_naive_aug = np.concatenate((x_naive[:, feat_keep], x_aug), axis=1)\n",
    "\n",
    "\n",
    "# normalize features\n",
    "#x_aug = (x_aug - np.nanmean(x_aug, axis=0))/np.nanstd(x_aug, axis=0)\n",
    "#x_aug = np.nan_to_num(x_aug)\n",
    "#print('\\nStd:', np.std(x_aug, axis=0))\n",
    "\n",
    "#print(np.max(x_train[:, 15]))\n",
    "\n",
    "x_naive_aug = (x_naive_aug - np.mean(x_naive_aug, axis=0))/np.std(x_naive_aug, axis=0)\n",
    "\n",
    "\n",
    "degrees = np.linspace(1, 6, 6).astype(int)\n",
    "for i, degree in enumerate(degrees):\n",
    "    acc, _, _ = cross_validation_ls(y_train, x_naive_aug, degree=degree)\n",
    "    print('{}/{} Least square deg {} with acc {:.4f}'.format(i+1, len(degrees), degree, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRI_tau_phi' 'PRI_lep_phi' 'PRI_met_phi']\n"
     ]
    }
   ],
   "source": [
    "print(head_naive[[10, 13, 15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
