{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-433 Machine Learning\n",
    "## Project 1 : The Higgs Boson Challenge\n",
    "\n",
    "Christian Abbet, Patryk Oleniuk, Ga√©tan Ramet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by loading our training data and splitting it in a training set (80%) and a validation set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scripts.implementations as lib  # Add personal library\n",
    "import scripts.proj1_helpers as helper  # Add personal library\n",
    "import scripts.ml as ml # Add personal library\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DATA_TRAIN = os.path.join(DATA_FOLDER, 'train.csv')\n",
    "DATA_TEST = os.path.join(DATA_FOLDER, 'test.csv')\n",
    "\n",
    "y, x, ids, header = helper.load_csv_data(DATA_TRAIN)\n",
    "y_train, x_train,  y_validation, x_validation = lib.sep_valid_train_data(x,y, 0.8);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration and Cleaning \n",
    "\n",
    "\n",
    "## 1.1 Data Exploration\n",
    "\n",
    "We first load the data to see what is the repartition of the data. The two possible classes in for the measurements are `s` for signal, indicating the presence of a Higgs boson and `b` for backgroud noise. In this case around 2/3 of the data (65.73%) is labeled as background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition of 200000 labels, s: 34.37%, b: 65.63%\n"
     ]
    }
   ],
   "source": [
    "print('Repartition of {} labels, s: {:.2f}%, b: {:.2f}%'.format(\n",
    "    len(y_train), np.mean(y_train==1)*100, np.mean(y_train==-1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [the Higgs boson machine learning challenge](https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf) some variable are indicated as \"may be undefined\" when it can happen that they are meaning-\n",
    "less or cannot be computed. In this case, their value is set to -999.0, which is outside the normal range of all variables. We will set them to NaN so they will be easier to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train[x_train == -999] = np.nan\n",
    "x_validation[x_validation == -999] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the repartition of the NaN along the features. We can see that some features seems to have the same amount of NaN value. The second graph shows that some features seems to have NaNs values axactly at the same location. We distinguish here 4 different kind of nan distributions: For some feature, we have a high proportions of nans, while for others, it is very low and we don't even see it on the plot. Having so many NaNs in our data lead us to think we can propably use them in some way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.bar(np.arange(len(header)), np.sum(np.isnan(x_train), axis=0))\n",
    "plt.xticks(np.arange(len(header)), header, rotation='vertical')\n",
    "plt.ylim(0, len(y_train)); plt.xlabel('Features'); plt.xlabel('#Sample'); plt.title('NaN sum per feature')\n",
    "plt.grid(); plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 20))\n",
    "plt.matshow(np.isnan(x_train)[:100, :].T)\n",
    "plt.yticks(np.arange(len(header)), header)\n",
    "plt.xlabel('Features'); plt.xlabel('#Sample'); plt.title('NAN sum per feature')\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the NaN value gave us any information (`s` or `b`) ? We can see that for some features, the presence of Nan values seems to change the repartition of signal and background measurement : this means that we can use information from the presence of NaNs for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i,feature in enumerate(x_train.T):\n",
    "    print('Feature {:d} : NaN is present, s: {:.2f}, b: {:.2f}'.format(i,\n",
    "        np.mean((y_train[(np.isnan(feature))] == 1)),\n",
    "        np.mean((y_train[(np.isnan(feature))] == -1))))\n",
    "    print('Feature {:d} : NaN is NOT present, s: {:.2f}, b: {:.2f}'.format(i,\n",
    "        np.mean((y_train[~(np.isnan(feature))] == 1)),\n",
    "        np.mean((y_train[~(np.isnan(feature))] == -1))))\n",
    "    pass\n",
    "print('NaN is present, s: {:.2f}, b: {:.2f}'.format(\n",
    "     np.mean(y_train[np.any(np.isnan(x_train), axis=1)] == 1), \n",
    "     np.mean(y_train[np.any(np.isnan(x_train), axis=1)] == -1)))\n",
    "print('NaN is not present, s: {:.2f}, b: {:.2f}'.format(\n",
    "     np.mean(y_train[~np.any(np.isnan(x_train), axis=1)] == 1), \n",
    "     np.mean(y_train[~np.any(np.isnan(x_train), axis=1)] == -1)))\n",
    "\n",
    "#for i, feature in enumerate(x_train.T):\n",
    "#    print(' Feature {:d}: NaN is present, s: {:.2f}, b: {:.2f}'.format(i,\n",
    "#        np.mean(y_train[np.any(np.isnan(feature), axis=0)] == 1), \n",
    "#        np.mean(y_train[np.any(np.isnan(feature), axis=0)] == -1)))\n",
    "#    print(' Feature {:d} : NaN is not present, s: {:.2f}, b: {:.2f}'.format(i,\n",
    "#        np.mean(y_train[~np.any(np.isnan(feature), axis=0)] == 1), \n",
    "#        np.mean(y_train[~np.any(np.isnan(feature), axis=0)] == -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the feature ranges. it can give us insights of the data. We can see that features (16), (19), (21), (26) and (29) are actually angles (range in $[-\\pi, \\pi]$). To be certain we checked it directly on the documentation. We decided to also use cosines and sines of these angles as features, as they might be relevant for classification. Note that we are ignoring the NaN values to compute the min and max.\n",
    "\n",
    "We have to be careful with those results since the output gives us no imformation about the data distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Base Features: \\n\")\n",
    "for i, feature in enumerate(x_train.T):\n",
    "    print('Feature {} - {} has range: [{:.4f}, {:.4f}]'.format(\n",
    "        i+1, header[i], np.nanmin(feature), np.nanmax(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,60))\n",
    "\n",
    "for i, feature in enumerate(x_train.T):\n",
    "    plt.subplot(15, 2, i+1)\n",
    "    id_keep = ~np.isnan(feature)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.hist(feature[id_keep], bins=100, alpha=0.4, label='total')\n",
    "    plt.hist(feature[id_b], alpha=0.4, bins=100, label='back')\n",
    "    plt.hist(feature[id_s], alpha=0.4, bins=100, label='signal')\n",
    "    plt.title('Feature {} - {}'.format(i+1, header[i]))\n",
    "    plt.legend()\n",
    "plt.suptitle(\"Distributions of base features\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,24))\n",
    "\n",
    "for i, feature in enumerate(x_train.T):\n",
    "    plt.subplot(10, 3, i+1)\n",
    "    id_keep = ~np.isnan(feature)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.boxplot([feature[id_keep], feature[id_b], feature[id_s]], whis=2.5, \n",
    "                vert=False, labels=['total', 'back', 'signal'])\n",
    "    plt.title('Feature {} - {}'.format(i+1, header[i]))\n",
    "plt.suptitle(\"Boxplot distributions of base features\")\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Feature augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the information gathered in previous section to generate new features that are more helpful for classification. First, we will start working with the angles, by adding new features that are the cosines and sines of each angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 - DER_mass_MMC has range: [9.0440, 1192.0260]\n",
      "Feature 2 - DER_mass_transverse_met_lep has range: [0.0000, 595.8190]\n",
      "Feature 3 - DER_mass_vis has range: [6.4620, 1329.9130]\n",
      "Feature 4 - DER_pt_h has range: [0.0000, 1053.8070]\n",
      "Feature 5 - DER_deltaeta_jet_jet has range: [0.0000, 8.5030]\n",
      "Feature 6 - DER_mass_jet_jet has range: [13.6020, 4974.9790]\n",
      "Feature 7 - DER_prodeta_jet_jet has range: [-18.0660, 16.6900]\n",
      "Feature 8 - DER_deltar_tau_lep has range: [0.2080, 5.6840]\n",
      "Feature 9 - DER_pt_tot has range: [0.0000, 513.6590]\n",
      "Feature 10 - DER_sum_pt has range: [46.1040, 1852.4620]\n",
      "Feature 11 - DER_pt_ratio_lep_tau has range: [0.0470, 19.7730]\n",
      "Feature 12 - DER_met_phi_centrality has range: [-1.4140, 1.4140]\n",
      "Feature 13 - DER_lep_eta_centrality has range: [0.0000, 1.0000]\n",
      "Feature 14 - PRI_tau_pt has range: [20.0000, 622.8620]\n",
      "Feature 15 - PRI_tau_eta has range: [-2.4990, 2.4970]\n",
      "Feature 16 - PRI_tau_phi has range: [-3.1420, 3.1420]\n",
      "Feature 17 - PRI_lep_pt has range: [26.0000, 461.8960]\n",
      "Feature 18 - PRI_lep_eta has range: [-2.5050, 2.5020]\n",
      "Feature 19 - PRI_lep_phi has range: [-3.1420, 3.1420]\n",
      "Feature 20 - PRI_met has range: [0.1090, 951.3630]\n",
      "Feature 21 - PRI_met_phi has range: [-3.1420, 3.1420]\n",
      "Feature 22 - PRI_met_sumet has range: [13.6780, 2003.9760]\n",
      "Feature 23 - PRI_jet_num has range: [0.0000, 3.0000]\n",
      "Feature 24 - PRI_jet_leading_pt has range: [30.0000, 1120.5730]\n",
      "Feature 25 - PRI_jet_leading_eta has range: [-4.4990, 4.4990]\n",
      "Feature 26 - PRI_jet_leading_phi has range: [-3.1420, 3.1410]\n",
      "Feature 27 - PRI_jet_subleading_pt has range: [30.0010, 721.4560]\n",
      "Feature 28 - PRI_jet_subleading_eta has range: [-4.5000, 4.5000]\n",
      "Feature 29 - PRI_jet_subleading_phi has range: [-3.1420, 3.1420]\n",
      "Feature 30 - PRI_jet_all_pt has range: [0.0000, 1633.4330]\n",
      "Feature 31 - PRI_tau_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 32 - PRI_lep_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 33 - PRI_met_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 34 - PRI_jet_leading_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 35 - PRI_jet_subleading_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 36 - PRI_tau_phi_sin has range: [-1.0000, 1.0000]\n",
      "Feature 37 - PRI_lep_phi_sin has range: [-1.0000, 1.0000]\n",
      "Feature 38 - PRI_met_phi_sin has range: [-1.0000, 1.0000]\n",
      "Feature 39 - PRI_jet_leading_phi_sin has range: [-1.0000, 1.0000]\n",
      "Feature 40 - PRI_jet_subleading_phi_sin has range: [-1.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "id_angle = [15, 18, 20, 25, 28] # The ids of the features that are angles\n",
    "x_aug, header = ml.augmented_feat_angle(x_train, id_angle, header)\n",
    "\n",
    "for i, feature in enumerate(x_aug.T):\n",
    "    print('Feature {} - {} has range: [{:.4f}, {:.4f}]'.format(\n",
    "        i+1, header[i], np.nanmin(feature), np.nanmax(feature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to generate new meaningful features by multiplying together the basic features and evaluating the difference between the median of the signal data and the background data for each new feature to determine which one are relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = x_aug.shape[1]\n",
    "m_b = np.zeros((dim, dim)) # the background medians\n",
    "m_s = np.zeros((dim, dim)) # the signal medians\n",
    "m_d = np.zeros((dim, dim)) # the difference medians\n",
    "\n",
    "for i, f1 in enumerate(x_aug.T):\n",
    "    for j, f2 in enumerate(x_aug.T):\n",
    "        if i == j:\n",
    "            f_res = f1\n",
    "        else:\n",
    "            f_res = f1*f2\n",
    "        id_keep = ~np.isnan(f_res)\n",
    "        id_b = np.logical_and(y_train == -1, id_keep)\n",
    "        id_s = np.logical_and(y_train == 1, id_keep)\n",
    "        f_norm = (f_res-np.nanmean(f_res))/np.nanstd(f_res) # We normalize the features\n",
    "        m_b[i,j] = np.median(f_norm[id_b])\n",
    "        m_s[i,j] = np.median(f_norm[id_s])\n",
    "        m_d[i,j] = np.median(f_norm[id_b]) - np.median(f_norm[id_s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to enforce the new features to be different from the old feature, so we penalize the median difference of the new feature by the one of it's first component in the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9c437f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARAAAAECCAYAAAA2FIiFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4zh14wbEcZy6cQPiOE7duAFxHKdu/j/cs9bCCEWb0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9c43710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "values_to_plot = np.abs(m_d)-np.abs(np.diag(m_d))\n",
    "values_to_plot[values_to_plot <0] = 0.0 # thresholding to better see where are the highest vals\n",
    "plt.matshow(values_to_plot ) \n",
    "plt.colorbar()\n",
    "plt.title('Map of Combined features relevance', y=1.12, fontsize=12)\n",
    "plt.xlabel('feature nr')\n",
    "plt.ylabel('feature nr')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a look at the new features distributions allows us to see that they separate signal from background much better than the base features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tresh_id = np.nonzero((np.abs(m_d)-np.abs(np.diag(m_d))).flatten() > 0.4)\n",
    "res = np.unravel_index(tresh_id, (dim, dim))\n",
    "combs = np.array(res).reshape((2,-1)).T\n",
    "\n",
    "# Take only unique pairs\n",
    "combs = np.sort(combs, axis=1)\n",
    "combs = list(set([tuple(row) for row in combs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_lines = np.ceil(len(combs)/2)\n",
    "plt.figure(figsize=(16, 3*n_lines))\n",
    "\n",
    "for i, comb in enumerate(combs):\n",
    "    plt.subplot(n_lines, 2, i+1)\n",
    "    \n",
    "    f1 = x_aug[:, comb[0]]\n",
    "    f2 = x_aug[:, comb[1]]\n",
    "    ft = f1*f2\n",
    "\n",
    "    id_keep = ~np.isnan(ft)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "\n",
    "    plt.hist(ft[id_keep], bins=100, alpha=0.4, label='total')\n",
    "    plt.hist(ft[id_b], alpha=0.4, bins=100, label='back')\n",
    "    plt.hist(ft[id_s], alpha=0.4, bins=100, label='signal')\n",
    "    plt.title('Combinaison with feat [{};{}]'.format(comb[0]+1, comb[1]+1))\n",
    "    plt.legend()\n",
    "plt.suptitle('Combined features distributions: ')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will also add the features based on the nan distributions as discussed above and normalize the data. For each of the 3 types of Nan distributions we will set the value of the NAN-feature to 1 if the initial feature is nan, and to -1 if it was not a Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 - DER_mass_MMC has range: [9.0440, 1192.0260]\n",
      "Feature 2 - DER_mass_transverse_met_lep has range: [0.0000, 595.8190]\n",
      "Feature 3 - DER_mass_vis has range: [6.4620, 1329.9130]\n",
      "Feature 4 - DER_pt_h has range: [0.0000, 1053.8070]\n",
      "Feature 5 - DER_deltaeta_jet_jet has range: [0.0000, 8.5030]\n",
      "Feature 6 - DER_mass_jet_jet has range: [13.6020, 4974.9790]\n",
      "Feature 7 - DER_prodeta_jet_jet has range: [-18.0660, 16.6900]\n",
      "Feature 8 - DER_deltar_tau_lep has range: [0.2080, 5.6840]\n",
      "Feature 9 - DER_pt_tot has range: [0.0000, 513.6590]\n",
      "Feature 10 - DER_sum_pt has range: [46.1040, 1852.4620]\n",
      "Feature 11 - DER_pt_ratio_lep_tau has range: [0.0470, 19.7730]\n",
      "Feature 12 - DER_met_phi_centrality has range: [-1.4140, 1.4140]\n",
      "Feature 13 - DER_lep_eta_centrality has range: [0.0000, 1.0000]\n",
      "Feature 14 - PRI_tau_pt has range: [20.0000, 622.8620]\n",
      "Feature 15 - PRI_tau_eta has range: [-2.4990, 2.4970]\n",
      "Feature 16 - PRI_tau_phi has range: [-3.1420, 3.1420]\n",
      "Feature 17 - PRI_lep_pt has range: [26.0000, 461.8960]\n",
      "Feature 18 - PRI_lep_eta has range: [-2.5050, 2.5020]\n",
      "Feature 19 - PRI_lep_phi has range: [-3.1420, 3.1420]\n",
      "Feature 20 - PRI_met has range: [0.1090, 951.3630]\n",
      "Feature 21 - PRI_met_phi has range: [-3.1420, 3.1420]\n",
      "Feature 22 - PRI_met_sumet has range: [13.6780, 2003.9760]\n",
      "Feature 23 - PRI_jet_num has range: [0.0000, 3.0000]\n",
      "Feature 24 - PRI_jet_leading_pt has range: [30.0000, 1120.5730]\n",
      "Feature 25 - PRI_jet_leading_eta has range: [-4.4990, 4.4990]\n",
      "Feature 26 - PRI_jet_leading_phi has range: [-3.1420, 3.1410]\n",
      "Feature 27 - PRI_jet_subleading_pt has range: [30.0010, 721.4560]\n",
      "Feature 28 - PRI_jet_subleading_eta has range: [-4.5000, 4.5000]\n",
      "Feature 29 - PRI_jet_subleading_phi has range: [-3.1420, 3.1420]\n",
      "Feature 30 - PRI_jet_all_pt has range: [0.0000, 1633.4330]\n",
      "Feature 31 - PRI_tau_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 32 - PRI_lep_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 33 - PRI_met_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 34 - PRI_jet_leading_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 35 - PRI_jet_subleading_phi_cos has range: [-1.0000, 1.0000]\n",
      "Feature 36 - PRI_tau_phi_sin has range: [-1.0000, 1.0000]\n",
      "Feature 37 - PRI_lep_phi_sin has range: [-1.0000, 1.0000]\n",
      "Feature 38 - PRI_met_phi_sin has range: [-1.0000, 1.0000]\n",
      "Feature 39 - PRI_jet_leading_phi_sin has range: [-1.0000, 1.0000]\n",
      "Feature 40 - PRI_jet_subleading_phi_sin has range: [-1.0000, 1.0000]\n",
      "Feature 41 - NAN-0 has range: [-1.0000, 1.0000]\n",
      "Feature 42 - NAN-1 has range: [-1.0000, 1.0000]\n",
      "Feature 43 - NAN-2 has range: [-1.0000, 1.0000]\n"
     ]
    }
   ],
   "source": [
    "id_nan = [0, 24, 27]\n",
    "x_aug, header = ml.add_nan_feature(x_aug, id_nan, header)\n",
    "\n",
    "for i, feature in enumerate(x_aug.T):\n",
    "    print('Feature {} - {} has range: [{:.4f}, {:.4f}]'.format(\n",
    "        i+1, header[i], np.nanmin(feature), np.nanmax(feature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will add the combined features we designed earlier and remove the angles as we already use their cos and sin that are more meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_aug = ml.add_features(x_aug, combs) # add features that are product of other features\n",
    "x_aug = ml.remove_useless(x_aug, id_useless=id_angle) #remove the angles as we used their cos and sin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step we will normalize all our features as if they were normally distributed, even though some are not and set the nan measures to 0 (mean value) so that the don't influence the prediction. We deal with outliers by thresholding them to 2.5 $\\sigma$ (the standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaet_2\\Documents\\EPFL\\Machine Learning\\projet\\ML_course\\projects\\project1\\scripts\\ml.py:68: RuntimeWarning: invalid value encountered in less\n",
      "  mean_update = np.nanmean(feature[np.abs(feat_cent) < maxval])\n",
      "C:\\Users\\Gaet_2\\Documents\\EPFL\\Machine Learning\\projet\\ML_course\\projects\\project1\\scripts\\ml.py:69: RuntimeWarning: invalid value encountered in less\n",
      "  std_update = np.nanstd(feature[np.abs(feat_cent) < maxval])\n",
      "C:\\Users\\Gaet_2\\Documents\\EPFL\\Machine Learning\\projet\\ML_course\\projects\\project1\\scripts\\ml.py:71: RuntimeWarning: invalid value encountered in greater\n",
      "  feat_final[feat_final > maxval] = maxval\n",
      "C:\\Users\\Gaet_2\\Documents\\EPFL\\Machine Learning\\projet\\ML_course\\projects\\project1\\scripts\\ml.py:72: RuntimeWarning: invalid value encountered in less\n",
      "  feat_final[feat_final < -maxval] = -maxval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Std: [ 1.074   1.0408  1.1532  1.1364  0.5519  0.6304  0.5907  1.0192  1.1146\n",
      "  1.1341  1.1064  1.      0.5399  1.1454  1.      1.1456  1.      1.1392\n",
      "  1.1057  1.      0.89    0.7758  0.6201  0.5399  1.1287  1.      1.      1.\n",
      "  0.7753  0.5399  1.      1.      1.      0.7753  0.5399  1.      1.      1.\n",
      "  1.0366  0.587   1.0436  0.5493  1.      1.0006  0.5589  0.5653  0.5932\n",
      "  1.0003  1.0565  1.1073  1.      1.0791  0.5907  1.0823  0.9597  1.0689\n",
      "  0.59    0.5514  1.0475  0.572   0.606   0.5444  1.0413  1.0791  1.0014\n",
      "  0.5399  0.5878  1.0917  1.0436  0.6118  0.5923  1.0661  0.6012  0.5752\n",
      "  0.5796] \n",
      "n_feat 75\n"
     ]
    }
   ],
   "source": [
    "distrib = ['g']*(x_aug.shape[1])\n",
    "x_aug, mean_train, std_train, max_train = ml.normalize_outliers(x_aug, distrib) #normalize all the features as gaussian\n",
    "x_aug = np.nan_to_num(x_aug) #put all nans to 0 (mean value) so that they don't influence the predictions\n",
    "print('\\nStd:', np.std(x_aug, axis=0), '\\nn_feat', x_aug.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly apply the same treatment to our validation set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaet_2\\Documents\\EPFL\\Machine Learning\\projet\\ML_course\\projects\\project1\\scripts\\ml.py:88: RuntimeWarning: invalid value encountered in greater\n",
      "  feat_final[feat_final > maxval]  = maxval\n",
      "C:\\Users\\Gaet_2\\Documents\\EPFL\\Machine Learning\\projet\\ML_course\\projects\\project1\\scripts\\ml.py:89: RuntimeWarning: invalid value encountered in less\n",
      "  feat_final[feat_final < -maxval] = -maxval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Std: [ 1.0809  1.0384  1.1548  1.134   0.5455  0.6189  0.5829  1.0176  1.106\n",
      "  1.1246  1.1044  1.0038  0.5337  1.1372  1.0017  1.1371  1.0049  1.1418\n",
      "  1.103   0.9936  0.8915  0.7739  0.6114  0.5313  1.1198  1.0002  0.9983\n",
      "  0.9979  0.7729  0.5312  0.9998  1.0017  1.0021  0.7734  0.5365  0.9993\n",
      "  1.0014  0.9934  1.0368  0.5769  1.0485  0.5396  1.0019  1.009   0.5531\n",
      "  0.557   0.5846  1.0066  1.0561  1.0976  0.9951  1.0809  0.5829  1.0742\n",
      "  0.9645  1.0647  0.5826  0.5512  1.0471  0.5661  0.5967  0.5403  1.0385\n",
      "  1.0747  1.0016  0.5329  0.5813  1.0816  1.0496  0.6188  0.5797  1.0659\n",
      "  0.6016  0.5704  0.575 ] \n",
      "n_feat 75\n"
     ]
    }
   ],
   "source": [
    "# normalize features\n",
    "x_aug_val = x_validation.copy()\n",
    "x_aug_val, _ = ml.augmented_feat_angle(x_aug_val, id_angle, header)\n",
    "x_aug_val, _ = ml.add_nan_feature(x_aug_val, id_nan, header)\n",
    "x_aug_val = ml.add_features(x_aug_val, combs)\n",
    "x_aug_val = ml.remove_useless(x_aug_val, id_useless=id_angle)\n",
    "x_aug_val = ml.normalize_outliers_feed(x_aug_val, mean_train, std_train, max_train, distrib)\n",
    "x_aug_val = np.nan_to_num(x_aug_val)\n",
    "print('\\nStd:', np.std(x_aug_val, axis=0), '\\nn_feat', x_aug_val.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Least Squares (Normal equations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_ls, degree_ls = lib.test_least_squares(\n",
    "    x_aug, y_train, x_aug_val, y_validation, degrees = np.linspace(1,6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Least Squares (Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_ls, degree_ls = lib.test_least_squares(\n",
    "    x_aug, y_train, x_aug_val, y_validation, degrees = np.linspace(1,6,6), mode='GD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Least Squares (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_ls, degree_ls = lib.test_least_squares(\n",
    "    x_aug, y_train, x_aug_val, y_validation, degrees = np.linspace(1,6,6), mode='SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ridge Regression, we sweeped the degrees from 6 to 13 and the regularize $\\lambda$ from $10^{-8}$ to $10^2$. Here, for the sake of speed, we execute a smaller sweep which gives us 83.34% validation accuracy at degree 10 and $\\lambda=5.45 * 10^{-6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_ridge, degree_ridge, lambda_ridge = lib.test_ridge_regression(\n",
    "    x_aug, y_train, x_aug_val, y_validation, degrees = np.linspace(1,10,1), lambdas=np.logspace(-8,-4, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Logistic Regression (by Newton Method)\n",
    "For Logistic Regression with  Newton Method, we sweeped the degrees from 1 to 10. $\\gamma$=1e-2. We had some problems with Logistic regression numerical errors (loss going to NaN for lots of dims) and that is why we couldnt make it work for Gradient descent for degrees >1. We present here Newton method that is working better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train_log = y_train.copy();\n",
    "y_train_log[y_train_log == 0] = -1;\n",
    "weights_log_newton, degree_log_newton = lib.test_logistic_Newton(\n",
    "    x_aug, y_train_log, x_aug_val, y_validation, degrees = np.linspace(1,10,10), gamma=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the logistic regression\n",
      "loss\n",
      "gradient\n",
      "Current iteration=0, loss=[ 138629.4361]\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "loss\n",
      "gradient\n",
      "STOP it=99, loss=[ 138441.3333] \n",
      "train acc :  0.0\n",
      "validation acc :  25810.9308\n",
      "Best params for Least Squares : degree =  1 , accuracy =  25810.9308\n"
     ]
    }
   ],
   "source": [
    "y_train_log = y_train.copy();\n",
    "y_train_log[y_train_log == -1] = 0;\n",
    "y_validation_log = y_validation.copy();\n",
    "y_validation_log[y_validation_log == -1] = 0;\n",
    "\n",
    "weights_log_newton, degree_log_newton = lib.test_logistic_GD(\n",
    "    x_aug, y_train_log, x_aug_val, y_validation_log, degrees = np.linspace(1,10,1), gamma=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1.,  1., -1., -1., -1., -1., -1., -1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Comparison\n",
    "The maximum accuracies from different methods have been gathered for different seeds, in order to see what is the variation of the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The maximum accuracies have been gathered for different seeds \n",
    "seeds =     [0     , 2      , 3 ,     4,     5,     6]\n",
    "ls_res =    [0.7771, 0.7742 , 0.776,  0.776 ,0.775, 0.775]\n",
    "lsgd_res =  [0.7719, 0.770  , 0.769,  0.769, 0.769, 0.766]\n",
    "lssdg_res = [0.7640, 0.7693 , 0.7632, 0.764, 0.763, 0.762]\n",
    "ridge_res = [0.8334, 0.8312 , 0.8310, 0.831, 0.832, 0.832]\n",
    "logistic =  [0.781, 0.777  , 0.7781,  0.776, 0.779,  0.775]\n",
    "\n",
    "def print_val(name,val):\n",
    "    print(name,' \\t: ', np.mean(val), ' +-',np.abs(np.max(val) - np.min(val)))\n",
    "    \n",
    "print_val('Least Squares ',ls_res)\n",
    "print_val('Least Squares GD',lsgd_res)\n",
    "print_val('Least Squares SGD',lssdg_res)\n",
    "print_val('Ridge Regr.   ',ridge_res)\n",
    "print_val('Logistic Regr. ',logistic)\n",
    "\n",
    "data = [ls_res, lsgd_res, lssdg_res, ridge_res, logistic]\n",
    "\n",
    "# don't show outlier points\n",
    "plt.figure()\n",
    "plt.boxplot(data, 0, '', labels = ['LS','LS-GD','LS-SGD','Ridge','Logistic(Newton)'])\n",
    "plt.ylabel('test set accuracy')\n",
    "plt.xlabel('method')\n",
    "plt.title('Achieved accuracy and its deviation for diff. methods')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Submission test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here take our best model, use it to classify the test data and generate the csv file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load data, add feature and normalize\n",
    "y_test, x_test, ids_test, header = helper.load_csv_data(DATA_TEST)\n",
    "x_test[x_test == -999] = np.nan\n",
    "\n",
    "x_aug_test = x_test.copy()\n",
    "x_aug_test, _ = ml.augmented_feat_angle(x_aug_test, id_angle, header)\n",
    "x_aug_test, _ = ml.add_nan_feature(x_aug_test, id_nan, header)\n",
    "x_aug_test = ml.add_features(x_aug_test, combs)\n",
    "x_aug_test = ml.remove_useless(x_aug_test, id_useless=id_angle)\n",
    "x_aug_test = ml.normalize_outliers_feed(x_aug_test, mean_train, std_train, max_train, distrib)\n",
    "x_aug_test = np.nan_to_num(x_aug_test)\n",
    "print('\\nStd:', np.std(x_aug_test, axis=0),'\\nn_feat', x_aug_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prediction\n",
    "\n",
    "degree_opt = degree_ridge\n",
    "weights_opt = weights_ridge\n",
    "\n",
    "_phi_test = lib.build_poly(x_aug_test, degree_opt)\n",
    "y_pred = helper.predict_labels(weights_opt, _phi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Submission\n",
    "helper.create_csv_submission(ids_test, y_pred, 'final_submit.csv')\n",
    "print('Results saved ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
