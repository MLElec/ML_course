{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scripts.implementations as lib  # Add personal library\n",
    "import scripts.proj1_helpers as helper  # Add personal library\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DATA_TRAIN = os.path.join(DATA_FOLDER, 'train.csv')\n",
    "DATA_TEST = os.path.join(DATA_FOLDER, 'test.csv')\n",
    "\n",
    "y, x, ids, header = helper.load_csv_data(DATA_TRAIN)\n",
    "y_test, x_test, ids_test, header = helper.load_csv_data(DATA_TEST)\n",
    "y_train, x_train,  y_validation, x_validation = lib.sep_valid_train_data(x,y,0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[x_train == -999] = np.nan\n",
    "x_test[x_test == -999] = np.nan\n",
    "x_validation[x_validation == -999] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition of #jet [ 0.  1.  2.  3.] along data [ 39.656  31.362  20.138   8.844]%\n"
     ]
    }
   ],
   "source": [
    "tags, count = np.unique(x_train[:, 22], return_counts=True)\n",
    "repart_jet = count/len(y_train)\n",
    "print('Repartition of #jet {} along data {}%'.format(tags, 100*repart_jet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 - DER_mass_MMC has range: [13.1340, 945.5550]\n",
      "Feature 2 - DER_mass_transverse_met_lep has range: [0.0010, 511.4080]\n",
      "Feature 3 - DER_mass_vis has range: [9.4950, 1034.2050]\n",
      "Feature 4 - DER_pt_h has range: [0.0000, 762.8060]\n",
      "Feature 5 - DER_deltaeta_jet_jet has range: [0.0000, 8.5030]\n",
      "Feature 6 - DER_mass_jet_jet has range: [15.3190, 4974.9790]\n",
      "Feature 7 - DER_prodeta_jet_jet has range: [-18.0660, 14.7000]\n",
      "Feature 8 - DER_deltar_tau_lep has range: [0.3460, 5.5460]\n",
      "Feature 9 - DER_pt_tot has range: [0.0000, 371.8760]\n",
      "Feature 10 - DER_sum_pt has range: [46.3130, 1703.7520]\n",
      "Feature 11 - DER_pt_ratio_lep_tau has range: [0.0800, 19.7730]\n",
      "Feature 12 - DER_met_phi_centrality has range: [-1.4140, 1.4140]\n",
      "Feature 13 - DER_lep_eta_centrality has range: [0.0000, 1.0000]\n",
      "Feature 14 - PRI_tau_pt has range: [20.0000, 505.0600]\n",
      "Feature 15 - PRI_tau_eta has range: [-2.4990, 2.4940]\n",
      "Feature 16 - PRI_tau_phi has range: [-3.1420, 3.1420]\n",
      "Feature 17 - PRI_lep_pt has range: [26.0000, 426.3980]\n",
      "Feature 18 - PRI_lep_eta has range: [-2.4890, 2.5020]\n",
      "Feature 19 - PRI_lep_phi has range: [-3.1420, 3.1410]\n",
      "Feature 20 - PRI_met has range: [0.2000, 695.5330]\n",
      "Feature 21 - PRI_met_phi has range: [-3.1410, 3.1410]\n",
      "Feature 22 - PRI_met_sumet has range: [13.6780, 1723.0870]\n",
      "Feature 23 - PRI_jet_num has range: [0.0000, 3.0000]\n",
      "Feature 24 - PRI_jet_leading_pt has range: [30.0030, 720.2820]\n",
      "Feature 25 - PRI_jet_leading_eta has range: [-4.4850, 4.4660]\n",
      "Feature 26 - PRI_jet_leading_phi has range: [-3.1410, 3.1410]\n",
      "Feature 27 - PRI_jet_subleading_pt has range: [30.0040, 706.6720]\n",
      "Feature 28 - PRI_jet_subleading_eta has range: [-4.4910, 4.5000]\n",
      "Feature 29 - PRI_jet_subleading_phi has range: [-3.1410, 3.1420]\n",
      "Feature 30 - PRI_jet_all_pt has range: [0.0000, 1633.4330]\n"
     ]
    }
   ],
   "source": [
    "for i, feature in enumerate(x_train.T):\n",
    "    print('Feature {} - {} has range: [{:.4f}, {:.4f}]'.format(\n",
    "        i+1, header[i], np.nanmin(feature), np.nanmax(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAANaCAYAAACpxYzFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbtw7ecVp4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2081ed6128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 1, sharex=True, sharey=True, figsize=(14, 12))\n",
    "nan_j_tr = []; nan_j_val = []; nan_j_te = []\n",
    "for i in range(4):\n",
    "    nan_j_tr.append(np.array([ \n",
    "        np.sum(np.logical_and(x_train[:, 22] == i, np.isnan(f))) for f in x_train.T])/len(y_train))\n",
    "    nan_j_val.append(np.array([\n",
    "        np.sum(np.logical_and(x_validation[:, 22] == i, np.isnan(f))) for f in x_validation.T])/len(y_validation))\n",
    "    nan_j_te.append(np.array([\n",
    "        np.sum(np.logical_and(x_test[:, 22] == i, np.isnan(f))) for f in x_test.T])/len(y_test))\n",
    "    axes[0].bar(np.arange(len(header)), nan_j_tr[-1], \n",
    "            label='Ratio Jet={}'.format(i), bottom = np.sum(np.array(nan_j_tr[:-1]), axis=0))\n",
    "    axes[1].bar(np.arange(len(header)), nan_j_val[-1], \n",
    "            label='Ratio Jet={}'.format(i), bottom = np.sum(np.array(nan_j_val[:-1]), axis=0))\n",
    "    axes[2].bar(np.arange(len(header)), nan_j_te[-1], \n",
    "            label='Ratio Jet={}'.format(i), bottom = np.sum(np.array(nan_j_te[:-1]), axis=0))\n",
    "    \n",
    "plt.xticks(np.arange(len(header)), header, rotation='vertical'); \n",
    "axes[0].set_ylim(0, 1); \n",
    "axes[0].set_ylabel('Ratio Jet'); axes[1].set_ylabel('Ratio Jet'); axes[2].set_ylabel('Ratio Jet'); \n",
    "axes[0].grid(); axes[1].grid(); axes[2].grid()\n",
    "plt.xlabel('Features'); plt.xlabel('#Sample'); \n",
    "axes[0].set_title('NaN sum per feature and Jet number - Train')\n",
    "axes[1].set_title('NaN sum per feature and Jet number - Validation')\n",
    "axes[2].set_title('NaN sum per feature and Jet number - Test')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.142  3.141  3.141    nan    nan]\n",
      "[-0.0025  0.0253 -0.0173 -0.0072  0.0071]\n",
      "[ 3.142  3.142  3.142    nan    nan]\n",
      "[-0.0096  0.0481 -0.0083 -0.0137 -0.0038]\n",
      "[ 3.142  3.142  3.142    nan    nan]\n",
      "[-0.0154  0.0521 -0.008  -0.0217 -0.0144]\n",
      "568238\n",
      "[ 3.142  3.141  3.141    nan    nan]\n",
      "[-0.0025  0.0253 -0.0173 -0.0072  0.0071]\n",
      "[ 3.142  3.142  3.142    nan    nan]\n",
      "[-0.0096  0.0481 -0.0083 -0.0137 -0.0038]\n",
      "[ 3.142  3.142  3.142    nan    nan]\n",
      "[-0.0154  0.0521 -0.008  -0.0217 -0.0144]\n",
      "568238\n",
      "[ 3.142  3.141  3.141    nan    nan]\n",
      "[-0.0025  0.0253 -0.0173 -0.0072  0.0071]\n",
      "[ 3.142  3.142  3.142    nan    nan]\n",
      "[-0.0096  0.0481 -0.0083 -0.0137 -0.0038]\n",
      "[ 3.142  3.142  3.142    nan    nan]\n",
      "[-0.0154  0.0521 -0.008  -0.0217 -0.0144]\n",
      "568238\n",
      "[ 3.142  3.141  3.141    nan    nan]\n",
      "[-0.0025  0.0253 -0.0173 -0.0072  0.0071]\n",
      "[ 3.142  3.142  3.142    nan    nan]\n",
      "[-0.0096  0.0481 -0.0083 -0.0137 -0.0038]\n",
      "[ 3.142  3.142  3.142    nan    nan]\n",
      "[-0.0154  0.0521 -0.008  -0.0217 -0.0144]\n",
      "568238\n"
     ]
    }
   ],
   "source": [
    "from scripts.ml import augmented_feat_angle\n",
    "\n",
    "def get_augmented(x, id_angle_feat=[15, 18, 20, 25, 28]):\n",
    "    x_aug = augmented_feat_angle(x, id_angle_feat)\n",
    "    return np.concatenate((x, x_aug), axis=1)\n",
    "\n",
    "def get_data_jet(y, x, id_current_jet, id_jet=22):\n",
    "    id_select = (x[:, id_jet] == id_current_jet)\n",
    "    x_jet = x[id_select, :]\n",
    "    y_jet = y[id_select]\n",
    "    jet_keep = np.arange(x.shape[1]) != id_jet  # Remove jet number feature\n",
    "    non_zero = np.std(x_jet, axis=0) != 0  # Remove features that have 0 std (i.e. constant)\n",
    "    no_nan_keep = ~np.any(np.isnan(x_jet), axis=0)  # Remove features with nan\n",
    "    return id_select, y_jet, x_jet[:, np.logical_and(np.logical_and(jet_keep, no_nan_keep), non_zero) ]\n",
    "   \n",
    "models = []\n",
    "for i in range(4):\n",
    "    _, y_train_j, x_train_j = get_data_jet(y_train, get_augmented(x_train), i)\n",
    "    _, y_validation_j, x_validation_j = get_data_jet(y_validation, get_augmented(x_validation), i)\n",
    "    id_test, y_test_j, x_test_j = get_data_jet(y_test, get_augmented(x_test), i)\n",
    "    # Last vector is only 0's for #jet == 0, we remove it\n",
    "   \n",
    "    x_train_j = (x_train_j - np.mean(x_train_j, axis=0))/np.std(x_train_j, axis=0)\n",
    "    x_validation_j = (x_validation_j - np.mean(x_validation_j, axis=0))/np.std(x_validation_j, axis=0)\n",
    "    x_test_j = (x_test_j - np.mean(x_test_j, axis=0))/np.std(x_test_j, axis=0)\n",
    "        \n",
    "    models.append( {'y_train': y_train_j, 'x_train': x_train_j, \n",
    "                    'y_validation': y_validation_j, 'x_validation': x_validation_j,\n",
    "                    'y_test': y_test_j, 'x_test': x_test_j, 'id_test': id_test} )\n",
    "    \n",
    "    print(len(models[i]['id_test']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"ridge_regression\")\n",
    "    \n",
    "def test_ridge_regression(x, y, x_val, y_val, degrees, lambdas):\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_degree = 0\n",
    "    best_lambda = 0\n",
    "    best_rmse_tr = []\n",
    "    best_rmse_te = []\n",
    "    best_weights = []\n",
    "    for degree in degrees:\n",
    "        degree = int(degree)\n",
    "        #lambdas = np.logspace(-7, 2, 20)\n",
    "\n",
    "        # Split sets\n",
    "        #x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "        # Get ploynomial\n",
    "        phi_train = lib.build_poly(x, degree)\n",
    "        phi_test = lib.build_poly(x_val, degree)\n",
    "\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        update_rmse = False\n",
    "\n",
    "        for ind, lambda_ in enumerate(lambdas):\n",
    "\n",
    "            mse_tr, weights = lib.ridge_regression(y, phi_train, lambda_)\n",
    "            mse_te = lib.compute_loss(y_val, phi_test.dot(weights))\n",
    "            rmse_tr.append(np.sqrt(2*mse_tr))\n",
    "            rmse_te.append(np.sqrt(2*mse_te))\n",
    "\n",
    "            print(\"degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "                    d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "            print('train acc : ', lib.accuracy(y, phi_train.dot(weights)))\n",
    "            val_acc = lib.accuracy(y_val, phi_test.dot(weights))\n",
    "            print('validation acc : ', val_acc)\n",
    "\n",
    "            if(val_acc > best_acc):\n",
    "                best_acc = val_acc\n",
    "                best_degree = degree\n",
    "                best_lambda = lambda_\n",
    "                best_weights = weights\n",
    "                update_rmse = True\n",
    "        \n",
    "        if(update_rmse):\n",
    "            best_rmse_tr = rmse_tr\n",
    "            best_rmse_te = rmse_te\n",
    "\n",
    "        # Plot the best obtained results\n",
    "    plot_train_test(best_rmse_tr, best_rmse_te, lambdas, best_degree)\n",
    "\n",
    "    print('Best params for Ridge regression : degree = ',best_degree, ', lambda = ',best_lambda,', accuracy = ', best_acc)\n",
    "    \n",
    "    return best_weights, best_degree, best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Model 1 ************* \n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818405444216\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818392957483\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818355497284\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818367984017\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818367984017\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818367984017\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818367984017\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818367984017\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818367984017\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.81838047075\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.81838047075\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818367984017\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814857776881\n",
      "validation acc :  0.818392957483\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814857776881\n",
      "validation acc :  0.818405444216\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814857776881\n",
      "validation acc :  0.818355497284\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814756909421\n",
      "validation acc :  0.818492851345\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814756909421\n",
      "validation acc :  0.818492851345\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814706475691\n",
      "validation acc :  0.818480364613\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814656041961\n",
      "validation acc :  0.818455391147\n",
      "degree=1, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.738\n",
      "train acc :  0.814807343151\n",
      "validation acc :  0.818343010551\n",
      "degree=1, lambda=0.000, Training RMSE=0.739, Testing RMSE=0.738\n",
      "train acc :  0.81410127093\n",
      "validation acc :  0.818255603421\n",
      "degree=1, lambda=0.000, Training RMSE=0.739, Testing RMSE=0.739\n",
      "train acc :  0.8140508372\n",
      "validation acc :  0.81811824936\n",
      "degree=1, lambda=0.001, Training RMSE=0.740, Testing RMSE=0.740\n",
      "train acc :  0.814656041961\n",
      "validation acc :  0.817980895299\n",
      "degree=1, lambda=0.001, Training RMSE=0.742, Testing RMSE=0.743\n",
      "train acc :  0.81420213839\n",
      "validation acc :  0.816882062808\n",
      "degree=1, lambda=0.001, Training RMSE=0.747, Testing RMSE=0.749\n",
      "train acc :  0.813344764979\n",
      "validation acc :  0.815720796654\n",
      "degree=1, lambda=0.002, Training RMSE=0.755, Testing RMSE=0.758\n",
      "train acc :  0.813143030059\n",
      "validation acc :  0.81504651308\n",
      "degree=1, lambda=0.002, Training RMSE=0.769, Testing RMSE=0.772\n",
      "train acc :  0.812235222917\n",
      "validation acc :  0.813935193857\n",
      "degree=1, lambda=0.003, Training RMSE=0.790, Testing RMSE=0.793\n",
      "train acc :  0.812436957837\n",
      "validation acc :  0.812424299182\n",
      "degree=1, lambda=0.004, Training RMSE=0.821, Testing RMSE=0.824\n",
      "train acc :  0.808755295542\n",
      "validation acc :  0.809215208841\n",
      "degree=1, lambda=0.005, Training RMSE=0.858, Testing RMSE=0.861\n",
      "train acc :  0.802804115392\n",
      "validation acc :  0.805069613536\n",
      "degree=1, lambda=0.007, Training RMSE=0.897, Testing RMSE=0.899\n",
      "train acc :  0.796802501513\n",
      "validation acc :  0.79846413186\n",
      "degree=1, lambda=0.009, Training RMSE=0.931, Testing RMSE=0.932\n",
      "train acc :  0.789590478112\n",
      "validation acc :  0.793094836736\n",
      "degree=1, lambda=0.013, Training RMSE=0.956, Testing RMSE=0.957\n",
      "train acc :  0.786564454307\n",
      "validation acc :  0.788399825186\n",
      "degree=1, lambda=0.017, Training RMSE=0.974, Testing RMSE=0.974\n",
      "train acc :  0.784244502723\n",
      "validation acc :  0.785977399014\n",
      "degree=1, lambda=0.023, Training RMSE=0.985, Testing RMSE=0.985\n",
      "train acc :  0.78227758725\n",
      "validation acc :  0.784491477805\n",
      "degree=1, lambda=0.031, Training RMSE=0.991, Testing RMSE=0.992\n",
      "train acc :  0.78212628606\n",
      "validation acc :  0.78333021165\n",
      "degree=1, lambda=0.041, Training RMSE=0.995, Testing RMSE=0.995\n",
      "train acc :  0.781218478919\n",
      "validation acc :  0.782693388275\n",
      "degree=1, lambda=0.055, Training RMSE=0.997, Testing RMSE=0.997\n",
      "train acc :  0.781016743998\n",
      "validation acc :  0.782306299557\n",
      "degree=1, lambda=0.074, Training RMSE=0.998, Testing RMSE=0.999\n",
      "train acc :  0.780613274158\n",
      "validation acc :  0.782094025098\n",
      "degree=1, lambda=0.100, Training RMSE=0.999, Testing RMSE=0.999\n",
      "train acc :  0.780411539237\n",
      "validation acc :  0.781919210838\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.82675906849\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.826746581757\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.826746581757\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.826734095024\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.826721608291\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.826721608291\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.826721608291\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.826721608291\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.826721608291\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823481944725\n",
      "validation acc :  0.826721608291\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823481944725\n",
      "validation acc :  0.826721608291\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823481944725\n",
      "validation acc :  0.826734095024\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823481944725\n",
      "validation acc :  0.82675906849\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823481944725\n",
      "validation acc :  0.826696634825\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823532378455\n",
      "validation acc :  0.826659174627\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.823431510995\n",
      "validation acc :  0.826696634825\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.605\n",
      "train acc :  0.823179342344\n",
      "validation acc :  0.826609227696\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.605\n",
      "train acc :  0.822675005043\n",
      "validation acc :  0.826659174627\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.605\n",
      "train acc :  0.822775872504\n",
      "validation acc :  0.826421926703\n",
      "degree=2, lambda=0.000, Training RMSE=0.712, Testing RMSE=1.604\n",
      "train acc :  0.822422836393\n",
      "validation acc :  0.825822563526\n",
      "degree=2, lambda=0.000, Training RMSE=0.713, Testing RMSE=1.601\n",
      "train acc :  0.822372402663\n",
      "validation acc :  0.825460448274\n",
      "degree=2, lambda=0.000, Training RMSE=0.714, Testing RMSE=1.588\n",
      "train acc :  0.821565462982\n",
      "validation acc :  0.824823624899\n",
      "degree=2, lambda=0.001, Training RMSE=0.715, Testing RMSE=1.553\n",
      "train acc :  0.819749848699\n",
      "validation acc :  0.823350190423\n",
      "degree=2, lambda=0.001, Training RMSE=0.718, Testing RMSE=1.471\n",
      "train acc :  0.819548113778\n",
      "validation acc :  0.821926702878\n",
      "degree=2, lambda=0.001, Training RMSE=0.721, Testing RMSE=1.308\n",
      "train acc :  0.818287270527\n",
      "validation acc :  0.819991259287\n",
      "degree=2, lambda=0.002, Training RMSE=0.726, Testing RMSE=1.055\n",
      "train acc :  0.815916885213\n",
      "validation acc :  0.818817506399\n",
      "degree=2, lambda=0.002, Training RMSE=0.731, Testing RMSE=0.793\n",
      "train acc :  0.81435343958\n",
      "validation acc :  0.816894549541\n",
      "degree=2, lambda=0.003, Training RMSE=0.739, Testing RMSE=0.786\n",
      "train acc :  0.809310066573\n",
      "validation acc :  0.813148529687\n",
      "degree=2, lambda=0.004, Training RMSE=0.751, Testing RMSE=1.115\n",
      "train acc :  0.803056284043\n",
      "validation acc :  0.804794905413\n",
      "degree=2, lambda=0.005, Training RMSE=0.770, Testing RMSE=1.527\n",
      "train acc :  0.786967924148\n",
      "validation acc :  0.788936754698\n",
      "degree=2, lambda=0.007, Training RMSE=0.795, Testing RMSE=1.876\n",
      "train acc :  0.764524914263\n",
      "validation acc :  0.766560529437\n",
      "degree=2, lambda=0.009, Training RMSE=0.824, Testing RMSE=2.094\n",
      "train acc :  0.748638289288\n",
      "validation acc :  0.751201848036\n",
      "degree=2, lambda=0.013, Training RMSE=0.856, Testing RMSE=2.120\n",
      "train acc :  0.742888844059\n",
      "validation acc :  0.745245676469\n",
      "degree=2, lambda=0.017, Training RMSE=0.889, Testing RMSE=1.942\n",
      "train acc :  0.742787976599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc :  0.745183242805\n",
      "degree=2, lambda=0.023, Training RMSE=0.920, Testing RMSE=1.641\n",
      "train acc :  0.742838410329\n",
      "validation acc :  0.745358057064\n",
      "degree=2, lambda=0.031, Training RMSE=0.945, Testing RMSE=1.349\n",
      "train acc :  0.742838410329\n",
      "validation acc :  0.745358057064\n",
      "degree=2, lambda=0.041, Training RMSE=0.965, Testing RMSE=1.151\n",
      "train acc :  0.742838410329\n",
      "validation acc :  0.745358057064\n",
      "degree=2, lambda=0.055, Training RMSE=0.978, Testing RMSE=1.052\n",
      "train acc :  0.742838410329\n",
      "validation acc :  0.745358057064\n",
      "degree=2, lambda=0.074, Training RMSE=0.987, Testing RMSE=1.014\n",
      "train acc :  0.742838410329\n",
      "validation acc :  0.745358057064\n",
      "degree=2, lambda=0.100, Training RMSE=0.992, Testing RMSE=1.002\n",
      "train acc :  0.742838410329\n",
      "validation acc :  0.745358057064\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.910\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830717362802\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.904\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830679902603\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.901\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830692389336\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.900\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830667415871\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.900\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830642442405\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.902\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830642442405\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.905\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830654929138\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.913\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830667415871\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.926\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830629955672\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.950\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830679902603\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=7.993\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.830654929138\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=8.070\n",
      "train acc :  0.825953197498\n",
      "validation acc :  0.830617468939\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=8.205\n",
      "train acc :  0.826205366149\n",
      "validation acc :  0.830542548542\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=8.439\n",
      "train acc :  0.826306233609\n",
      "validation acc :  0.830492601611\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=8.835\n",
      "train acc :  0.826003631229\n",
      "validation acc :  0.830492601611\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=9.475\n",
      "train acc :  0.825953197498\n",
      "validation acc :  0.830430167947\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=10.458\n",
      "train acc :  0.825902763768\n",
      "validation acc :  0.830430167947\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=11.888\n",
      "train acc :  0.825499293928\n",
      "validation acc :  0.830317787351\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=13.901\n",
      "train acc :  0.825247125277\n",
      "validation acc :  0.830430167947\n",
      "degree=3, lambda=0.000, Training RMSE=0.702, Testing RMSE=16.742\n",
      "train acc :  0.824843655437\n",
      "validation acc :  0.829805831304\n",
      "degree=3, lambda=0.000, Training RMSE=0.703, Testing RMSE=20.830\n",
      "train acc :  0.824238450676\n",
      "validation acc :  0.829094087532\n",
      "degree=3, lambda=0.000, Training RMSE=0.703, Testing RMSE=26.790\n",
      "train acc :  0.824188016946\n",
      "validation acc :  0.828419803958\n",
      "degree=3, lambda=0.001, Training RMSE=0.705, Testing RMSE=35.422\n",
      "train acc :  0.823935848295\n",
      "validation acc :  0.827782980583\n",
      "degree=3, lambda=0.001, Training RMSE=0.707, Testing RMSE=47.574\n",
      "train acc :  0.823582812185\n",
      "validation acc :  0.826696634825\n",
      "degree=3, lambda=0.001, Training RMSE=0.709, Testing RMSE=63.737\n",
      "train acc :  0.822422836393\n",
      "validation acc :  0.824848598364\n",
      "degree=3, lambda=0.002, Training RMSE=0.713, Testing RMSE=83.064\n",
      "train acc :  0.822019366552\n",
      "validation acc :  0.823612411812\n",
      "degree=3, lambda=0.002, Training RMSE=0.717, Testing RMSE=102.060\n",
      "train acc :  0.819951583619\n",
      "validation acc :  0.822326278329\n",
      "degree=3, lambda=0.003, Training RMSE=0.726, Testing RMSE=114.718\n",
      "train acc :  0.816169053863\n",
      "validation acc :  0.819429356309\n",
      "degree=3, lambda=0.004, Training RMSE=0.739, Testing RMSE=115.774\n",
      "train acc :  0.809663102683\n",
      "validation acc :  0.81298620216\n",
      "degree=3, lambda=0.005, Training RMSE=0.760, Testing RMSE=104.476\n",
      "train acc :  0.795390357071\n",
      "validation acc :  0.798938627708\n",
      "degree=3, lambda=0.007, Training RMSE=0.786, Testing RMSE=84.163\n",
      "train acc :  0.774006455517\n",
      "validation acc :  0.776425048386\n",
      "degree=3, lambda=0.009, Training RMSE=0.816, Testing RMSE=58.648\n",
      "train acc :  0.758371999193\n",
      "validation acc :  0.759031029531\n",
      "degree=3, lambda=0.013, Training RMSE=0.848, Testing RMSE=30.714\n",
      "train acc :  0.748688723018\n",
      "validation acc :  0.751014547044\n",
      "degree=3, lambda=0.017, Training RMSE=0.879, Testing RMSE=3.649\n",
      "train acc :  0.7432923139\n",
      "validation acc :  0.74573265905\n",
      "degree=3, lambda=0.023, Training RMSE=0.910, Testing RMSE=18.315\n",
      "train acc :  0.743040145249\n",
      "validation acc :  0.745370543797\n",
      "degree=3, lambda=0.031, Training RMSE=0.936, Testing RMSE=30.610\n",
      "train acc :  0.742989711519\n",
      "validation acc :  0.745358057064\n",
      "degree=3, lambda=0.041, Training RMSE=0.956, Testing RMSE=32.683\n",
      "train acc :  0.743040145249\n",
      "validation acc :  0.745320596866\n",
      "degree=3, lambda=0.055, Training RMSE=0.971, Testing RMSE=27.793\n",
      "train acc :  0.742989711519\n",
      "validation acc :  0.745308110133\n",
      "degree=3, lambda=0.074, Training RMSE=0.982, Testing RMSE=20.429\n",
      "train acc :  0.743040145249\n",
      "validation acc :  0.7452956234\n",
      "degree=3, lambda=0.100, Training RMSE=0.989, Testing RMSE=13.629\n",
      "train acc :  0.742737542869\n",
      "validation acc :  0.74520821627\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=207.286\n",
      "train acc :  0.831853943918\n",
      "validation acc :  0.828981706936\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=211.182\n",
      "train acc :  0.831954811378\n",
      "validation acc :  0.831553973903\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=212.349\n",
      "train acc :  0.831954811378\n",
      "validation acc :  0.833277143036\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=212.202\n",
      "train acc :  0.831853943918\n",
      "validation acc :  0.834213647999\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=211.582\n",
      "train acc :  0.831853943918\n",
      "validation acc :  0.834725604046\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=210.763\n",
      "train acc :  0.831803510188\n",
      "validation acc :  0.834750577511\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=209.675\n",
      "train acc :  0.831803510188\n",
      "validation acc :  0.834663170381\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=208.023\n",
      "train acc :  0.831753076458\n",
      "validation acc :  0.834688143847\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=205.279\n",
      "train acc :  0.831753076458\n",
      "validation acc :  0.834738090779\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=200.585\n",
      "train acc :  0.831601775267\n",
      "validation acc :  0.834775550977\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=192.617\n",
      "train acc :  0.831601775267\n",
      "validation acc :  0.834937878504\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=179.536\n",
      "train acc :  0.831500907807\n",
      "validation acc :  0.835025285634\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=159.330\n",
      "train acc :  0.831248739157\n",
      "validation acc :  0.834912905038\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=130.863\n",
      "train acc :  0.831450474077\n",
      "validation acc :  0.834775550977\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=95.332\n",
      "train acc :  0.831097437967\n",
      "validation acc :  0.834475869389\n",
      "degree=4, lambda=0.000, Training RMSE=0.697, Testing RMSE=56.352\n",
      "train acc :  0.830593100666\n",
      "validation acc :  0.834326028595\n",
      "degree=4, lambda=0.000, Training RMSE=0.698, Testing RMSE=17.454\n",
      "train acc :  0.830240064555\n",
      "validation acc :  0.834201161266\n",
      "degree=4, lambda=0.000, Training RMSE=0.698, Testing RMSE=24.765\n",
      "train acc :  0.829987895905\n",
      "validation acc :  0.834101267403\n",
      "degree=4, lambda=0.000, Training RMSE=0.698, Testing RMSE=73.800\n",
      "train acc :  0.829836594715\n",
      "validation acc :  0.834001373541\n",
      "degree=4, lambda=0.000, Training RMSE=0.698, Testing RMSE=144.775\n",
      "train acc :  0.829735727254\n",
      "validation acc :  0.833851532746\n",
      "degree=4, lambda=0.000, Training RMSE=0.699, Testing RMSE=255.876\n",
      "train acc :  0.829584426064\n",
      "validation acc :  0.833464444028\n",
      "degree=4, lambda=0.000, Training RMSE=0.699, Testing RMSE=424.516\n",
      "train acc :  0.829029655033\n",
      "validation acc :  0.832665293126\n",
      "degree=4, lambda=0.001, Training RMSE=0.701, Testing RMSE=647.232\n",
      "train acc :  0.828525317732\n",
      "validation acc :  0.831566460636\n",
      "degree=4, lambda=0.001, Training RMSE=0.702, Testing RMSE=868.393\n",
      "train acc :  0.827617510591\n",
      "validation acc :  0.830517575076\n",
      "degree=4, lambda=0.001, Training RMSE=0.705, Testing RMSE=964.575\n",
      "train acc :  0.826104498689\n",
      "validation acc :  0.828132609103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=4, lambda=0.002, Training RMSE=0.708, Testing RMSE=782.296\n",
      "train acc :  0.824994956627\n",
      "validation acc :  0.825959917588\n",
      "degree=4, lambda=0.002, Training RMSE=0.713, Testing RMSE=230.048\n",
      "train acc :  0.823179342344\n",
      "validation acc :  0.822975588437\n",
      "degree=4, lambda=0.003, Training RMSE=0.722, Testing RMSE=638.945\n",
      "train acc :  0.817228162195\n",
      "validation acc :  0.819104701255\n",
      "degree=4, lambda=0.004, Training RMSE=0.735, Testing RMSE=1620.848\n",
      "train acc :  0.810066572524\n",
      "validation acc :  0.811700068677\n",
      "degree=4, lambda=0.005, Training RMSE=0.752, Testing RMSE=2444.788\n",
      "train acc :  0.797861609845\n",
      "validation acc :  0.801848036461\n",
      "degree=4, lambda=0.007, Training RMSE=0.772, Testing RMSE=2896.273\n",
      "train acc :  0.783891466613\n",
      "validation acc :  0.787450833489\n",
      "degree=4, lambda=0.009, Training RMSE=0.793, Testing RMSE=2894.754\n",
      "train acc :  0.772392576155\n",
      "validation acc :  0.773790347756\n",
      "degree=4, lambda=0.013, Training RMSE=0.813, Testing RMSE=2493.705\n",
      "train acc :  0.763617107121\n",
      "validation acc :  0.764175563464\n",
      "degree=4, lambda=0.017, Training RMSE=0.834, Testing RMSE=1828.414\n",
      "train acc :  0.757312890861\n",
      "validation acc :  0.757457701192\n",
      "degree=4, lambda=0.023, Training RMSE=0.858, Testing RMSE=1050.403\n",
      "train acc :  0.751714746823\n",
      "validation acc :  0.752762689642\n",
      "degree=4, lambda=0.031, Training RMSE=0.884, Testing RMSE=284.385\n",
      "train acc :  0.745460964293\n",
      "validation acc :  0.748642067803\n",
      "degree=4, lambda=0.041, Training RMSE=0.909, Testing RMSE=389.169\n",
      "train acc :  0.74349404882\n",
      "validation acc :  0.745520384591\n",
      "degree=4, lambda=0.055, Training RMSE=0.932, Testing RMSE=928.219\n",
      "train acc :  0.7432923139\n",
      "validation acc :  0.745358057064\n",
      "degree=4, lambda=0.074, Training RMSE=0.952, Testing RMSE=1303.561\n",
      "train acc :  0.74334274763\n",
      "validation acc :  0.745532871324\n",
      "degree=4, lambda=0.100, Training RMSE=0.967, Testing RMSE=1483.172\n",
      "train acc :  0.74344361509\n",
      "validation acc :  0.745620278454\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=369160.326\n",
      "train acc :  0.836796449465\n",
      "validation acc :  0.824723731036\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=370001.621\n",
      "train acc :  0.836846883195\n",
      "validation acc :  0.834038833739\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=371164.740\n",
      "train acc :  0.836947750656\n",
      "validation acc :  0.835462321284\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=372593.223\n",
      "train acc :  0.836947750656\n",
      "validation acc :  0.837085596554\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=374103.966\n",
      "train acc :  0.836846883195\n",
      "validation acc :  0.838171942311\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=375465.530\n",
      "train acc :  0.836846883195\n",
      "validation acc :  0.838521570831\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=376532.057\n",
      "train acc :  0.836947750656\n",
      "validation acc :  0.838821252419\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=377291.117\n",
      "train acc :  0.836796449465\n",
      "validation acc :  0.838608977961\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=377816.214\n",
      "train acc :  0.836493847085\n",
      "validation acc :  0.838584004495\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=378201.488\n",
      "train acc :  0.836443413355\n",
      "validation acc :  0.838396703503\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=378520.294\n",
      "train acc :  0.836292112165\n",
      "validation acc :  0.838321783105\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=378801.598\n",
      "train acc :  0.836140810974\n",
      "validation acc :  0.838334269838\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=379013.059\n",
      "train acc :  0.836342545895\n",
      "validation acc :  0.838321783105\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=379059.878\n",
      "train acc :  0.835838208594\n",
      "validation acc :  0.838146968846\n",
      "degree=5, lambda=0.000, Training RMSE=0.692, Testing RMSE=378798.344\n",
      "train acc :  0.835888642324\n",
      "validation acc :  0.83820940251\n",
      "degree=5, lambda=0.000, Training RMSE=0.693, Testing RMSE=378033.927\n",
      "train acc :  0.835636473674\n",
      "validation acc :  0.837972154586\n",
      "degree=5, lambda=0.000, Training RMSE=0.693, Testing RMSE=376484.756\n",
      "train acc :  0.835283437563\n",
      "validation acc :  0.837984641319\n",
      "degree=5, lambda=0.000, Training RMSE=0.693, Testing RMSE=373725.383\n",
      "train acc :  0.835182570103\n",
      "validation acc :  0.838022101517\n",
      "degree=5, lambda=0.000, Training RMSE=0.693, Testing RMSE=369108.156\n",
      "train acc :  0.835182570103\n",
      "validation acc :  0.837635012799\n",
      "degree=5, lambda=0.000, Training RMSE=0.693, Testing RMSE=361581.537\n",
      "train acc :  0.835384305023\n",
      "validation acc :  0.837460198539\n",
      "degree=5, lambda=0.000, Training RMSE=0.693, Testing RMSE=349274.859\n",
      "train acc :  0.834627799072\n",
      "validation acc :  0.837060623088\n",
      "degree=5, lambda=0.000, Training RMSE=0.694, Testing RMSE=328886.308\n",
      "train acc :  0.834022594311\n",
      "validation acc :  0.836511206843\n",
      "degree=5, lambda=0.001, Training RMSE=0.695, Testing RMSE=295445.114\n",
      "train acc :  0.83346782328\n",
      "validation acc :  0.835362427421\n",
      "degree=5, lambda=0.001, Training RMSE=0.697, Testing RMSE=243764.382\n",
      "train acc :  0.831652208997\n",
      "validation acc :  0.833614284822\n",
      "degree=5, lambda=0.001, Training RMSE=0.701, Testing RMSE=172502.529\n",
      "train acc :  0.828777486383\n",
      "validation acc :  0.831042017856\n",
      "degree=5, lambda=0.002, Training RMSE=0.706, Testing RMSE=88395.691\n",
      "train acc :  0.826104498689\n",
      "validation acc :  0.826771555223\n",
      "degree=5, lambda=0.002, Training RMSE=0.713, Testing RMSE=6173.520\n",
      "train acc :  0.822422836393\n",
      "validation acc :  0.822451145658\n",
      "degree=5, lambda=0.003, Training RMSE=0.721, Testing RMSE=55734.090\n",
      "train acc :  0.816017752673\n",
      "validation acc :  0.817568833115\n",
      "degree=5, lambda=0.004, Training RMSE=0.732, Testing RMSE=81315.159\n",
      "train acc :  0.809562235223\n",
      "validation acc :  0.812049697197\n",
      "degree=5, lambda=0.005, Training RMSE=0.745, Testing RMSE=64905.288\n",
      "train acc :  0.804317127295\n",
      "validation acc :  0.804682524817\n",
      "degree=5, lambda=0.007, Training RMSE=0.760, Testing RMSE=15764.078\n",
      "train acc :  0.79503732096\n",
      "validation acc :  0.798414184929\n",
      "degree=5, lambda=0.009, Training RMSE=0.778, Testing RMSE=48394.958\n",
      "train acc :  0.786009683276\n",
      "validation acc :  0.789348816882\n",
      "degree=5, lambda=0.013, Training RMSE=0.799, Testing RMSE=112860.737\n",
      "train acc :  0.77556990115\n",
      "validation acc :  0.777836049198\n",
      "degree=5, lambda=0.017, Training RMSE=0.823, Testing RMSE=169460.557\n",
      "train acc :  0.767601371797\n",
      "validation acc :  0.767047512018\n",
      "degree=5, lambda=0.023, Training RMSE=0.849, Testing RMSE=212009.906\n",
      "train acc :  0.758926770224\n",
      "validation acc :  0.759542985578\n",
      "degree=5, lambda=0.031, Training RMSE=0.877, Testing RMSE=232354.784\n",
      "train acc :  0.753580794836\n",
      "validation acc :  0.753998876194\n",
      "degree=5, lambda=0.041, Training RMSE=0.903, Testing RMSE=223437.223\n",
      "train acc :  0.747730482146\n",
      "validation acc :  0.749303864644\n",
      "degree=5, lambda=0.055, Training RMSE=0.926, Testing RMSE=187148.507\n",
      "train acc :  0.743998386121\n",
      "validation acc :  0.745745145783\n",
      "degree=5, lambda=0.074, Training RMSE=0.945, Testing RMSE=136322.608\n",
      "train acc :  0.743191446439\n",
      "validation acc :  0.744833614285\n",
      "degree=5, lambda=0.100, Training RMSE=0.960, Testing RMSE=86421.246\n",
      "train acc :  0.741577567077\n",
      "validation acc :  0.742998064556\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28238090.731\n",
      "train acc :  0.838057292717\n",
      "validation acc :  0.665055878129\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28234435.842\n",
      "train acc :  0.838057292717\n",
      "validation acc :  0.834500842854\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28228702.132\n",
      "train acc :  0.837855557797\n",
      "validation acc :  0.835424861085\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28219757.697\n",
      "train acc :  0.837653822877\n",
      "validation acc :  0.83189111569\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28207099.243\n",
      "train acc :  0.837805124067\n",
      "validation acc :  0.832827620653\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28190996.071\n",
      "train acc :  0.838006858987\n",
      "validation acc :  0.835524754948\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28172361.067\n",
      "train acc :  0.837855557797\n",
      "validation acc :  0.837759880127\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28151786.383\n",
      "train acc :  0.837905991527\n",
      "validation acc :  0.838658924892\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28128541.794\n",
      "train acc :  0.837653822877\n",
      "validation acc :  0.839320721733\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28100716.907\n",
      "train acc :  0.837704256607\n",
      "validation acc :  0.83922082787\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28065039.961\n",
      "train acc :  0.837754690337\n",
      "validation acc :  0.838971093213\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=28015428.154\n",
      "train acc :  0.837653822877\n",
      "validation acc :  0.838796278954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=27940605.330\n",
      "train acc :  0.837603389147\n",
      "validation acc :  0.838858712618\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=27821893.857\n",
      "train acc :  0.837300786766\n",
      "validation acc :  0.838683898358\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=27631602.162\n",
      "train acc :  0.837099051846\n",
      "validation acc :  0.838746332022\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=27331853.844\n",
      "train acc :  0.836998184386\n",
      "validation acc :  0.838658924892\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=26870782.464\n",
      "train acc :  0.837099051846\n",
      "validation acc :  0.838746332022\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=26167192.306\n",
      "train acc :  0.836846883195\n",
      "validation acc :  0.838683898358\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=25078055.557\n",
      "train acc :  0.836897316926\n",
      "validation acc :  0.838883686084\n",
      "degree=6, lambda=0.000, Training RMSE=0.690, Testing RMSE=23365413.154\n",
      "train acc :  0.836544280815\n",
      "validation acc :  0.838796278954\n",
      "degree=6, lambda=0.000, Training RMSE=0.691, Testing RMSE=20702584.906\n",
      "train acc :  0.836292112165\n",
      "validation acc :  0.838246862708\n",
      "degree=6, lambda=0.000, Training RMSE=0.692, Testing RMSE=16774784.999\n",
      "train acc :  0.835434738753\n",
      "validation acc :  0.8375975526\n",
      "degree=6, lambda=0.001, Training RMSE=0.693, Testing RMSE=11523121.874\n",
      "train acc :  0.834274762961\n",
      "validation acc :  0.836049197727\n",
      "degree=6, lambda=0.001, Training RMSE=0.696, Testing RMSE=5456095.280\n",
      "train acc :  0.832307847488\n",
      "validation acc :  0.834276081663\n",
      "degree=6, lambda=0.001, Training RMSE=0.699, Testing RMSE=292399.102\n",
      "train acc :  0.829533992334\n",
      "validation acc :  0.831141911719\n",
      "degree=6, lambda=0.002, Training RMSE=0.704, Testing RMSE=4409660.164\n",
      "train acc :  0.826054064959\n",
      "validation acc :  0.82702128988\n",
      "degree=6, lambda=0.002, Training RMSE=0.710, Testing RMSE=6025444.962\n",
      "train acc :  0.822775872504\n",
      "validation acc :  0.822251357932\n",
      "degree=6, lambda=0.003, Training RMSE=0.718, Testing RMSE=4995674.532\n",
      "train acc :  0.816269921323\n",
      "validation acc :  0.816832115877\n",
      "degree=6, lambda=0.004, Training RMSE=0.727, Testing RMSE=1857578.228\n",
      "train acc :  0.811630018156\n",
      "validation acc :  0.811063245302\n",
      "degree=6, lambda=0.005, Training RMSE=0.737, Testing RMSE=2333016.054\n",
      "train acc :  0.805779705467\n",
      "validation acc :  0.805181994131\n",
      "degree=6, lambda=0.007, Training RMSE=0.750, Testing RMSE=6299503.831\n",
      "train acc :  0.796852935243\n",
      "validation acc :  0.79947555722\n",
      "degree=6, lambda=0.009, Training RMSE=0.765, Testing RMSE=8953558.028\n",
      "train acc :  0.789590478112\n",
      "validation acc :  0.791896110383\n",
      "degree=6, lambda=0.013, Training RMSE=0.783, Testing RMSE=9711221.555\n",
      "train acc :  0.779301997176\n",
      "validation acc :  0.781681962914\n",
      "degree=6, lambda=0.017, Training RMSE=0.804, Testing RMSE=8607055.165\n",
      "train acc :  0.770829130522\n",
      "validation acc :  0.77135543485\n",
      "degree=6, lambda=0.023, Training RMSE=0.824, Testing RMSE=6193273.876\n",
      "train acc :  0.762104095219\n",
      "validation acc :  0.761902978086\n",
      "degree=6, lambda=0.031, Training RMSE=0.845, Testing RMSE=3209712.904\n",
      "train acc :  0.755446842848\n",
      "validation acc :  0.755447337204\n",
      "degree=6, lambda=0.041, Training RMSE=0.865, Testing RMSE=242738.231\n",
      "train acc :  0.752420819044\n",
      "validation acc :  0.752213273397\n",
      "degree=6, lambda=0.055, Training RMSE=0.887, Testing RMSE=2352320.421\n",
      "train acc :  0.750958240871\n",
      "validation acc :  0.750977086845\n",
      "degree=6, lambda=0.074, Training RMSE=0.908, Testing RMSE=4345009.226\n",
      "train acc :  0.75015130119\n",
      "validation acc :  0.750277829806\n",
      "degree=6, lambda=0.100, Training RMSE=0.928, Testing RMSE=5564289.049\n",
      "train acc :  0.744855759532\n",
      "validation acc :  0.746631703815\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=607027476.011\n",
      "train acc :  0.842041557394\n",
      "validation acc :  0.6230754823\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=607587207.505\n",
      "train acc :  0.841688521283\n",
      "validation acc :  0.658475369919\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=608380949.393\n",
      "train acc :  0.841890256203\n",
      "validation acc :  0.826309546107\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=609361533.698\n",
      "train acc :  0.841587653823\n",
      "validation acc :  0.823774739339\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=610395641.006\n",
      "train acc :  0.841537220093\n",
      "validation acc :  0.838284322907\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=611333272.223\n",
      "train acc :  0.841587653823\n",
      "validation acc :  0.841318598989\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=612097399.740\n",
      "train acc :  0.841486786363\n",
      "validation acc :  0.841643254043\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=612656372.718\n",
      "train acc :  0.841537220093\n",
      "validation acc :  0.841755634638\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=612948008.388\n",
      "train acc :  0.841436352633\n",
      "validation acc :  0.841643254043\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=612820021.920\n",
      "train acc :  0.841436352633\n",
      "validation acc :  0.841580820378\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=611906635.959\n",
      "train acc :  0.841537220093\n",
      "validation acc :  0.841593307111\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=609537704.174\n",
      "train acc :  0.841537220093\n",
      "validation acc :  0.841618280577\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=604631359.743\n",
      "train acc :  0.841587653823\n",
      "validation acc :  0.841518386714\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=595555725.713\n",
      "train acc :  0.841738955013\n",
      "validation acc :  0.841668227508\n",
      "degree=7, lambda=0.000, Training RMSE=0.685, Testing RMSE=579882092.092\n",
      "train acc :  0.841688521283\n",
      "validation acc :  0.841668227508\n",
      "degree=7, lambda=0.000, Training RMSE=0.686, Testing RMSE=553843538.975\n",
      "train acc :  0.841335485172\n",
      "validation acc :  0.84145595305\n",
      "degree=7, lambda=0.000, Training RMSE=0.686, Testing RMSE=511214884.330\n",
      "train acc :  0.841184183982\n",
      "validation acc :  0.841605793844\n",
      "degree=7, lambda=0.000, Training RMSE=0.686, Testing RMSE=441916990.023\n",
      "train acc :  0.841184183982\n",
      "validation acc :  0.841480926516\n",
      "degree=7, lambda=0.000, Training RMSE=0.686, Testing RMSE=331664948.824\n",
      "train acc :  0.840629412951\n",
      "validation acc :  0.841243678591\n",
      "degree=7, lambda=0.000, Training RMSE=0.686, Testing RMSE=165360144.504\n",
      "train acc :  0.840276376841\n",
      "validation acc :  0.840919023537\n",
      "degree=7, lambda=0.000, Training RMSE=0.686, Testing RMSE=62177882.878\n",
      "train acc :  0.840528545491\n",
      "validation acc :  0.840332147094\n",
      "degree=7, lambda=0.000, Training RMSE=0.687, Testing RMSE=328626778.752\n",
      "train acc :  0.838359895098\n",
      "validation acc :  0.839470562527\n",
      "degree=7, lambda=0.001, Training RMSE=0.689, Testing RMSE=574882170.556\n",
      "train acc :  0.837603389147\n",
      "validation acc :  0.837897234189\n",
      "degree=7, lambda=0.001, Training RMSE=0.692, Testing RMSE=726657251.686\n",
      "train acc :  0.834577365342\n",
      "validation acc :  0.835437347818\n",
      "degree=7, lambda=0.001, Training RMSE=0.696, Testing RMSE=736693087.466\n",
      "train acc :  0.830895703046\n",
      "validation acc :  0.831741274895\n",
      "degree=7, lambda=0.002, Training RMSE=0.702, Testing RMSE=605347614.410\n",
      "train acc :  0.82681057091\n",
      "validation acc :  0.827171130674\n",
      "degree=7, lambda=0.002, Training RMSE=0.709, Testing RMSE=372453671.981\n",
      "train acc :  0.822321968933\n",
      "validation acc :  0.82188924268\n",
      "degree=7, lambda=0.003, Training RMSE=0.717, Testing RMSE=107449129.567\n",
      "train acc :  0.816269921323\n",
      "validation acc :  0.816295186364\n",
      "degree=7, lambda=0.004, Training RMSE=0.726, Testing RMSE=105893300.764\n",
      "train acc :  0.811377849506\n",
      "validation acc :  0.810825997378\n",
      "degree=7, lambda=0.005, Training RMSE=0.737, Testing RMSE=204888366.872\n",
      "train acc :  0.804922332056\n",
      "validation acc :  0.805194480864\n",
      "degree=7, lambda=0.007, Training RMSE=0.749, Testing RMSE=176091287.335\n",
      "train acc :  0.798164212225\n",
      "validation acc :  0.800149840794\n",
      "degree=7, lambda=0.009, Training RMSE=0.762, Testing RMSE=50312379.123\n",
      "train acc :  0.791708694775\n",
      "validation acc :  0.794430917151\n",
      "degree=7, lambda=0.013, Training RMSE=0.778, Testing RMSE=121505677.240\n",
      "train acc :  0.783841032883\n",
      "validation acc :  0.788175063995\n",
      "degree=7, lambda=0.017, Training RMSE=0.797, Testing RMSE=291663260.768\n",
      "train acc :  0.777788985273\n",
      "validation acc :  0.780295935568\n",
      "degree=7, lambda=0.023, Training RMSE=0.816, Testing RMSE=426314888.089\n",
      "train acc :  0.770929997983\n",
      "validation acc :  0.771779983767\n",
      "degree=7, lambda=0.031, Training RMSE=0.836, Testing RMSE=501472561.051\n",
      "train acc :  0.765735323785\n",
      "validation acc :  0.76512455516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=7, lambda=0.041, Training RMSE=0.857, Testing RMSE=498704861.252\n",
      "train acc :  0.760742384507\n",
      "validation acc :  0.760729225198\n",
      "degree=7, lambda=0.055, Training RMSE=0.880, Testing RMSE=414381641.819\n",
      "train acc :  0.758018963083\n",
      "validation acc :  0.757769869514\n",
      "degree=7, lambda=0.074, Training RMSE=0.903, Testing RMSE=272911193.347\n",
      "train acc :  0.755295541658\n",
      "validation acc :  0.755534744334\n",
      "degree=7, lambda=0.100, Training RMSE=0.924, Testing RMSE=118376236.554\n",
      "train acc :  0.74974783135\n",
      "validation acc :  0.752875070238\n",
      "degree=8, lambda=0.000, Training RMSE=0.686, Testing RMSE=117175668652.612\n",
      "train acc :  0.842041557394\n",
      "validation acc :  0.52015983018\n",
      "degree=8, lambda=0.000, Training RMSE=0.687, Testing RMSE=117199757002.215\n",
      "train acc :  0.841991123664\n",
      "validation acc :  0.499132172067\n",
      "degree=8, lambda=0.000, Training RMSE=0.687, Testing RMSE=117204156901.204\n",
      "train acc :  0.842495460964\n",
      "validation acc :  0.494000124867\n",
      "degree=8, lambda=0.000, Training RMSE=0.684, Testing RMSE=117224115812.004\n",
      "train acc :  0.842445027234\n",
      "validation acc :  0.536817131797\n",
      "degree=8, lambda=0.000, Training RMSE=0.684, Testing RMSE=117250924610.212\n",
      "train acc :  0.843302400646\n",
      "validation acc :  0.513916463757\n",
      "degree=8, lambda=0.000, Training RMSE=0.683, Testing RMSE=117291509850.979\n",
      "train acc :  0.843050231995\n",
      "validation acc :  0.622163950802\n",
      "degree=8, lambda=0.000, Training RMSE=0.683, Testing RMSE=117337575251.045\n",
      "train acc :  0.843151099455\n",
      "validation acc :  0.595692077168\n",
      "degree=8, lambda=0.000, Training RMSE=0.683, Testing RMSE=117384879267.696\n",
      "train acc :  0.842949364535\n",
      "validation acc :  0.833888992945\n",
      "degree=8, lambda=0.000, Training RMSE=0.683, Testing RMSE=117422887192.730\n",
      "train acc :  0.843050231995\n",
      "validation acc :  0.837497658738\n",
      "degree=8, lambda=0.000, Training RMSE=0.683, Testing RMSE=117425762669.478\n",
      "train acc :  0.842999798265\n",
      "validation acc :  0.841930448898\n",
      "degree=8, lambda=0.000, Training RMSE=0.683, Testing RMSE=117362639680.699\n",
      "train acc :  0.842697195885\n",
      "validation acc :  0.842417431479\n",
      "degree=8, lambda=0.000, Training RMSE=0.683, Testing RMSE=117187433686.748\n",
      "train acc :  0.842848497075\n",
      "validation acc :  0.842292564151\n",
      "degree=8, lambda=0.000, Training RMSE=0.683, Testing RMSE=116826864984.548\n",
      "train acc :  0.842747629615\n",
      "validation acc :  0.842130236624\n",
      "degree=8, lambda=0.000, Training RMSE=0.683, Testing RMSE=116162493577.923\n",
      "train acc :  0.842747629615\n",
      "validation acc :  0.842217643754\n",
      "degree=8, lambda=0.000, Training RMSE=0.684, Testing RMSE=114974896781.530\n",
      "train acc :  0.842394593504\n",
      "validation acc :  0.841992882562\n",
      "degree=8, lambda=0.000, Training RMSE=0.684, Testing RMSE=112879727696.974\n",
      "train acc :  0.842545894694\n",
      "validation acc :  0.841855528501\n",
      "degree=8, lambda=0.000, Training RMSE=0.684, Testing RMSE=109285283627.650\n",
      "train acc :  0.842293726044\n",
      "validation acc :  0.841980395829\n",
      "degree=8, lambda=0.000, Training RMSE=0.684, Testing RMSE=103424909273.769\n",
      "train acc :  0.842243292314\n",
      "validation acc :  0.841693200974\n",
      "degree=8, lambda=0.000, Training RMSE=0.684, Testing RMSE=94666784037.504\n",
      "train acc :  0.841839822473\n",
      "validation acc :  0.841618280577\n",
      "degree=8, lambda=0.000, Training RMSE=0.684, Testing RMSE=83290293005.206\n",
      "train acc :  0.841587653823\n",
      "validation acc :  0.841406006118\n",
      "degree=8, lambda=0.000, Training RMSE=0.685, Testing RMSE=71421027916.720\n",
      "train acc :  0.840679846681\n",
      "validation acc :  0.840619341949\n",
      "degree=8, lambda=0.000, Training RMSE=0.686, Testing RMSE=62845686910.330\n",
      "train acc :  0.839419003429\n",
      "validation acc :  0.839982518574\n",
      "degree=8, lambda=0.001, Training RMSE=0.688, Testing RMSE=60666692749.760\n",
      "train acc :  0.837401654226\n",
      "validation acc :  0.838184429044\n",
      "degree=8, lambda=0.001, Training RMSE=0.691, Testing RMSE=64717358136.154\n",
      "train acc :  0.834375630422\n",
      "validation acc :  0.835836923269\n",
      "degree=8, lambda=0.001, Training RMSE=0.695, Testing RMSE=71554247462.191\n",
      "train acc :  0.830441799475\n",
      "validation acc :  0.832440531935\n",
      "degree=8, lambda=0.002, Training RMSE=0.701, Testing RMSE=76520952408.465\n",
      "train acc :  0.827920112972\n",
      "validation acc :  0.827832927515\n",
      "degree=8, lambda=0.002, Training RMSE=0.708, Testing RMSE=75624470835.081\n",
      "train acc :  0.823784547105\n",
      "validation acc :  0.823525004683\n",
      "degree=8, lambda=0.003, Training RMSE=0.715, Testing RMSE=66892206125.396\n",
      "train acc :  0.818236836796\n",
      "validation acc :  0.818143222826\n",
      "degree=8, lambda=0.004, Training RMSE=0.724, Testing RMSE=51278931030.712\n",
      "train acc :  0.813143030059\n",
      "validation acc :  0.812773927702\n",
      "degree=8, lambda=0.005, Training RMSE=0.734, Testing RMSE=32283553997.473\n",
      "train acc :  0.806737946339\n",
      "validation acc :  0.806555534744\n",
      "degree=8, lambda=0.007, Training RMSE=0.747, Testing RMSE=14396596302.194\n",
      "train acc :  0.799677224127\n",
      "validation acc :  0.800511956047\n",
      "degree=8, lambda=0.009, Training RMSE=0.761, Testing RMSE=1501254105.404\n",
      "train acc :  0.792717369377\n",
      "validation acc :  0.795841917962\n",
      "degree=8, lambda=0.013, Training RMSE=0.775, Testing RMSE=4188256736.755\n",
      "train acc :  0.787875731289\n",
      "validation acc :  0.789985640257\n",
      "degree=8, lambda=0.017, Training RMSE=0.789, Testing RMSE=2767199692.449\n",
      "train acc :  0.780260238047\n",
      "validation acc :  0.783779734033\n",
      "degree=8, lambda=0.023, Training RMSE=0.804, Testing RMSE=3435085158.431\n",
      "train acc :  0.774208190438\n",
      "validation acc :  0.777061871761\n",
      "degree=8, lambda=0.031, Training RMSE=0.819, Testing RMSE=11288271078.046\n",
      "train acc :  0.76941698608\n",
      "validation acc :  0.770206655429\n",
      "degree=8, lambda=0.041, Training RMSE=0.835, Testing RMSE=18644218281.764\n",
      "train acc :  0.763818842042\n",
      "validation acc :  0.763214085035\n",
      "degree=8, lambda=0.055, Training RMSE=0.851, Testing RMSE=24690974124.838\n",
      "train acc :  0.758321565463\n",
      "validation acc :  0.757757382781\n",
      "degree=8, lambda=0.074, Training RMSE=0.866, Testing RMSE=29097757921.735\n",
      "train acc :  0.753883397216\n",
      "validation acc :  0.754323531248\n",
      "degree=8, lambda=0.100, Training RMSE=0.881, Testing RMSE=31294664626.354\n",
      "train acc :  0.751311276982\n",
      "validation acc :  0.751638883686\n",
      "degree=9, lambda=0.000, Training RMSE=0.820, Testing RMSE=25269626168176.203\n",
      "train acc :  0.811730885616\n",
      "validation acc :  0.495948055191\n",
      "degree=9, lambda=0.000, Training RMSE=1.003, Testing RMSE=25256626932037.246\n",
      "train acc :  0.789943514222\n",
      "validation acc :  0.487669351314\n",
      "degree=9, lambda=0.000, Training RMSE=1.419, Testing RMSE=25244172305032.352\n",
      "train acc :  0.779201129716\n",
      "validation acc :  0.489767122432\n",
      "degree=9, lambda=0.000, Training RMSE=1.120, Testing RMSE=25223854063511.258\n",
      "train acc :  0.765432721404\n",
      "validation acc :  0.510719860149\n",
      "degree=9, lambda=0.000, Training RMSE=0.830, Testing RMSE=25197433291786.039\n",
      "train acc :  0.774914262659\n",
      "validation acc :  0.480951489043\n",
      "degree=9, lambda=0.000, Training RMSE=1.453, Testing RMSE=25162650874018.316\n",
      "train acc :  0.795743393181\n",
      "validation acc :  0.510869700943\n",
      "degree=9, lambda=0.000, Training RMSE=1.054, Testing RMSE=25113444602389.520\n",
      "train acc :  0.811579584426\n",
      "validation acc :  0.493300867828\n",
      "degree=9, lambda=0.000, Training RMSE=0.722, Testing RMSE=25056141001880.367\n",
      "train acc :  0.832257413758\n",
      "validation acc :  0.510669913217\n",
      "degree=9, lambda=0.000, Training RMSE=0.730, Testing RMSE=24986159591616.688\n",
      "train acc :  0.83972160581\n",
      "validation acc :  0.516688518449\n",
      "degree=9, lambda=0.000, Training RMSE=0.706, Testing RMSE=24904099470185.320\n",
      "train acc :  0.843050231995\n",
      "validation acc :  0.503939564213\n",
      "degree=9, lambda=0.000, Training RMSE=0.686, Testing RMSE=24811328143446.348\n",
      "train acc :  0.841335485172\n",
      "validation acc :  0.597914715615\n",
      "degree=9, lambda=0.000, Training RMSE=0.701, Testing RMSE=24702412150794.262\n",
      "train acc :  0.842949364535\n",
      "validation acc :  0.55573453206\n",
      "degree=9, lambda=0.000, Training RMSE=0.685, Testing RMSE=24553979470073.484\n",
      "train acc :  0.842747629615\n",
      "validation acc :  0.670525067116\n",
      "degree=9, lambda=0.000, Training RMSE=0.695, Testing RMSE=24310506687377.875\n",
      "train acc :  0.842596328424\n",
      "validation acc :  0.784066928888\n",
      "degree=9, lambda=0.000, Training RMSE=0.694, Testing RMSE=23915972111941.641\n",
      "train acc :  0.842495460964\n",
      "validation acc :  0.798926140975\n",
      "degree=9, lambda=0.000, Training RMSE=0.693, Testing RMSE=23253779072327.914\n",
      "train acc :  0.842646762155\n",
      "validation acc :  0.841605793844\n",
      "degree=9, lambda=0.000, Training RMSE=0.717, Testing RMSE=22134630123381.539\n",
      "train acc :  0.842495460964\n",
      "validation acc :  0.830192920022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=9, lambda=0.000, Training RMSE=0.742, Testing RMSE=20387055057698.992\n",
      "train acc :  0.842293726044\n",
      "validation acc :  0.838234375976\n",
      "degree=9, lambda=0.000, Training RMSE=0.683, Testing RMSE=17878078870485.469\n",
      "train acc :  0.842041557394\n",
      "validation acc :  0.834388462259\n",
      "degree=9, lambda=0.000, Training RMSE=0.683, Testing RMSE=14713171199917.248\n",
      "train acc :  0.841738955013\n",
      "validation acc :  0.840631828682\n",
      "degree=9, lambda=0.000, Training RMSE=0.684, Testing RMSE=11194322444775.322\n",
      "train acc :  0.841335485172\n",
      "validation acc :  0.827920334644\n",
      "degree=9, lambda=0.000, Training RMSE=0.685, Testing RMSE=7817140583347.131\n",
      "train acc :  0.83997377446\n",
      "validation acc :  0.829256415059\n",
      "degree=9, lambda=0.001, Training RMSE=0.688, Testing RMSE=5082862135786.403\n",
      "train acc :  0.837653822877\n",
      "validation acc :  0.836099144659\n",
      "degree=9, lambda=0.001, Training RMSE=0.691, Testing RMSE=3333431081684.859\n",
      "train acc :  0.834577365342\n",
      "validation acc :  0.8358868702\n",
      "degree=9, lambda=0.001, Training RMSE=0.695, Testing RMSE=2662601009307.234\n",
      "train acc :  0.830996570506\n",
      "validation acc :  0.832915027783\n",
      "degree=9, lambda=0.002, Training RMSE=0.700, Testing RMSE=2928031483038.417\n",
      "train acc :  0.827768811781\n",
      "validation acc :  0.828744459012\n",
      "degree=9, lambda=0.002, Training RMSE=0.706, Testing RMSE=3766202352264.343\n",
      "train acc :  0.824490619326\n",
      "validation acc :  0.824399075982\n",
      "degree=9, lambda=0.003, Training RMSE=0.713, Testing RMSE=4671056916252.877\n",
      "train acc :  0.819447246318\n",
      "validation acc :  0.819217081851\n",
      "degree=9, lambda=0.004, Training RMSE=0.723, Testing RMSE=5144574301053.944\n",
      "train acc :  0.81435343958\n",
      "validation acc :  0.813036149092\n",
      "degree=9, lambda=0.005, Training RMSE=0.734, Testing RMSE=4917819608825.774\n",
      "train acc :  0.806939681259\n",
      "validation acc :  0.806393207217\n",
      "degree=9, lambda=0.007, Training RMSE=0.746, Testing RMSE=4054753204411.014\n",
      "train acc :  0.800837199919\n",
      "validation acc :  0.800636823375\n",
      "degree=9, lambda=0.009, Training RMSE=0.759, Testing RMSE=2856799167129.428\n",
      "train acc :  0.793473875328\n",
      "validation acc :  0.795642130237\n",
      "degree=9, lambda=0.013, Training RMSE=0.771, Testing RMSE=1638570406344.444\n",
      "train acc :  0.789741779302\n",
      "validation acc :  0.791808703253\n",
      "degree=9, lambda=0.017, Training RMSE=0.783, Testing RMSE=577139161969.400\n",
      "train acc :  0.785606213436\n",
      "validation acc :  0.787862895673\n",
      "degree=9, lambda=0.023, Training RMSE=0.795, Testing RMSE=268318288030.058\n",
      "train acc :  0.780159370587\n",
      "validation acc :  0.782930636199\n",
      "degree=9, lambda=0.031, Training RMSE=0.809, Testing RMSE=855169168760.962\n",
      "train acc :  0.774662094008\n",
      "validation acc :  0.777336579884\n",
      "degree=9, lambda=0.041, Training RMSE=0.825, Testing RMSE=1118219998062.670\n",
      "train acc :  0.770677829332\n",
      "validation acc :  0.771892364363\n",
      "degree=9, lambda=0.055, Training RMSE=0.842, Testing RMSE=1029213999532.259\n",
      "train acc :  0.765130119024\n",
      "validation acc :  0.766086033589\n",
      "degree=9, lambda=0.074, Training RMSE=0.859, Testing RMSE=655437583690.330\n",
      "train acc :  0.760742384507\n",
      "validation acc :  0.76214022601\n",
      "degree=9, lambda=0.100, Training RMSE=0.876, Testing RMSE=135967861798.200\n",
      "train acc :  0.757918095622\n",
      "validation acc :  0.758781294874\n",
      "degree=10, lambda=0.000, Training RMSE=51.399, Testing RMSE=748561885823283.625\n",
      "train acc :  0.402965503329\n",
      "validation acc :  0.511856152838\n",
      "degree=10, lambda=0.000, Training RMSE=32.711, Testing RMSE=747533468402835.375\n",
      "train acc :  0.503126891265\n",
      "validation acc :  0.487794218643\n",
      "degree=10, lambda=0.000, Training RMSE=96.261, Testing RMSE=746287059516612.125\n",
      "train acc :  0.691244704458\n",
      "validation acc :  0.486695386152\n",
      "degree=10, lambda=0.000, Training RMSE=56.650, Testing RMSE=746039549465773.125\n",
      "train acc :  0.528343756304\n",
      "validation acc :  0.490316538678\n",
      "degree=10, lambda=0.000, Training RMSE=50.531, Testing RMSE=746502460474747.500\n",
      "train acc :  0.578676618923\n",
      "validation acc :  0.508559655366\n",
      "degree=10, lambda=0.000, Training RMSE=53.505, Testing RMSE=747525721153881.375\n",
      "train acc :  0.556435343958\n",
      "validation acc :  0.507810451395\n",
      "degree=10, lambda=0.000, Training RMSE=66.904, Testing RMSE=748625037141478.375\n",
      "train acc :  0.463788581804\n",
      "validation acc :  0.497708684523\n",
      "degree=10, lambda=0.000, Training RMSE=33.184, Testing RMSE=750317446986809.125\n",
      "train acc :  0.561428283236\n",
      "validation acc :  0.509933195979\n",
      "degree=10, lambda=0.000, Training RMSE=87.622, Testing RMSE=751612668745734.000\n",
      "train acc :  0.520324793222\n",
      "validation acc :  0.533096085409\n",
      "degree=10, lambda=0.000, Training RMSE=28.873, Testing RMSE=750118354080842.500\n",
      "train acc :  0.345370183579\n",
      "validation acc :  0.486982581008\n",
      "degree=10, lambda=0.000, Training RMSE=33.720, Testing RMSE=746225094087362.875\n",
      "train acc :  0.481591688521\n",
      "validation acc :  0.530561278641\n",
      "degree=10, lambda=0.000, Training RMSE=96.210, Testing RMSE=734364623059586.375\n",
      "train acc :  0.58992334073\n",
      "validation acc :  0.497384029469\n",
      "degree=10, lambda=0.000, Training RMSE=54.049, Testing RMSE=710764412119656.125\n",
      "train acc :  0.511902360299\n",
      "validation acc :  0.482612224511\n",
      "degree=10, lambda=0.000, Training RMSE=20.451, Testing RMSE=683259957785132.875\n",
      "train acc :  0.418700827113\n",
      "validation acc :  0.456976961978\n",
      "degree=10, lambda=0.000, Training RMSE=28.578, Testing RMSE=602467508473276.875\n",
      "train acc :  0.60868468832\n",
      "validation acc :  0.460335893114\n",
      "degree=10, lambda=0.000, Training RMSE=52.954, Testing RMSE=491361205119650.750\n",
      "train acc :  0.680401452491\n",
      "validation acc :  0.467241056378\n",
      "degree=10, lambda=0.000, Training RMSE=13.301, Testing RMSE=266586720651882.438\n",
      "train acc :  0.760691950777\n",
      "validation acc :  0.584816132859\n",
      "degree=10, lambda=0.000, Training RMSE=17.636, Testing RMSE=34239858172751.000\n",
      "train acc :  0.55098850111\n",
      "validation acc :  0.547330960854\n",
      "degree=10, lambda=0.000, Training RMSE=8.181, Testing RMSE=249293002369159.125\n",
      "train acc :  0.754992939278\n",
      "validation acc :  0.474146219642\n",
      "degree=10, lambda=0.000, Training RMSE=5.590, Testing RMSE=505639982422289.812\n",
      "train acc :  0.771636070204\n",
      "validation acc :  0.577124305425\n",
      "degree=10, lambda=0.000, Training RMSE=7.423, Testing RMSE=722142882202700.500\n",
      "train acc :  0.771837805124\n",
      "validation acc :  0.690304051945\n",
      "degree=10, lambda=0.000, Training RMSE=0.993, Testing RMSE=792841489626617.125\n",
      "train acc :  0.7567076861\n",
      "validation acc :  0.570793531872\n",
      "degree=10, lambda=0.001, Training RMSE=1.164, Testing RMSE=784438391338981.875\n",
      "train acc :  0.756808553561\n",
      "validation acc :  0.508809390023\n",
      "degree=10, lambda=0.001, Training RMSE=0.818, Testing RMSE=685737391718588.000\n",
      "train acc :  0.814706475691\n",
      "validation acc :  0.700255978023\n",
      "degree=10, lambda=0.001, Training RMSE=0.964, Testing RMSE=525326400655701.188\n",
      "train acc :  0.763768408311\n",
      "validation acc :  0.471886120996\n",
      "degree=10, lambda=0.002, Training RMSE=0.716, Testing RMSE=355645203465747.250\n",
      "train acc :  0.826104498689\n",
      "validation acc :  0.565399263283\n",
      "degree=10, lambda=0.002, Training RMSE=0.815, Testing RMSE=228554737070931.906\n",
      "train acc :  0.809259632842\n",
      "validation acc :  0.588799400637\n",
      "degree=10, lambda=0.003, Training RMSE=0.728, Testing RMSE=172418270932571.000\n",
      "train acc :  0.817278595925\n",
      "validation acc :  0.814621964163\n",
      "degree=10, lambda=0.004, Training RMSE=0.764, Testing RMSE=183146080155468.562\n",
      "train acc :  0.812487391567\n",
      "validation acc :  0.814159955048\n",
      "degree=10, lambda=0.005, Training RMSE=0.739, Testing RMSE=230314076687705.594\n",
      "train acc :  0.802703247932\n",
      "validation acc :  0.800024973466\n",
      "degree=10, lambda=0.007, Training RMSE=0.749, Testing RMSE=278525989153260.938\n",
      "train acc :  0.799223320557\n",
      "validation acc :  0.799300742961\n",
      "degree=10, lambda=0.009, Training RMSE=0.754, Testing RMSE=302312825880911.688\n",
      "train acc :  0.793625176518\n",
      "validation acc :  0.793407005057\n",
      "degree=10, lambda=0.013, Training RMSE=0.765, Testing RMSE=290370481426212.312\n",
      "train acc :  0.795339923341\n",
      "validation acc :  0.796890803521\n",
      "degree=10, lambda=0.017, Training RMSE=0.778, Testing RMSE=246300382828392.750\n",
      "train acc :  0.785656647166\n",
      "validation acc :  0.787450833489\n",
      "degree=10, lambda=0.023, Training RMSE=0.788, Testing RMSE=183492598946268.938\n",
      "train acc :  0.780764575348\n",
      "validation acc :  0.7837672473\n",
      "degree=10, lambda=0.031, Training RMSE=0.801, Testing RMSE=117322355628521.094\n",
      "train acc :  0.779806334476\n",
      "validation acc :  0.782506087282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=10, lambda=0.041, Training RMSE=0.814, Testing RMSE=57736322929270.102\n",
      "train acc :  0.771787371394\n",
      "validation acc :  0.773628020229\n",
      "degree=10, lambda=0.055, Training RMSE=0.831, Testing RMSE=9235662334775.994\n",
      "train acc :  0.767197901957\n",
      "validation acc :  0.767971530249\n",
      "degree=10, lambda=0.074, Training RMSE=0.846, Testing RMSE=25957943837365.672\n",
      "train acc :  0.764524914263\n",
      "validation acc :  0.764462758319\n",
      "degree=10, lambda=0.100, Training RMSE=0.860, Testing RMSE=46486438923308.766\n",
      "train acc :  0.758220698003\n",
      "validation acc :  0.757919710308\n",
      "degree=11, lambda=0.000, Training RMSE=73.331, Testing RMSE=51592522720076640.000\n",
      "train acc :  0.545239055881\n",
      "validation acc :  0.489367546981\n",
      "degree=11, lambda=0.000, Training RMSE=40.918, Testing RMSE=74898155519687008.000\n",
      "train acc :  0.545592091991\n",
      "validation acc :  0.480352125866\n",
      "degree=11, lambda=0.000, Training RMSE=33.381, Testing RMSE=80268243305756928.000\n",
      "train acc :  0.553106717773\n",
      "validation acc :  0.531460323406\n",
      "degree=11, lambda=0.000, Training RMSE=33.017, Testing RMSE=82129853437991584.000\n",
      "train acc :  0.539086140811\n",
      "validation acc :  0.476381344821\n",
      "degree=11, lambda=0.000, Training RMSE=18.671, Testing RMSE=83108530028620096.000\n",
      "train acc :  0.594058906597\n",
      "validation acc :  0.501966660423\n",
      "degree=11, lambda=0.000, Training RMSE=81.382, Testing RMSE=83310021563785376.000\n",
      "train acc :  0.534748840024\n",
      "validation acc :  0.506898919898\n",
      "degree=11, lambda=0.000, Training RMSE=20.113, Testing RMSE=83236503925365664.000\n",
      "train acc :  0.420819043776\n",
      "validation acc :  0.493363301492\n",
      "degree=11, lambda=0.000, Training RMSE=50.836, Testing RMSE=82199292996080192.000\n",
      "train acc :  0.606011700625\n",
      "validation acc :  0.529924455266\n",
      "degree=11, lambda=0.000, Training RMSE=70.373, Testing RMSE=77006975569286336.000\n",
      "train acc :  0.520879564253\n",
      "validation acc :  0.493937691203\n",
      "degree=11, lambda=0.000, Training RMSE=134.599, Testing RMSE=105816390412681136.000\n",
      "train acc :  0.514020576962\n",
      "validation acc :  0.486970094275\n",
      "degree=11, lambda=0.000, Training RMSE=75.034, Testing RMSE=91433576427384432.000\n",
      "train acc :  0.51926568489\n",
      "validation acc :  0.503090466379\n",
      "degree=11, lambda=0.000, Training RMSE=280.809, Testing RMSE=88062332260794304.000\n",
      "train acc :  0.504992939278\n",
      "validation acc :  0.509121558344\n",
      "degree=11, lambda=0.000, Training RMSE=86.171, Testing RMSE=88303756443461408.000\n",
      "train acc :  0.648073431511\n",
      "validation acc :  0.471399138415\n",
      "degree=11, lambda=0.000, Training RMSE=134.812, Testing RMSE=88499725535769856.000\n",
      "train acc :  0.465805931007\n",
      "validation acc :  0.473734157458\n",
      "degree=11, lambda=0.000, Training RMSE=121.016, Testing RMSE=105119794118070272.000\n",
      "train acc :  0.471807544886\n",
      "validation acc :  0.542448648311\n",
      "degree=11, lambda=0.000, Training RMSE=204.325, Testing RMSE=88114235314517280.000\n",
      "train acc :  0.537926165019\n",
      "validation acc :  0.459936317662\n",
      "degree=11, lambda=0.000, Training RMSE=64.364, Testing RMSE=99073404323709104.000\n",
      "train acc :  0.446994149687\n",
      "validation acc :  0.456802147718\n",
      "degree=11, lambda=0.000, Training RMSE=93.546, Testing RMSE=93501971485574224.000\n",
      "train acc :  0.518257010288\n",
      "validation acc :  0.542473621777\n",
      "degree=11, lambda=0.000, Training RMSE=26.895, Testing RMSE=88314587401404976.000\n",
      "train acc :  0.592798063345\n",
      "validation acc :  0.643491290504\n",
      "degree=11, lambda=0.000, Training RMSE=32.486, Testing RMSE=69756046840844312.000\n",
      "train acc :  0.501966915473\n",
      "validation acc :  0.542810763564\n",
      "degree=11, lambda=0.000, Training RMSE=5.019, Testing RMSE=57983062879587272.000\n",
      "train acc :  0.535354044785\n",
      "validation acc :  0.464568895548\n",
      "degree=11, lambda=0.000, Training RMSE=105.602, Testing RMSE=25793314066078712.000\n",
      "train acc :  0.523401250757\n",
      "validation acc :  0.543547480802\n",
      "degree=11, lambda=0.001, Training RMSE=21.256, Testing RMSE=26529344416385788.000\n",
      "train acc :  0.702138390155\n",
      "validation acc :  0.548379846413\n",
      "degree=11, lambda=0.001, Training RMSE=3.634, Testing RMSE=6480480284259143.000\n",
      "train acc :  0.377799072019\n",
      "validation acc :  0.539264531435\n",
      "degree=11, lambda=0.001, Training RMSE=1.650, Testing RMSE=3575926044687021.500\n",
      "train acc :  0.675206778293\n",
      "validation acc :  0.55060248486\n",
      "degree=11, lambda=0.002, Training RMSE=2.183, Testing RMSE=7693168674858557.000\n",
      "train acc :  0.714141617914\n",
      "validation acc :  0.461434725604\n",
      "degree=11, lambda=0.002, Training RMSE=1.048, Testing RMSE=6957941033948265.000\n",
      "train acc :  0.730028242889\n",
      "validation acc :  0.587338452894\n",
      "degree=11, lambda=0.003, Training RMSE=1.000, Testing RMSE=2614449227771893.500\n",
      "train acc :  0.69845672786\n",
      "validation acc :  0.46273334582\n",
      "degree=11, lambda=0.004, Training RMSE=1.695, Testing RMSE=2480774384258510.500\n",
      "train acc :  0.668650393383\n",
      "validation acc :  0.554086283324\n",
      "degree=11, lambda=0.005, Training RMSE=2.157, Testing RMSE=5204183971933629.000\n",
      "train acc :  0.515886624975\n",
      "validation acc :  0.544234251108\n",
      "degree=11, lambda=0.007, Training RMSE=3.904, Testing RMSE=4457119100420555.500\n",
      "train acc :  0.444068993343\n",
      "validation acc :  0.45667728039\n",
      "degree=11, lambda=0.009, Training RMSE=5.232, Testing RMSE=1608750129607279.250\n",
      "train acc :  0.631127698205\n",
      "validation acc :  0.628407317225\n",
      "degree=11, lambda=0.013, Training RMSE=105.779, Testing RMSE=29572379971778996.000\n",
      "train acc :  0.564706475691\n",
      "validation acc :  0.575501030155\n",
      "degree=11, lambda=0.017, Training RMSE=2.609, Testing RMSE=9143936006966120.000\n",
      "train acc :  0.274964696389\n",
      "validation acc :  0.266092276956\n",
      "degree=11, lambda=0.023, Training RMSE=2.955, Testing RMSE=12124884238523262.000\n",
      "train acc :  0.655184587452\n",
      "validation acc :  0.677704938503\n",
      "degree=11, lambda=0.031, Training RMSE=1.187, Testing RMSE=13536843170560358.000\n",
      "train acc :  0.671272947347\n",
      "validation acc :  0.680726727852\n",
      "degree=11, lambda=0.041, Training RMSE=1.269, Testing RMSE=12907728049706454.000\n",
      "train acc :  0.392576154932\n",
      "validation acc :  0.387176125367\n",
      "degree=11, lambda=0.055, Training RMSE=0.914, Testing RMSE=10545748244853924.000\n",
      "train acc :  0.709955618318\n",
      "validation acc :  0.70897171755\n",
      "degree=11, lambda=0.074, Training RMSE=1.078, Testing RMSE=7130620095345103.000\n",
      "train acc :  0.485575953197\n",
      "validation acc :  0.476356371355\n",
      "degree=11, lambda=0.100, Training RMSE=0.946, Testing RMSE=3606345746402870.500\n",
      "train acc :  0.66698608029\n",
      "validation acc :  0.680714241119\n",
      "Best params for Ridge regression : degree =  8 , lambda =  1.91448197617e-05 , accuracy =  0.842417431479\n",
      "************ Model 2 ************* \n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.70896982041\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708614195884\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708501042626\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.70854953688\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708501042626\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708517207378\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708468713124\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708517207378\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708517207378\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708533372129\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708517207378\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708501042626\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712454562847\n",
      "validation acc :  0.708501042626\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.71251833429\n",
      "validation acc :  0.708533372129\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.71251833429\n",
      "validation acc :  0.70854953688\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.708581866382\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712263248517\n",
      "validation acc :  0.708581866382\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712135705631\n",
      "validation acc :  0.708711184391\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.712263248517\n",
      "validation acc :  0.708727349142\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.713156048721\n",
      "validation acc :  0.709066808916\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.713666220267\n",
      "validation acc :  0.709034479414\n",
      "degree=1, lambda=0.000, Training RMSE=0.858, Testing RMSE=0.865\n",
      "train acc :  0.71372999171\n",
      "validation acc :  0.709196126926\n",
      "degree=1, lambda=0.001, Training RMSE=0.859, Testing RMSE=0.865\n",
      "train acc :  0.714622791914\n",
      "validation acc :  0.709745728465\n",
      "degree=1, lambda=0.001, Training RMSE=0.860, Testing RMSE=0.866\n",
      "train acc :  0.714240163255\n",
      "validation acc :  0.710343824257\n",
      "degree=1, lambda=0.001, Training RMSE=0.862, Testing RMSE=0.868\n",
      "train acc :  0.714431477584\n",
      "validation acc :  0.709972034981\n",
      "degree=1, lambda=0.002, Training RMSE=0.866, Testing RMSE=0.871\n",
      "train acc :  0.715005420573\n",
      "validation acc :  0.710036693985\n",
      "degree=1, lambda=0.002, Training RMSE=0.873, Testing RMSE=0.877\n",
      "train acc :  0.714048848925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc :  0.709212291677\n",
      "degree=1, lambda=0.003, Training RMSE=0.883, Testing RMSE=0.886\n",
      "train acc :  0.712390791404\n",
      "validation acc :  0.7088405024\n",
      "degree=1, lambda=0.004, Training RMSE=0.897, Testing RMSE=0.899\n",
      "train acc :  0.708819590587\n",
      "validation acc :  0.70601167095\n",
      "degree=1, lambda=0.005, Training RMSE=0.915, Testing RMSE=0.916\n",
      "train acc :  0.706460047191\n",
      "validation acc :  0.704007241809\n",
      "degree=1, lambda=0.007, Training RMSE=0.935, Testing RMSE=0.935\n",
      "train acc :  0.701804731841\n",
      "validation acc :  0.701970483164\n",
      "degree=1, lambda=0.009, Training RMSE=0.954, Testing RMSE=0.954\n",
      "train acc :  0.700401760092\n",
      "validation acc :  0.700014548276\n",
      "degree=1, lambda=0.013, Training RMSE=0.969, Testing RMSE=0.969\n",
      "train acc :  0.698424845354\n",
      "validation acc :  0.698495061669\n",
      "degree=1, lambda=0.017, Training RMSE=0.981, Testing RMSE=0.981\n",
      "train acc :  0.696001530515\n",
      "validation acc :  0.696264326011\n",
      "degree=1, lambda=0.023, Training RMSE=0.989, Testing RMSE=0.989\n",
      "train acc :  0.695236273197\n",
      "validation acc :  0.695148958182\n",
      "degree=1, lambda=0.031, Training RMSE=0.993, Testing RMSE=0.993\n",
      "train acc :  0.693641987118\n",
      "validation acc :  0.693952766597\n",
      "degree=1, lambda=0.041, Training RMSE=0.996, Testing RMSE=0.996\n",
      "train acc :  0.693195587016\n",
      "validation acc :  0.693079870035\n",
      "degree=1, lambda=0.055, Training RMSE=0.998, Testing RMSE=0.998\n",
      "train acc :  0.692812958357\n",
      "validation acc :  0.692675751257\n",
      "degree=1, lambda=0.074, Training RMSE=0.999, Testing RMSE=0.999\n",
      "train acc :  0.692302786812\n",
      "validation acc :  0.692142314469\n",
      "degree=1, lambda=0.100, Training RMSE=0.999, Testing RMSE=0.999\n",
      "train acc :  0.692047701039\n",
      "validation acc :  0.69215847922\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.75067487836\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.750820361121\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.75094967913\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.750998173383\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.75094967913\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.751030502885\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.751062832388\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.751046667637\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.751014338134\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.750998173383\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758880173458\n",
      "validation acc :  0.750998173383\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758880173458\n",
      "validation acc :  0.751030502885\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758880173458\n",
      "validation acc :  0.751078997139\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.751062832388\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758625087686\n",
      "validation acc :  0.751078997139\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758561316243\n",
      "validation acc :  0.75109516189\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758880173458\n",
      "validation acc :  0.750965843881\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.758880173458\n",
      "validation acc :  0.751062832388\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.759262802117\n",
      "validation acc :  0.751321468406\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.759454116447\n",
      "validation acc :  0.751531610171\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.759071487788\n",
      "validation acc :  0.751628598678\n",
      "degree=2, lambda=0.000, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.759581659333\n",
      "validation acc :  0.751272974153\n",
      "degree=2, lambda=0.001, Training RMSE=0.823, Testing RMSE=0.829\n",
      "train acc :  0.758497544799\n",
      "validation acc :  0.75151544542\n",
      "degree=2, lambda=0.001, Training RMSE=0.824, Testing RMSE=0.830\n",
      "train acc :  0.757413430266\n",
      "validation acc :  0.750691043111\n",
      "degree=2, lambda=0.001, Training RMSE=0.825, Testing RMSE=0.831\n",
      "train acc :  0.757540973152\n",
      "validation acc :  0.750367748089\n",
      "degree=2, lambda=0.002, Training RMSE=0.827, Testing RMSE=0.833\n",
      "train acc :  0.754352400995\n",
      "validation acc :  0.749801981798\n",
      "degree=2, lambda=0.002, Training RMSE=0.831, Testing RMSE=0.836\n",
      "train acc :  0.753140743575\n",
      "validation acc :  0.748250165689\n",
      "degree=2, lambda=0.003, Training RMSE=0.837, Testing RMSE=0.842\n",
      "train acc :  0.750653657292\n",
      "validation acc :  0.744289801659\n",
      "degree=2, lambda=0.004, Training RMSE=0.847, Testing RMSE=0.852\n",
      "train acc :  0.74006759773\n",
      "validation acc :  0.736029613824\n",
      "degree=2, lambda=0.005, Training RMSE=0.863, Testing RMSE=0.867\n",
      "train acc :  0.720170907468\n",
      "validation acc :  0.718280717068\n",
      "degree=2, lambda=0.007, Training RMSE=0.882, Testing RMSE=0.886\n",
      "train acc :  0.688795357439\n",
      "validation acc :  0.688505245462\n",
      "degree=2, lambda=0.009, Training RMSE=0.901, Testing RMSE=0.905\n",
      "train acc :  0.664562209043\n",
      "validation acc :  0.661283804536\n",
      "degree=2, lambda=0.013, Training RMSE=0.919, Testing RMSE=0.923\n",
      "train acc :  0.655634207002\n",
      "validation acc :  0.651568789098\n",
      "degree=2, lambda=0.017, Training RMSE=0.936, Testing RMSE=0.938\n",
      "train acc :  0.653784835151\n",
      "validation acc :  0.648691463395\n",
      "degree=2, lambda=0.023, Training RMSE=0.951, Testing RMSE=0.953\n",
      "train acc :  0.651233977425\n",
      "validation acc :  0.647058823529\n",
      "degree=2, lambda=0.031, Training RMSE=0.965, Testing RMSE=0.966\n",
      "train acc :  0.650277405778\n",
      "validation acc :  0.645458513166\n",
      "degree=2, lambda=0.041, Training RMSE=0.976, Testing RMSE=0.977\n",
      "train acc :  0.64830049104\n",
      "validation acc :  0.643922861808\n",
      "degree=2, lambda=0.055, Training RMSE=0.985, Testing RMSE=0.986\n",
      "train acc :  0.647407690836\n",
      "validation acc :  0.642597352214\n",
      "degree=2, lambda=0.074, Training RMSE=0.991, Testing RMSE=0.991\n",
      "train acc :  0.646387347746\n",
      "validation acc :  0.641950762168\n",
      "degree=2, lambda=0.100, Training RMSE=0.995, Testing RMSE=0.995\n",
      "train acc :  0.64606849053\n",
      "validation acc :  0.641789114657\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.759226031715\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.759080548955\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.759048219453\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.759064384204\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.758967395697\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.758902736692\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.758886571941\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.770040176009\n",
      "validation acc :  0.758918901444\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.770040176009\n",
      "validation acc :  0.758951230946\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.758967395697\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.758951230946\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.758918901444\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.770103947452\n",
      "validation acc :  0.758902736692\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.769976404566\n",
      "validation acc :  0.758999725199\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.770040176009\n",
      "validation acc :  0.758902736692\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.770359033225\n",
      "validation acc :  0.758967395697\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.770040176009\n",
      "validation acc :  0.759129043208\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.770614118998\n",
      "validation acc :  0.75901588995\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.840\n",
      "train acc :  0.771124290543\n",
      "validation acc :  0.758983560448\n",
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.841\n",
      "train acc :  0.771124290543\n",
      "validation acc :  0.759306855471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=3, lambda=0.000, Training RMSE=0.810, Testing RMSE=0.841\n",
      "train acc :  0.771315604872\n",
      "validation acc :  0.759306855471\n",
      "degree=3, lambda=0.000, Training RMSE=0.811, Testing RMSE=0.842\n",
      "train acc :  0.770805433327\n",
      "validation acc :  0.759258361218\n",
      "degree=3, lambda=0.001, Training RMSE=0.811, Testing RMSE=0.843\n",
      "train acc :  0.771188061986\n",
      "validation acc :  0.759403843978\n",
      "degree=3, lambda=0.001, Training RMSE=0.812, Testing RMSE=0.844\n",
      "train acc :  0.76965754735\n",
      "validation acc :  0.759484667734\n",
      "degree=3, lambda=0.001, Training RMSE=0.813, Testing RMSE=0.845\n",
      "train acc :  0.769338690135\n",
      "validation acc :  0.75914520796\n",
      "degree=3, lambda=0.002, Training RMSE=0.816, Testing RMSE=0.845\n",
      "train acc :  0.767808175499\n",
      "validation acc :  0.759420008729\n",
      "degree=3, lambda=0.002, Training RMSE=0.820, Testing RMSE=0.845\n",
      "train acc :  0.766915375295\n",
      "validation acc :  0.758611771172\n",
      "degree=3, lambda=0.003, Training RMSE=0.826, Testing RMSE=0.848\n",
      "train acc :  0.76296154582\n",
      "validation acc :  0.757156943569\n",
      "degree=3, lambda=0.004, Training RMSE=0.837, Testing RMSE=0.853\n",
      "train acc :  0.753140743575\n",
      "validation acc :  0.749559510531\n",
      "degree=3, lambda=0.005, Training RMSE=0.851, Testing RMSE=0.863\n",
      "train acc :  0.739493654741\n",
      "validation acc :  0.73554467129\n",
      "degree=3, lambda=0.007, Training RMSE=0.869, Testing RMSE=0.877\n",
      "train acc :  0.710924048211\n",
      "validation acc :  0.70727252154\n",
      "degree=3, lambda=0.009, Training RMSE=0.889, Testing RMSE=0.894\n",
      "train acc :  0.675785983037\n",
      "validation acc :  0.672259670562\n",
      "degree=3, lambda=0.013, Training RMSE=0.910, Testing RMSE=0.914\n",
      "train acc :  0.65761112174\n",
      "validation acc :  0.653039781453\n",
      "degree=3, lambda=0.017, Training RMSE=0.929, Testing RMSE=0.933\n",
      "train acc :  0.652254320515\n",
      "validation acc :  0.646800187511\n",
      "degree=3, lambda=0.023, Training RMSE=0.947, Testing RMSE=0.951\n",
      "train acc :  0.650341177221\n",
      "validation acc :  0.645119053392\n",
      "degree=3, lambda=0.031, Training RMSE=0.962, Testing RMSE=0.967\n",
      "train acc :  0.650149862891\n",
      "validation acc :  0.645038229636\n",
      "degree=3, lambda=0.041, Training RMSE=0.974, Testing RMSE=0.978\n",
      "train acc :  0.65053249155\n",
      "validation acc :  0.645620160678\n",
      "degree=3, lambda=0.055, Training RMSE=0.982, Testing RMSE=0.986\n",
      "train acc :  0.651807920413\n",
      "validation acc :  0.646622375249\n",
      "degree=3, lambda=0.074, Training RMSE=0.988, Testing RMSE=0.991\n",
      "train acc :  0.653019577833\n",
      "validation acc :  0.648222685612\n",
      "degree=3, lambda=0.100, Training RMSE=0.991, Testing RMSE=0.994\n",
      "train acc :  0.654741406798\n",
      "validation acc :  0.649564359957\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.972\n",
      "train acc :  0.777692749187\n",
      "validation acc :  0.728561498796\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.899\n",
      "train acc :  0.778075377846\n",
      "validation acc :  0.76000193977\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.888\n",
      "train acc :  0.777692749187\n",
      "validation acc :  0.768019656337\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.892\n",
      "train acc :  0.77775652063\n",
      "validation acc :  0.764431081583\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.888\n",
      "train acc :  0.777884063516\n",
      "validation acc :  0.767615537559\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.887\n",
      "train acc :  0.777884063516\n",
      "validation acc :  0.76861775213\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.888\n",
      "train acc :  0.777884063516\n",
      "validation acc :  0.768019656337\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.887\n",
      "train acc :  0.777884063516\n",
      "validation acc :  0.7681974686\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.887\n",
      "train acc :  0.777884063516\n",
      "validation acc :  0.768245962853\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.887\n",
      "train acc :  0.777884063516\n",
      "validation acc :  0.768229798102\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.887\n",
      "train acc :  0.777820292073\n",
      "validation acc :  0.768262127605\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.887\n",
      "train acc :  0.777820292073\n",
      "validation acc :  0.7681974686\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.887\n",
      "train acc :  0.777884063516\n",
      "validation acc :  0.768213633351\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.887\n",
      "train acc :  0.77794783496\n",
      "validation acc :  0.768181303849\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.887\n",
      "train acc :  0.778011606403\n",
      "validation acc :  0.768213633351\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.886\n",
      "train acc :  0.77794783496\n",
      "validation acc :  0.768245962853\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.886\n",
      "train acc :  0.778011606403\n",
      "validation acc :  0.768326786609\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.885\n",
      "train acc :  0.777246349085\n",
      "validation acc :  0.768359116111\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.884\n",
      "train acc :  0.776927491869\n",
      "validation acc :  0.768068150591\n",
      "degree=4, lambda=0.000, Training RMSE=0.802, Testing RMSE=0.883\n",
      "train acc :  0.777501434857\n",
      "validation acc :  0.767874173577\n",
      "degree=4, lambda=0.000, Training RMSE=0.803, Testing RMSE=0.882\n",
      "train acc :  0.777055034755\n",
      "validation acc :  0.768165139098\n",
      "degree=4, lambda=0.000, Training RMSE=0.803, Testing RMSE=0.880\n",
      "train acc :  0.776927491869\n",
      "validation acc :  0.768148974347\n",
      "degree=4, lambda=0.001, Training RMSE=0.803, Testing RMSE=0.875\n",
      "train acc :  0.777182577642\n",
      "validation acc :  0.767987326835\n",
      "degree=4, lambda=0.001, Training RMSE=0.804, Testing RMSE=0.868\n",
      "train acc :  0.776672406097\n",
      "validation acc :  0.767421560545\n",
      "degree=4, lambda=0.001, Training RMSE=0.806, Testing RMSE=0.857\n",
      "train acc :  0.774440405586\n",
      "validation acc :  0.76622536896\n",
      "degree=4, lambda=0.002, Training RMSE=0.809, Testing RMSE=0.844\n",
      "train acc :  0.772144633633\n",
      "validation acc :  0.764269434072\n",
      "degree=4, lambda=0.002, Training RMSE=0.815, Testing RMSE=0.834\n",
      "train acc :  0.766468975193\n",
      "validation acc :  0.759904951263\n",
      "degree=4, lambda=0.003, Training RMSE=0.823, Testing RMSE=0.833\n",
      "train acc :  0.758816402015\n",
      "validation acc :  0.754376606372\n",
      "degree=4, lambda=0.004, Training RMSE=0.835, Testing RMSE=0.845\n",
      "train acc :  0.74899559977\n",
      "validation acc :  0.744790908944\n",
      "degree=4, lambda=0.005, Training RMSE=0.848, Testing RMSE=0.866\n",
      "train acc :  0.737580511447\n",
      "validation acc :  0.733669560157\n",
      "degree=4, lambda=0.007, Training RMSE=0.863, Testing RMSE=0.890\n",
      "train acc :  0.717109878197\n",
      "validation acc :  0.717359326253\n",
      "degree=4, lambda=0.009, Training RMSE=0.877, Testing RMSE=0.910\n",
      "train acc :  0.699827817104\n",
      "validation acc :  0.697719153614\n",
      "degree=4, lambda=0.013, Training RMSE=0.890, Testing RMSE=0.924\n",
      "train acc :  0.683119698999\n",
      "validation acc :  0.681748379484\n",
      "degree=4, lambda=0.017, Training RMSE=0.904, Testing RMSE=0.935\n",
      "train acc :  0.672533639436\n",
      "validation acc :  0.669657145628\n",
      "degree=4, lambda=0.023, Training RMSE=0.918, Testing RMSE=0.943\n",
      "train acc :  0.664243351827\n",
      "validation acc :  0.659634999919\n",
      "degree=4, lambda=0.031, Training RMSE=0.931, Testing RMSE=0.951\n",
      "train acc :  0.657929978955\n",
      "validation acc :  0.652118390637\n",
      "degree=4, lambda=0.041, Training RMSE=0.945, Testing RMSE=0.957\n",
      "train acc :  0.65397614948\n",
      "validation acc :  0.648723792897\n",
      "degree=4, lambda=0.055, Training RMSE=0.958, Testing RMSE=0.964\n",
      "train acc :  0.653402206492\n",
      "validation acc :  0.648319674119\n",
      "degree=4, lambda=0.074, Training RMSE=0.969, Testing RMSE=0.972\n",
      "train acc :  0.653210892162\n",
      "validation acc :  0.648529815883\n",
      "degree=4, lambda=0.100, Training RMSE=0.979, Testing RMSE=0.981\n",
      "train acc :  0.653210892162\n",
      "validation acc :  0.648562145386\n",
      "degree=5, lambda=0.000, Training RMSE=0.802, Testing RMSE=85.921\n",
      "train acc :  0.775205662904\n",
      "validation acc :  0.500299047896\n",
      "degree=5, lambda=0.000, Training RMSE=0.800, Testing RMSE=40.944\n",
      "train acc :  0.776927491869\n",
      "validation acc :  0.463152449768\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=13.977\n",
      "train acc :  0.777310120528\n",
      "validation acc :  0.595218466612\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=4.812\n",
      "train acc :  0.778202920732\n",
      "validation acc :  0.52325299452\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=4.136\n",
      "train acc :  0.776991263312\n",
      "validation acc :  0.485104181821\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=1.232\n",
      "train acc :  0.777182577642\n",
      "validation acc :  0.656337390686\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.981\n",
      "train acc :  0.777820292073\n",
      "validation acc :  0.761343614115\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=1.227\n",
      "train acc :  0.777565206301\n",
      "validation acc :  0.708533372129\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.977\n",
      "train acc :  0.77775652063\n",
      "validation acc :  0.764883694616\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.985\n",
      "train acc :  0.777628977744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc :  0.758999725199\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.973\n",
      "train acc :  0.777628977744\n",
      "validation acc :  0.769555307696\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.973\n",
      "train acc :  0.77775652063\n",
      "validation acc :  0.769264342175\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.972\n",
      "train acc :  0.77794783496\n",
      "validation acc :  0.769232012673\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.971\n",
      "train acc :  0.778011606403\n",
      "validation acc :  0.769425989687\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.969\n",
      "train acc :  0.778011606403\n",
      "validation acc :  0.769361330682\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.967\n",
      "train acc :  0.77794783496\n",
      "validation acc :  0.769490648691\n",
      "degree=5, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.964\n",
      "train acc :  0.778139149289\n",
      "validation acc :  0.769991755977\n",
      "degree=5, lambda=0.000, Training RMSE=0.800, Testing RMSE=0.960\n",
      "train acc :  0.778458006505\n",
      "validation acc :  0.770153403488\n",
      "degree=5, lambda=0.000, Training RMSE=0.800, Testing RMSE=0.955\n",
      "train acc :  0.778011606403\n",
      "validation acc :  0.770218062493\n",
      "degree=5, lambda=0.000, Training RMSE=0.800, Testing RMSE=0.949\n",
      "train acc :  0.778075377846\n",
      "validation acc :  0.770072579733\n",
      "degree=5, lambda=0.000, Training RMSE=0.800, Testing RMSE=0.939\n",
      "train acc :  0.778011606403\n",
      "validation acc :  0.769636131452\n",
      "degree=5, lambda=0.000, Training RMSE=0.801, Testing RMSE=0.924\n",
      "train acc :  0.778585549391\n",
      "validation acc :  0.769442154438\n",
      "degree=5, lambda=0.001, Training RMSE=0.801, Testing RMSE=0.904\n",
      "train acc :  0.779031949493\n",
      "validation acc :  0.769684625705\n",
      "degree=5, lambda=0.001, Training RMSE=0.802, Testing RMSE=0.885\n",
      "train acc :  0.777884063516\n",
      "validation acc :  0.768536928374\n",
      "degree=5, lambda=0.001, Training RMSE=0.804, Testing RMSE=0.880\n",
      "train acc :  0.77533320579\n",
      "validation acc :  0.766839629504\n",
      "degree=5, lambda=0.002, Training RMSE=0.808, Testing RMSE=0.908\n",
      "train acc :  0.772017090747\n",
      "validation acc :  0.764334093077\n",
      "degree=5, lambda=0.002, Training RMSE=0.814, Testing RMSE=0.974\n",
      "train acc :  0.767808175499\n",
      "validation acc :  0.759840292259\n",
      "degree=5, lambda=0.003, Training RMSE=0.823, Testing RMSE=1.050\n",
      "train acc :  0.759709202219\n",
      "validation acc :  0.753940158091\n",
      "degree=5, lambda=0.004, Training RMSE=0.834, Testing RMSE=1.102\n",
      "train acc :  0.752311714814\n",
      "validation acc :  0.745372839985\n",
      "degree=5, lambda=0.005, Training RMSE=0.846, Testing RMSE=1.109\n",
      "train acc :  0.74108794082\n",
      "validation acc :  0.736045778575\n",
      "degree=5, lambda=0.007, Training RMSE=0.858, Testing RMSE=1.074\n",
      "train acc :  0.728843823736\n",
      "validation acc :  0.725522525581\n",
      "degree=5, lambda=0.009, Training RMSE=0.869, Testing RMSE=1.017\n",
      "train acc :  0.712900962949\n",
      "validation acc :  0.711766322357\n",
      "degree=5, lambda=0.013, Training RMSE=0.880, Testing RMSE=0.967\n",
      "train acc :  0.69976404566\n",
      "validation acc :  0.697864636374\n",
      "degree=5, lambda=0.017, Training RMSE=0.892, Testing RMSE=0.945\n",
      "train acc :  0.687775014349\n",
      "validation acc :  0.684771187948\n",
      "degree=5, lambda=0.023, Training RMSE=0.906, Testing RMSE=0.963\n",
      "train acc :  0.673809068299\n",
      "validation acc :  0.670885666715\n",
      "degree=5, lambda=0.031, Training RMSE=0.921, Testing RMSE=1.014\n",
      "train acc :  0.663924494611\n",
      "validation acc :  0.658438808335\n",
      "degree=5, lambda=0.041, Training RMSE=0.937, Testing RMSE=1.079\n",
      "train acc :  0.656782092979\n",
      "validation acc :  0.652247708647\n",
      "degree=5, lambda=0.055, Training RMSE=0.953, Testing RMSE=1.139\n",
      "train acc :  0.654358778139\n",
      "validation acc :  0.649499700952\n",
      "degree=5, lambda=0.074, Training RMSE=0.966, Testing RMSE=1.176\n",
      "train acc :  0.654039920923\n",
      "validation acc :  0.649079417422\n",
      "degree=5, lambda=0.100, Training RMSE=0.975, Testing RMSE=1.185\n",
      "train acc :  0.654868949684\n",
      "validation acc :  0.649532030454\n",
      "degree=6, lambda=0.000, Training RMSE=1.183, Testing RMSE=906.668\n",
      "train acc :  0.657547350297\n",
      "validation acc :  0.533242810727\n",
      "degree=6, lambda=0.000, Training RMSE=1.086, Testing RMSE=1744.358\n",
      "train acc :  0.6803137555\n",
      "validation acc :  0.485492135849\n",
      "degree=6, lambda=0.000, Training RMSE=2.205, Testing RMSE=695.890\n",
      "train acc :  0.477775652063\n",
      "validation acc :  0.51547774922\n",
      "degree=6, lambda=0.000, Training RMSE=1.101, Testing RMSE=363.799\n",
      "train acc :  0.684650213634\n",
      "validation acc :  0.478201833083\n",
      "degree=6, lambda=0.000, Training RMSE=1.035, Testing RMSE=128.444\n",
      "train acc :  0.672342325107\n",
      "validation acc :  0.474386951813\n",
      "degree=6, lambda=0.000, Training RMSE=1.003, Testing RMSE=257.196\n",
      "train acc :  0.693897072891\n",
      "validation acc :  0.451691641207\n",
      "degree=6, lambda=0.000, Training RMSE=0.917, Testing RMSE=73.134\n",
      "train acc :  0.712454562847\n",
      "validation acc :  0.554871247757\n",
      "degree=6, lambda=0.000, Training RMSE=0.818, Testing RMSE=96.236\n",
      "train acc :  0.763280403036\n",
      "validation acc :  0.421835345845\n",
      "degree=6, lambda=0.000, Training RMSE=0.849, Testing RMSE=16.843\n",
      "train acc :  0.749824628531\n",
      "validation acc :  0.479301036161\n",
      "degree=6, lambda=0.000, Training RMSE=0.804, Testing RMSE=5.677\n",
      "train acc :  0.772591033735\n",
      "validation acc :  0.624347348173\n",
      "degree=6, lambda=0.000, Training RMSE=0.802, Testing RMSE=6.560\n",
      "train acc :  0.77329251961\n",
      "validation acc :  0.629423080032\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.719\n",
      "train acc :  0.778713092277\n",
      "validation acc :  0.661542440554\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.442\n",
      "train acc :  0.780690007015\n",
      "validation acc :  0.742398525775\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.519\n",
      "train acc :  0.78017983547\n",
      "validation acc :  0.693451659312\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.468\n",
      "train acc :  0.780753778458\n",
      "validation acc :  0.770492863262\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.488\n",
      "train acc :  0.78120017856\n",
      "validation acc :  0.771575901589\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.507\n",
      "train acc :  0.780498692685\n",
      "validation acc :  0.77201234987\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.522\n",
      "train acc :  0.780817549901\n",
      "validation acc :  0.772141667879\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.532\n",
      "train acc :  0.78120017856\n",
      "validation acc :  0.772772093174\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.531\n",
      "train acc :  0.78120017856\n",
      "validation acc :  0.772739763671\n",
      "degree=6, lambda=0.000, Training RMSE=0.797, Testing RMSE=5.517\n",
      "train acc :  0.781391492889\n",
      "validation acc :  0.772464962902\n",
      "degree=6, lambda=0.000, Training RMSE=0.798, Testing RMSE=5.484\n",
      "train acc :  0.780945092787\n",
      "validation acc :  0.772190162132\n",
      "degree=6, lambda=0.001, Training RMSE=0.799, Testing RMSE=5.418\n",
      "train acc :  0.780626235572\n",
      "validation acc :  0.771511242584\n",
      "degree=6, lambda=0.001, Training RMSE=0.800, Testing RMSE=5.295\n",
      "train acc :  0.779797206811\n",
      "validation acc :  0.770929311543\n",
      "degree=6, lambda=0.001, Training RMSE=0.802, Testing RMSE=5.069\n",
      "train acc :  0.776799948983\n",
      "validation acc :  0.768860223397\n",
      "degree=6, lambda=0.002, Training RMSE=0.805, Testing RMSE=4.678\n",
      "train acc :  0.776608634653\n",
      "validation acc :  0.766807300002\n",
      "degree=6, lambda=0.002, Training RMSE=0.810, Testing RMSE=4.069\n",
      "train acc :  0.772208405076\n",
      "validation acc :  0.763331878506\n",
      "degree=6, lambda=0.003, Training RMSE=0.818, Testing RMSE=3.257\n",
      "train acc :  0.767234232511\n",
      "validation acc :  0.757367085334\n",
      "degree=6, lambda=0.004, Training RMSE=0.828, Testing RMSE=2.359\n",
      "train acc :  0.757477201709\n",
      "validation acc :  0.748751272974\n",
      "degree=6, lambda=0.005, Training RMSE=0.840, Testing RMSE=1.590\n",
      "train acc :  0.746253427715\n",
      "validation acc :  0.739391882062\n",
      "degree=6, lambda=0.007, Training RMSE=0.854, Testing RMSE=1.204\n",
      "train acc :  0.730501881258\n",
      "validation acc :  0.725474031327\n",
      "degree=6, lambda=0.009, Training RMSE=0.867, Testing RMSE=1.264\n",
      "train acc :  0.712900962949\n",
      "validation acc :  0.70968106946\n",
      "degree=6, lambda=0.013, Training RMSE=0.879, Testing RMSE=1.492\n",
      "train acc :  0.696192844844\n",
      "validation acc :  0.694292226371\n",
      "degree=6, lambda=0.017, Training RMSE=0.890, Testing RMSE=1.698\n",
      "train acc :  0.683438556215\n",
      "validation acc :  0.682572781792\n",
      "degree=6, lambda=0.023, Training RMSE=0.899, Testing RMSE=1.827\n",
      "train acc :  0.675403354378\n",
      "validation acc :  0.674053957939\n",
      "degree=6, lambda=0.031, Training RMSE=0.908, Testing RMSE=1.859\n",
      "train acc :  0.673171353868\n",
      "validation acc :  0.66954399237\n",
      "degree=6, lambda=0.041, Training RMSE=0.917, Testing RMSE=1.777\n",
      "train acc :  0.669345067279\n",
      "validation acc :  0.666585842911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=6, lambda=0.055, Training RMSE=0.927, Testing RMSE=1.571\n",
      "train acc :  0.668005866973\n",
      "validation acc :  0.663870164719\n",
      "degree=6, lambda=0.074, Training RMSE=0.939, Testing RMSE=1.289\n",
      "train acc :  0.665391237804\n",
      "validation acc :  0.662011218337\n",
      "degree=6, lambda=0.100, Training RMSE=0.952, Testing RMSE=1.059\n",
      "train acc :  0.664115808941\n",
      "validation acc :  0.659731988426\n",
      "degree=7, lambda=0.000, Training RMSE=6.633, Testing RMSE=1799.000\n",
      "train acc :  0.480581595562\n",
      "validation acc :  0.517207377592\n",
      "degree=7, lambda=0.000, Training RMSE=3.045, Testing RMSE=2222.497\n",
      "train acc :  0.493973598623\n",
      "validation acc :  0.492798603366\n",
      "degree=7, lambda=0.000, Training RMSE=6.215, Testing RMSE=1577.599\n",
      "train acc :  0.579299789554\n",
      "validation acc :  0.541551492815\n",
      "degree=7, lambda=0.000, Training RMSE=6.656, Testing RMSE=379.515\n",
      "train acc :  0.602703909189\n",
      "validation acc :  0.502562113056\n",
      "degree=7, lambda=0.000, Training RMSE=3.460, Testing RMSE=177.113\n",
      "train acc :  0.563038071552\n",
      "validation acc :  0.491990365808\n",
      "degree=7, lambda=0.000, Training RMSE=6.815, Testing RMSE=2746.678\n",
      "train acc :  0.422039410752\n",
      "validation acc :  0.542440554128\n",
      "degree=7, lambda=0.000, Training RMSE=3.723, Testing RMSE=724.603\n",
      "train acc :  0.50385817231\n",
      "validation acc :  0.446518920841\n",
      "degree=7, lambda=0.000, Training RMSE=5.654, Testing RMSE=511.081\n",
      "train acc :  0.602767680633\n",
      "validation acc :  0.48225918562\n",
      "degree=7, lambda=0.000, Training RMSE=7.797, Testing RMSE=206.108\n",
      "train acc :  0.642433518271\n",
      "validation acc :  0.514847323925\n",
      "degree=7, lambda=0.000, Training RMSE=4.792, Testing RMSE=836.224\n",
      "train acc :  0.616669855239\n",
      "validation acc :  0.570583385869\n",
      "degree=7, lambda=0.000, Training RMSE=2.745, Testing RMSE=420.290\n",
      "train acc :  0.557617498884\n",
      "validation acc :  0.441814978258\n",
      "degree=7, lambda=0.000, Training RMSE=3.267, Testing RMSE=614.837\n",
      "train acc :  0.488808111728\n",
      "validation acc :  0.413607487513\n",
      "degree=7, lambda=0.000, Training RMSE=6.978, Testing RMSE=164.894\n",
      "train acc :  0.600663223009\n",
      "validation acc :  0.55816885699\n",
      "degree=7, lambda=0.000, Training RMSE=6.699, Testing RMSE=187.905\n",
      "train acc :  0.528282635036\n",
      "validation acc :  0.532402243667\n",
      "degree=7, lambda=0.000, Training RMSE=2.498, Testing RMSE=72.441\n",
      "train acc :  0.55774504177\n",
      "validation acc :  0.581882546918\n",
      "degree=7, lambda=0.000, Training RMSE=2.176, Testing RMSE=158.901\n",
      "train acc :  0.497417256553\n",
      "validation acc :  0.588510094887\n",
      "degree=7, lambda=0.000, Training RMSE=0.922, Testing RMSE=36.185\n",
      "train acc :  0.717492506855\n",
      "validation acc :  0.494059453955\n",
      "degree=7, lambda=0.000, Training RMSE=0.950, Testing RMSE=40.259\n",
      "train acc :  0.66328678018\n",
      "validation acc :  0.597400708016\n",
      "degree=7, lambda=0.000, Training RMSE=0.826, Testing RMSE=35.717\n",
      "train acc :  0.762260059945\n",
      "validation acc :  0.74218838401\n",
      "degree=7, lambda=0.000, Training RMSE=0.792, Testing RMSE=35.260\n",
      "train acc :  0.789745551942\n",
      "validation acc :  0.776231349918\n",
      "degree=7, lambda=0.000, Training RMSE=0.790, Testing RMSE=34.446\n",
      "train acc :  0.78929915184\n",
      "validation acc :  0.777556859512\n",
      "degree=7, lambda=0.000, Training RMSE=0.789, Testing RMSE=33.040\n",
      "train acc :  0.789426694726\n",
      "validation acc :  0.778154955304\n",
      "degree=7, lambda=0.001, Training RMSE=0.789, Testing RMSE=30.717\n",
      "train acc :  0.788661437408\n",
      "validation acc :  0.778866204355\n",
      "degree=7, lambda=0.001, Training RMSE=0.791, Testing RMSE=27.117\n",
      "train acc :  0.786875837\n",
      "validation acc :  0.777734671775\n",
      "degree=7, lambda=0.001, Training RMSE=0.794, Testing RMSE=22.026\n",
      "train acc :  0.78464383649\n",
      "validation acc :  0.774889675573\n",
      "degree=7, lambda=0.002, Training RMSE=0.799, Testing RMSE=15.651\n",
      "train acc :  0.780434921242\n",
      "validation acc :  0.769781614212\n",
      "degree=7, lambda=0.002, Training RMSE=0.806, Testing RMSE=8.809\n",
      "train acc :  0.775588291563\n",
      "validation acc :  0.765481790408\n",
      "degree=7, lambda=0.003, Training RMSE=0.814, Testing RMSE=3.022\n",
      "train acc :  0.767808175499\n",
      "validation acc :  0.758498617914\n",
      "degree=7, lambda=0.004, Training RMSE=0.825, Testing RMSE=3.612\n",
      "train acc :  0.760219373764\n",
      "validation acc :  0.751563939673\n",
      "degree=7, lambda=0.005, Training RMSE=0.837, Testing RMSE=6.610\n",
      "train acc :  0.748230342453\n",
      "validation acc :  0.741735770978\n",
      "degree=7, lambda=0.007, Training RMSE=0.850, Testing RMSE=8.323\n",
      "train acc :  0.735794911039\n",
      "validation acc :  0.729741525629\n",
      "degree=7, lambda=0.009, Training RMSE=0.864, Testing RMSE=8.828\n",
      "train acc :  0.718257764173\n",
      "validation acc :  0.713948563762\n",
      "degree=7, lambda=0.013, Training RMSE=0.877, Testing RMSE=8.395\n",
      "train acc :  0.700529302978\n",
      "validation acc :  0.69710489307\n",
      "degree=7, lambda=0.017, Training RMSE=0.887, Testing RMSE=7.307\n",
      "train acc :  0.688221414451\n",
      "validation acc :  0.685353118989\n",
      "degree=7, lambda=0.023, Training RMSE=0.896, Testing RMSE=5.886\n",
      "train acc :  0.680823927045\n",
      "validation acc :  0.6777395212\n",
      "degree=7, lambda=0.031, Training RMSE=0.904, Testing RMSE=4.491\n",
      "train acc :  0.676359926025\n",
      "validation acc :  0.673407367894\n",
      "degree=7, lambda=0.041, Training RMSE=0.913, Testing RMSE=3.409\n",
      "train acc :  0.674383011288\n",
      "validation acc :  0.670934160969\n",
      "degree=7, lambda=0.055, Training RMSE=0.922, Testing RMSE=2.757\n",
      "train acc :  0.671895925005\n",
      "validation acc :  0.668751919564\n",
      "degree=7, lambda=0.074, Training RMSE=0.933, Testing RMSE=2.554\n",
      "train acc :  0.669472610165\n",
      "validation acc :  0.666181724132\n",
      "degree=7, lambda=0.100, Training RMSE=0.944, Testing RMSE=2.722\n",
      "train acc :  0.668260952745\n",
      "validation acc :  0.664225789244\n",
      "degree=8, lambda=0.000, Training RMSE=3.919, Testing RMSE=11507.709\n",
      "train acc :  0.440788215037\n",
      "validation acc :  0.489016051598\n",
      "degree=8, lambda=0.000, Training RMSE=4.960, Testing RMSE=6187.091\n",
      "train acc :  0.509151202092\n",
      "validation acc :  0.505471768262\n",
      "degree=8, lambda=0.000, Training RMSE=5.914, Testing RMSE=3421.583\n",
      "train acc :  0.442510044002\n",
      "validation acc :  0.509981733831\n",
      "degree=8, lambda=0.000, Training RMSE=15.431, Testing RMSE=3997.932\n",
      "train acc :  0.633569287673\n",
      "validation acc :  0.509933239578\n",
      "degree=8, lambda=0.000, Training RMSE=7.122, Testing RMSE=759.815\n",
      "train acc :  0.469740450226\n",
      "validation acc :  0.489032216349\n",
      "degree=8, lambda=0.000, Training RMSE=84.407, Testing RMSE=6741.252\n",
      "train acc :  0.576366303169\n",
      "validation acc :  0.421560545075\n",
      "degree=8, lambda=0.000, Training RMSE=7.372, Testing RMSE=982.452\n",
      "train acc :  0.545883553345\n",
      "validation acc :  0.562468680795\n",
      "degree=8, lambda=0.000, Training RMSE=4.878, Testing RMSE=1041.432\n",
      "train acc :  0.56482367196\n",
      "validation acc :  0.434185215719\n",
      "degree=8, lambda=0.000, Training RMSE=3.406, Testing RMSE=540.432\n",
      "train acc :  0.563038071552\n",
      "validation acc :  0.580734849587\n",
      "degree=8, lambda=0.000, Training RMSE=4.949, Testing RMSE=250.432\n",
      "train acc :  0.501562400357\n",
      "validation acc :  0.424534859286\n",
      "degree=8, lambda=0.000, Training RMSE=6.759, Testing RMSE=349.573\n",
      "train acc :  0.57515464575\n",
      "validation acc :  0.417858817064\n",
      "degree=8, lambda=0.000, Training RMSE=2.329, Testing RMSE=516.472\n",
      "train acc :  0.525476691538\n",
      "validation acc :  0.58629552398\n",
      "degree=8, lambda=0.000, Training RMSE=2.263, Testing RMSE=156.616\n",
      "train acc :  0.598431222499\n",
      "validation acc :  0.52495029339\n",
      "degree=8, lambda=0.000, Training RMSE=4.389, Testing RMSE=199.172\n",
      "train acc :  0.461896562719\n",
      "validation acc :  0.411764705882\n",
      "degree=8, lambda=0.000, Training RMSE=4.227, Testing RMSE=179.027\n",
      "train acc :  0.621644027804\n",
      "validation acc :  0.423209349692\n",
      "degree=8, lambda=0.000, Training RMSE=4.330, Testing RMSE=1056.067\n",
      "train acc :  0.538358523053\n",
      "validation acc :  0.588445435883\n",
      "degree=8, lambda=0.000, Training RMSE=7.629, Testing RMSE=564.418\n",
      "train acc :  0.555321726931\n",
      "validation acc :  0.412605272942\n",
      "degree=8, lambda=0.000, Training RMSE=12.144, Testing RMSE=247.256\n",
      "train acc :  0.567310758242\n",
      "validation acc :  0.419911740459\n",
      "degree=8, lambda=0.000, Training RMSE=23.532, Testing RMSE=1662.028\n",
      "train acc :  0.593839678592\n",
      "validation acc :  0.590272052762\n",
      "degree=8, lambda=0.000, Training RMSE=26.481, Testing RMSE=327.567\n",
      "train acc :  0.425801925898\n",
      "validation acc :  0.573897159853\n",
      "degree=8, lambda=0.000, Training RMSE=3.527, Testing RMSE=517.524\n",
      "train acc :  0.601619794656\n",
      "validation acc :  0.589512309458\n",
      "degree=8, lambda=0.000, Training RMSE=10.503, Testing RMSE=550.950\n",
      "train acc :  0.570818187616\n",
      "validation acc :  0.590886313305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=8, lambda=0.001, Training RMSE=5.761, Testing RMSE=135.099\n",
      "train acc :  0.399273005548\n",
      "validation acc :  0.578633431938\n",
      "degree=8, lambda=0.001, Training RMSE=2.923, Testing RMSE=69.762\n",
      "train acc :  0.439449014731\n",
      "validation acc :  0.436626093141\n",
      "degree=8, lambda=0.001, Training RMSE=1.750, Testing RMSE=38.486\n",
      "train acc :  0.516484918054\n",
      "validation acc :  0.513133860304\n",
      "degree=8, lambda=0.002, Training RMSE=0.896, Testing RMSE=13.497\n",
      "train acc :  0.712071934188\n",
      "validation acc :  0.710586295524\n",
      "degree=8, lambda=0.002, Training RMSE=0.981, Testing RMSE=24.393\n",
      "train acc :  0.71028633378\n",
      "validation acc :  0.709228456428\n",
      "degree=8, lambda=0.003, Training RMSE=0.830, Testing RMSE=39.834\n",
      "train acc :  0.74676359926\n",
      "validation acc :  0.741816594734\n",
      "degree=8, lambda=0.004, Training RMSE=0.827, Testing RMSE=47.144\n",
      "train acc :  0.756648172948\n",
      "validation acc :  0.747668234648\n",
      "degree=8, lambda=0.005, Training RMSE=0.834, Testing RMSE=46.298\n",
      "train acc :  0.756903258721\n",
      "validation acc :  0.747926870666\n",
      "degree=8, lambda=0.007, Training RMSE=0.846, Testing RMSE=38.424\n",
      "train acc :  0.73885594031\n",
      "validation acc :  0.730808399205\n",
      "degree=8, lambda=0.009, Training RMSE=0.863, Testing RMSE=25.940\n",
      "train acc :  0.722020279319\n",
      "validation acc :  0.715435720867\n",
      "degree=8, lambda=0.013, Training RMSE=0.874, Testing RMSE=13.272\n",
      "train acc :  0.705567246987\n",
      "validation acc :  0.700871280087\n",
      "degree=8, lambda=0.017, Training RMSE=0.885, Testing RMSE=8.619\n",
      "train acc :  0.691665072381\n",
      "validation acc :  0.689960073065\n",
      "degree=8, lambda=0.023, Training RMSE=0.895, Testing RMSE=13.277\n",
      "train acc :  0.683438556215\n",
      "validation acc :  0.679954092107\n",
      "degree=8, lambda=0.031, Training RMSE=0.903, Testing RMSE=17.389\n",
      "train acc :  0.675913525923\n",
      "validation acc :  0.672453647576\n",
      "degree=8, lambda=0.041, Training RMSE=0.909, Testing RMSE=19.641\n",
      "train acc :  0.671704610675\n",
      "validation acc :  0.668509448297\n",
      "degree=8, lambda=0.055, Training RMSE=0.915, Testing RMSE=20.433\n",
      "train acc :  0.669600153051\n",
      "validation acc :  0.666311042142\n",
      "degree=8, lambda=0.074, Training RMSE=0.920, Testing RMSE=19.825\n",
      "train acc :  0.668133409859\n",
      "validation acc :  0.665001697299\n",
      "degree=8, lambda=0.100, Training RMSE=0.926, Testing RMSE=17.544\n",
      "train acc :  0.66794209553\n",
      "validation acc :  0.665001697299\n",
      "degree=9, lambda=0.000, Training RMSE=13.080, Testing RMSE=198780.282\n",
      "train acc :  0.420827753332\n",
      "validation acc :  0.474936553352\n",
      "degree=9, lambda=0.000, Training RMSE=10.540, Testing RMSE=17580.181\n",
      "train acc :  0.575728588738\n",
      "validation acc :  0.497340898437\n",
      "degree=9, lambda=0.000, Training RMSE=18.409, Testing RMSE=22392.742\n",
      "train acc :  0.616223455137\n",
      "validation acc :  0.491246787256\n",
      "degree=9, lambda=0.000, Training RMSE=18.248, Testing RMSE=16166.079\n",
      "train acc :  0.42261335374\n",
      "validation acc :  0.503030890839\n",
      "degree=9, lambda=0.000, Training RMSE=27.764, Testing RMSE=11634.628\n",
      "train acc :  0.606211338563\n",
      "validation acc :  0.503952281655\n",
      "degree=9, lambda=0.000, Training RMSE=30.385, Testing RMSE=11627.703\n",
      "train acc :  0.390536317837\n",
      "validation acc :  0.491634741283\n",
      "degree=9, lambda=0.000, Training RMSE=335.522, Testing RMSE=34175.055\n",
      "train acc :  0.382883744659\n",
      "validation acc :  0.47632672195\n",
      "degree=9, lambda=0.000, Training RMSE=19.624, Testing RMSE=5076.215\n",
      "train acc :  0.592054078184\n",
      "validation acc :  0.532240596156\n",
      "degree=9, lambda=0.000, Training RMSE=22.413, Testing RMSE=2243.162\n",
      "train acc :  0.393916204324\n",
      "validation acc :  0.446890710118\n",
      "degree=9, lambda=0.000, Training RMSE=10.047, Testing RMSE=508.778\n",
      "train acc :  0.602002423315\n",
      "validation acc :  0.551266508252\n",
      "degree=9, lambda=0.000, Training RMSE=13.890, Testing RMSE=674.180\n",
      "train acc :  0.448313245329\n",
      "validation acc :  0.431615020287\n",
      "degree=9, lambda=0.000, Training RMSE=6.015, Testing RMSE=957.846\n",
      "train acc :  0.607933167528\n",
      "validation acc :  0.586699642759\n",
      "degree=9, lambda=0.000, Training RMSE=10.367, Testing RMSE=264.086\n",
      "train acc :  0.501243543141\n",
      "validation acc :  0.427751644763\n",
      "degree=9, lambda=0.000, Training RMSE=7.804, Testing RMSE=783.101\n",
      "train acc :  0.474650851349\n",
      "validation acc :  0.585923734704\n",
      "degree=9, lambda=0.000, Training RMSE=9.107, Testing RMSE=149.034\n",
      "train acc :  0.635227345195\n",
      "validation acc :  0.581429933886\n",
      "degree=9, lambda=0.000, Training RMSE=13.222, Testing RMSE=1272.696\n",
      "train acc :  0.503220457879\n",
      "validation acc :  0.588477765385\n",
      "degree=9, lambda=0.000, Training RMSE=16.323, Testing RMSE=841.360\n",
      "train acc :  0.475798737325\n",
      "validation acc :  0.582949420494\n",
      "degree=9, lambda=0.000, Training RMSE=20.847, Testing RMSE=1414.624\n",
      "train acc :  0.462661820037\n",
      "validation acc :  0.587621033574\n",
      "degree=9, lambda=0.000, Training RMSE=80.981, Testing RMSE=1388.929\n",
      "train acc :  0.410496779542\n",
      "validation acc :  0.404991675153\n",
      "degree=9, lambda=0.000, Training RMSE=83.295, Testing RMSE=1097.937\n",
      "train acc :  0.393661118551\n",
      "validation acc :  0.573784006595\n",
      "degree=9, lambda=0.000, Training RMSE=16.542, Testing RMSE=1339.592\n",
      "train acc :  0.501498628914\n",
      "validation acc :  0.588057481855\n",
      "degree=9, lambda=0.000, Training RMSE=197.895, Testing RMSE=15845.454\n",
      "train acc :  0.515209489191\n",
      "validation acc :  0.41173237638\n",
      "degree=9, lambda=0.001, Training RMSE=300.902, Testing RMSE=3230.847\n",
      "train acc :  0.607295453096\n",
      "validation acc :  0.599081842135\n",
      "degree=9, lambda=0.001, Training RMSE=76.178, Testing RMSE=4158.206\n",
      "train acc :  0.616861169568\n",
      "validation acc :  0.591290432084\n",
      "degree=9, lambda=0.001, Training RMSE=14.972, Testing RMSE=333.727\n",
      "train acc :  0.443594158536\n",
      "validation acc :  0.407578035336\n",
      "degree=9, lambda=0.002, Training RMSE=6.526, Testing RMSE=430.613\n",
      "train acc :  0.55774504177\n",
      "validation acc :  0.413122544978\n",
      "degree=9, lambda=0.002, Training RMSE=614.000, Testing RMSE=618.309\n",
      "train acc :  0.40418340667\n",
      "validation acc :  0.403342870537\n",
      "degree=9, lambda=0.003, Training RMSE=418.323, Testing RMSE=418.646\n",
      "train acc :  0.404055863784\n",
      "validation acc :  0.403472188546\n",
      "degree=9, lambda=0.004, Training RMSE=351.053, Testing RMSE=352.437\n",
      "train acc :  0.605509852688\n",
      "validation acc :  0.600536669738\n",
      "degree=9, lambda=0.005, Training RMSE=222.265, Testing RMSE=230.225\n",
      "train acc :  0.491741598112\n",
      "validation acc :  0.490212243183\n",
      "degree=9, lambda=0.007, Training RMSE=7.576, Testing RMSE=105.464\n",
      "train acc :  0.494101141509\n",
      "validation acc :  0.492135848569\n",
      "degree=9, lambda=0.009, Training RMSE=5.120, Testing RMSE=137.375\n",
      "train acc :  0.4513742746\n",
      "validation acc :  0.445743012786\n",
      "degree=9, lambda=0.013, Training RMSE=3.201, Testing RMSE=150.031\n",
      "train acc :  0.426503411772\n",
      "validation acc :  0.428656870827\n",
      "degree=9, lambda=0.017, Training RMSE=1.855, Testing RMSE=144.733\n",
      "train acc :  0.653848606594\n",
      "validation acc :  0.65342773548\n",
      "degree=9, lambda=0.023, Training RMSE=1.081, Testing RMSE=126.001\n",
      "train acc :  0.537593265736\n",
      "validation acc :  0.539450075166\n",
      "degree=9, lambda=0.031, Training RMSE=0.895, Testing RMSE=97.651\n",
      "train acc :  0.6928767298\n",
      "validation acc :  0.681279601701\n",
      "degree=9, lambda=0.041, Training RMSE=0.966, Testing RMSE=63.234\n",
      "train acc :  0.651170205982\n",
      "validation acc :  0.65187591937\n",
      "degree=9, lambda=0.055, Training RMSE=0.909, Testing RMSE=29.893\n",
      "train acc :  0.668898667177\n",
      "validation acc :  0.666359536395\n",
      "degree=9, lambda=0.074, Training RMSE=0.909, Testing RMSE=22.837\n",
      "train acc :  0.677252726229\n",
      "validation acc :  0.674555065225\n",
      "degree=9, lambda=0.100, Training RMSE=0.918, Testing RMSE=43.103\n",
      "train acc :  0.675212040048\n",
      "validation acc :  0.672098023051\n",
      "degree=10, lambda=0.000, Training RMSE=78.306, Testing RMSE=193607.560\n",
      "train acc :  0.544034181494\n",
      "validation acc :  0.494592890742\n",
      "degree=10, lambda=0.000, Training RMSE=330.363, Testing RMSE=556637.181\n",
      "train acc :  0.600726994452\n",
      "validation acc :  0.502448959798\n",
      "degree=10, lambda=0.000, Training RMSE=255.075, Testing RMSE=324097.863\n",
      "train acc :  0.590587334991\n",
      "validation acc :  0.49921600957\n",
      "degree=10, lambda=0.000, Training RMSE=133.102, Testing RMSE=139638.432\n",
      "train acc :  0.426886040431\n",
      "validation acc :  0.476714675978\n",
      "degree=10, lambda=0.000, Training RMSE=115.709, Testing RMSE=58035.139\n",
      "train acc :  0.479561252471\n",
      "validation acc :  0.477425925028\n",
      "degree=10, lambda=0.000, Training RMSE=68.091, Testing RMSE=22774.364\n",
      "train acc :  0.576557617499\n",
      "validation acc :  0.506522477086\n",
      "degree=10, lambda=0.000, Training RMSE=30.148, Testing RMSE=8179.483\n",
      "train acc :  0.530578406989\n",
      "validation acc :  0.546627224674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=10, lambda=0.000, Training RMSE=27.268, Testing RMSE=4804.683\n",
      "train acc :  0.384605573624\n",
      "validation acc :  0.465431679679\n",
      "degree=10, lambda=0.000, Training RMSE=18.068, Testing RMSE=2944.277\n",
      "train acc :  0.55226069766\n",
      "validation acc :  0.531755653622\n",
      "degree=10, lambda=0.000, Training RMSE=11.932, Testing RMSE=2787.538\n",
      "train acc :  0.499968114278\n",
      "validation acc :  0.450155989849\n",
      "degree=10, lambda=0.000, Training RMSE=23.686, Testing RMSE=2233.268\n",
      "train acc :  0.619284484408\n",
      "validation acc :  0.568659780483\n",
      "degree=10, lambda=0.000, Training RMSE=50.913, Testing RMSE=2846.838\n",
      "train acc :  0.40195140616\n",
      "validation acc :  0.582529136964\n",
      "degree=10, lambda=0.000, Training RMSE=12.284, Testing RMSE=3132.721\n",
      "train acc :  0.46980422167\n",
      "validation acc :  0.410956468325\n",
      "degree=10, lambda=0.000, Training RMSE=28.146, Testing RMSE=1758.888\n",
      "train acc :  0.559530642178\n",
      "validation acc :  0.472172380906\n",
      "degree=10, lambda=0.000, Training RMSE=11.808, Testing RMSE=2251.672\n",
      "train acc :  0.49493017027\n",
      "validation acc :  0.588736401403\n",
      "degree=10, lambda=0.000, Training RMSE=23.120, Testing RMSE=2844.789\n",
      "train acc :  0.597602193738\n",
      "validation acc :  0.412977062218\n",
      "degree=10, lambda=0.000, Training RMSE=9.661, Testing RMSE=2174.973\n",
      "train acc :  0.462151648492\n",
      "validation acc :  0.410762491311\n",
      "degree=10, lambda=0.000, Training RMSE=6.245, Testing RMSE=1748.870\n",
      "train acc :  0.547095210765\n",
      "validation acc :  0.412556778688\n",
      "degree=10, lambda=0.000, Training RMSE=11.170, Testing RMSE=1639.229\n",
      "train acc :  0.512977488681\n",
      "validation acc :  0.413122544978\n",
      "degree=10, lambda=0.000, Training RMSE=16.369, Testing RMSE=1862.587\n",
      "train acc :  0.557298641668\n",
      "validation acc :  0.589916428237\n",
      "degree=10, lambda=0.000, Training RMSE=30.506, Testing RMSE=1848.430\n",
      "train acc :  0.442573815445\n",
      "validation acc :  0.409986583257\n",
      "degree=10, lambda=0.000, Training RMSE=21.013, Testing RMSE=1707.038\n",
      "train acc :  0.554110069511\n",
      "validation acc :  0.412766920453\n",
      "degree=10, lambda=0.001, Training RMSE=6.720, Testing RMSE=1120.815\n",
      "train acc :  0.547286525094\n",
      "validation acc :  0.588962707919\n",
      "degree=10, lambda=0.001, Training RMSE=10.939, Testing RMSE=568.894\n",
      "train acc :  0.490976340795\n",
      "validation acc :  0.587685692579\n",
      "degree=10, lambda=0.001, Training RMSE=7.021, Testing RMSE=673.506\n",
      "train acc :  0.509087430649\n",
      "validation acc :  0.588154470362\n",
      "degree=10, lambda=0.002, Training RMSE=35.637, Testing RMSE=819.327\n",
      "train acc :  0.4858108539\n",
      "validation acc :  0.417341545027\n",
      "degree=10, lambda=0.002, Training RMSE=20.084, Testing RMSE=1371.670\n",
      "train acc :  0.5923729354\n",
      "validation acc :  0.591258102582\n",
      "degree=10, lambda=0.003, Training RMSE=51.613, Testing RMSE=1982.276\n",
      "train acc :  0.484089024935\n",
      "validation acc :  0.587475550814\n",
      "degree=10, lambda=0.004, Training RMSE=1404.659, Testing RMSE=38389.498\n",
      "train acc :  0.510299088068\n",
      "validation acc :  0.412783085204\n",
      "degree=10, lambda=0.005, Training RMSE=1076.884, Testing RMSE=1642.527\n",
      "train acc :  0.495504113258\n",
      "validation acc :  0.493412863909\n",
      "degree=10, lambda=0.007, Training RMSE=1902.959, Testing RMSE=2187.907\n",
      "train acc :  0.382054715898\n",
      "validation acc :  0.38077687794\n",
      "degree=10, lambda=0.009, Training RMSE=471.340, Testing RMSE=981.294\n",
      "train acc :  0.604680823927\n",
      "validation acc :  0.604739505035\n",
      "degree=10, lambda=0.013, Training RMSE=456.029, Testing RMSE=736.604\n",
      "train acc :  0.575537274409\n",
      "validation acc :  0.567851542925\n",
      "degree=10, lambda=0.017, Training RMSE=797.866, Testing RMSE=843.694\n",
      "train acc :  0.609973853708\n",
      "validation acc :  0.609879895899\n",
      "degree=10, lambda=0.023, Training RMSE=594.540, Testing RMSE=603.133\n",
      "train acc :  0.485045596582\n",
      "validation acc :  0.487868354267\n",
      "degree=10, lambda=0.031, Training RMSE=538.092, Testing RMSE=602.259\n",
      "train acc :  0.490466169249\n",
      "validation acc :  0.487464235488\n",
      "degree=10, lambda=0.041, Training RMSE=24.271, Testing RMSE=415.517\n",
      "train acc :  0.52815509215\n",
      "validation acc :  0.525095776151\n",
      "degree=10, lambda=0.055, Training RMSE=8.648, Testing RMSE=490.386\n",
      "train acc :  0.506409030036\n",
      "validation acc :  0.504776683963\n",
      "degree=10, lambda=0.074, Training RMSE=20.793, Testing RMSE=506.592\n",
      "train acc :  0.421019067662\n",
      "validation acc :  0.42007338797\n",
      "degree=10, lambda=0.100, Training RMSE=6.370, Testing RMSE=469.344\n",
      "train acc :  0.53261909317\n",
      "validation acc :  0.526453615247\n",
      "degree=11, lambda=0.000, Training RMSE=7.372, Testing RMSE=346815.061\n",
      "train acc :  0.550156240036\n",
      "validation acc :  0.502448959798\n",
      "degree=11, lambda=0.000, Training RMSE=10.220, Testing RMSE=470566.011\n",
      "train acc :  0.590714877878\n",
      "validation acc :  0.49384931219\n",
      "degree=11, lambda=0.000, Training RMSE=6.558, Testing RMSE=50902.067\n",
      "train acc :  0.514635546202\n",
      "validation acc :  0.497922829478\n",
      "degree=11, lambda=0.000, Training RMSE=10.462, Testing RMSE=18903.647\n",
      "train acc :  0.582233275939\n",
      "validation acc :  0.49921600957\n",
      "degree=11, lambda=0.000, Training RMSE=16.279, Testing RMSE=14421.958\n",
      "train acc :  0.442637586889\n",
      "validation acc :  0.511274913923\n",
      "degree=11, lambda=0.000, Training RMSE=11.138, Testing RMSE=13297.895\n",
      "train acc :  0.509470059307\n",
      "validation acc :  0.541470669059\n",
      "degree=11, lambda=0.000, Training RMSE=11.828, Testing RMSE=12719.714\n",
      "train acc :  0.605318538359\n",
      "validation acc :  0.529508753213\n",
      "degree=11, lambda=0.000, Training RMSE=4.576, Testing RMSE=11561.191\n",
      "train acc :  0.542057266756\n",
      "validation acc :  0.476407545706\n",
      "degree=11, lambda=0.000, Training RMSE=18.059, Testing RMSE=13608.491\n",
      "train acc :  0.476308908871\n",
      "validation acc :  0.43929327708\n",
      "degree=11, lambda=0.000, Training RMSE=15.151, Testing RMSE=11399.529\n",
      "train acc :  0.578534532236\n",
      "validation acc :  0.56112700645\n",
      "degree=11, lambda=0.000, Training RMSE=11.917, Testing RMSE=11181.908\n",
      "train acc :  0.409667750781\n",
      "validation acc :  0.556681699885\n",
      "degree=11, lambda=0.000, Training RMSE=35.531, Testing RMSE=11109.302\n",
      "train acc :  0.570052930298\n",
      "validation acc :  0.577016956824\n",
      "degree=11, lambda=0.000, Training RMSE=65.063, Testing RMSE=11340.330\n",
      "train acc :  0.448377016772\n",
      "validation acc :  0.579910447279\n",
      "degree=11, lambda=0.000, Training RMSE=100.793, Testing RMSE=28839.672\n",
      "train acc :  0.470250621772\n",
      "validation acc :  0.588429271131\n",
      "degree=11, lambda=0.000, Training RMSE=90.471, Testing RMSE=19582.742\n",
      "train acc :  0.395638033289\n",
      "validation acc :  0.410697832307\n",
      "degree=11, lambda=0.000, Training RMSE=35.142, Testing RMSE=18902.430\n",
      "train acc :  0.440660672151\n",
      "validation acc :  0.411344422353\n",
      "degree=11, lambda=0.000, Training RMSE=25.714, Testing RMSE=11560.564\n",
      "train acc :  0.466296792296\n",
      "validation acc :  0.588041317104\n",
      "degree=11, lambda=0.000, Training RMSE=30.487, Testing RMSE=10088.803\n",
      "train acc :  0.596390536318\n",
      "validation acc :  0.589916428237\n",
      "degree=11, lambda=0.000, Training RMSE=19.545, Testing RMSE=10589.603\n",
      "train acc :  0.566226643709\n",
      "validation acc :  0.411861694389\n",
      "degree=11, lambda=0.000, Training RMSE=24.283, Testing RMSE=10182.533\n",
      "train acc :  0.450927874498\n",
      "validation acc :  0.411522234615\n",
      "degree=11, lambda=0.000, Training RMSE=8.344, Testing RMSE=5719.162\n",
      "train acc :  0.431222498565\n",
      "validation acc :  0.587718022081\n",
      "degree=11, lambda=0.000, Training RMSE=8.691, Testing RMSE=3147.736\n",
      "train acc :  0.61042025381\n",
      "validation acc :  0.589205179186\n",
      "degree=11, lambda=0.001, Training RMSE=11.697, Testing RMSE=505.196\n",
      "train acc :  0.395446718959\n",
      "validation acc :  0.400788839856\n",
      "degree=11, lambda=0.001, Training RMSE=17.131, Testing RMSE=3289.464\n",
      "train acc :  0.485938396786\n",
      "validation acc :  0.410649338053\n",
      "degree=11, lambda=0.001, Training RMSE=5.699, Testing RMSE=5655.691\n",
      "train acc :  0.431860212997\n",
      "validation acc :  0.585196320903\n",
      "degree=11, lambda=0.002, Training RMSE=10.013, Testing RMSE=7078.487\n",
      "train acc :  0.603851795166\n",
      "validation acc :  0.589851769232\n",
      "degree=11, lambda=0.002, Training RMSE=16.040, Testing RMSE=7330.922\n",
      "train acc :  0.567565844015\n",
      "validation acc :  0.590611512536\n",
      "degree=11, lambda=0.003, Training RMSE=14.342, Testing RMSE=6585.889\n",
      "train acc :  0.607678081755\n",
      "validation acc :  0.425246108336\n",
      "degree=11, lambda=0.004, Training RMSE=17.847, Testing RMSE=5217.126\n",
      "train acc :  0.410560550985\n",
      "validation acc :  0.51589803275\n",
      "degree=11, lambda=0.005, Training RMSE=124.758, Testing RMSE=6457.820\n",
      "train acc :  0.424271411262\n",
      "validation acc :  0.58529330941\n",
      "degree=11, lambda=0.007, Training RMSE=79.536, Testing RMSE=2011.170\n",
      "train acc :  0.56463235763\n",
      "validation acc :  0.419038843897\n",
      "degree=11, lambda=0.009, Training RMSE=882.689, Testing RMSE=50816.280\n",
      "train acc :  0.50506982973\n",
      "validation acc :  0.588542424389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=11, lambda=0.013, Training RMSE=5.160, Testing RMSE=1288.635\n",
      "train acc :  0.461067533958\n",
      "validation acc :  0.463330262031\n",
      "degree=11, lambda=0.017, Training RMSE=4.451, Testing RMSE=2103.001\n",
      "train acc :  0.524328805561\n",
      "validation acc :  0.526696086514\n",
      "degree=11, lambda=0.023, Training RMSE=6.841, Testing RMSE=2535.072\n",
      "train acc :  0.61163191123\n",
      "validation acc :  0.61144787676\n",
      "degree=11, lambda=0.031, Training RMSE=7.885, Testing RMSE=2586.522\n",
      "train acc :  0.468082392705\n",
      "validation acc :  0.470167951764\n",
      "degree=11, lambda=0.041, Training RMSE=5.323, Testing RMSE=2291.835\n",
      "train acc :  0.433071870416\n",
      "validation acc :  0.436335127621\n",
      "degree=11, lambda=0.055, Training RMSE=4.377, Testing RMSE=1734.192\n",
      "train acc :  0.414131751802\n",
      "validation acc :  0.410212889773\n",
      "degree=11, lambda=0.074, Training RMSE=6.699, Testing RMSE=1052.173\n",
      "train acc :  0.412983865825\n",
      "validation acc :  0.414545043079\n",
      "degree=11, lambda=0.100, Training RMSE=9.731, Testing RMSE=395.479\n",
      "train acc :  0.422422039411\n",
      "validation acc :  0.421900004849\n",
      "Best params for Ridge regression : degree =  7 , lambda =  0.000661474064123 , accuracy =  0.778866204355\n",
      "************ Model 3 ************* \n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.7326717936\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.7327958323\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.732919871\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73309352518\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73311833292\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73319275614\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73326717936\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.7332919871\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73331679484\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73324237162\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73321756388\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73321756388\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73321756388\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73326717936\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735028304698\n",
      "validation acc :  0.73324237162\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735127619426\n",
      "validation acc :  0.7332919871\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735226934154\n",
      "validation acc :  0.73321756388\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735226934154\n",
      "validation acc :  0.73309352518\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735425563611\n",
      "validation acc :  0.732919871\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735922137253\n",
      "validation acc :  0.732919871\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735425563611\n",
      "validation acc :  0.73277102456\n",
      "degree=1, lambda=0.000, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.735326248883\n",
      "validation acc :  0.73284544778\n",
      "degree=1, lambda=0.001, Training RMSE=0.841, Testing RMSE=0.845\n",
      "train acc :  0.734928989969\n",
      "validation acc :  0.7325477549\n",
      "degree=1, lambda=0.001, Training RMSE=0.842, Testing RMSE=0.845\n",
      "train acc :  0.734134472142\n",
      "validation acc :  0.731580253039\n",
      "degree=1, lambda=0.001, Training RMSE=0.843, Testing RMSE=0.846\n",
      "train acc :  0.732346807031\n",
      "validation acc :  0.730612751178\n",
      "degree=1, lambda=0.002, Training RMSE=0.846, Testing RMSE=0.849\n",
      "train acc :  0.729863938822\n",
      "validation acc :  0.728628131977\n",
      "degree=1, lambda=0.002, Training RMSE=0.850, Testing RMSE=0.853\n",
      "train acc :  0.724997517132\n",
      "validation acc :  0.725130240635\n",
      "degree=1, lambda=0.003, Training RMSE=0.856, Testing RMSE=0.858\n",
      "train acc :  0.72082629854\n",
      "validation acc :  0.721185809973\n",
      "degree=1, lambda=0.004, Training RMSE=0.863, Testing RMSE=0.866\n",
      "train acc :  0.717548912504\n",
      "validation acc :  0.717985611511\n",
      "degree=1, lambda=0.005, Training RMSE=0.874, Testing RMSE=0.876\n",
      "train acc :  0.715463303208\n",
      "validation acc :  0.715132721409\n",
      "degree=1, lambda=0.007, Training RMSE=0.889, Testing RMSE=0.891\n",
      "train acc :  0.71288112027\n",
      "validation acc :  0.711634830067\n",
      "degree=1, lambda=0.009, Training RMSE=0.908, Testing RMSE=0.910\n",
      "train acc :  0.707021551296\n",
      "validation acc :  0.708707516745\n",
      "degree=1, lambda=0.013, Training RMSE=0.931, Testing RMSE=0.932\n",
      "train acc :  0.706822921839\n",
      "validation acc :  0.706053088564\n",
      "degree=1, lambda=0.017, Training RMSE=0.952, Testing RMSE=0.952\n",
      "train acc :  0.703048962161\n",
      "validation acc :  0.705060778963\n",
      "degree=1, lambda=0.023, Training RMSE=0.969, Testing RMSE=0.969\n",
      "train acc :  0.702055814877\n",
      "validation acc :  0.703522699082\n",
      "degree=1, lambda=0.031, Training RMSE=0.981, Testing RMSE=0.981\n",
      "train acc :  0.701062667594\n",
      "validation acc :  0.702778466882\n",
      "degree=1, lambda=0.041, Training RMSE=0.989, Testing RMSE=0.989\n",
      "train acc :  0.699771576125\n",
      "validation acc :  0.702356735301\n",
      "degree=1, lambda=0.055, Training RMSE=0.994, Testing RMSE=0.994\n",
      "train acc :  0.699870890853\n",
      "validation acc :  0.702232696601\n",
      "degree=1, lambda=0.074, Training RMSE=0.996, Testing RMSE=0.996\n",
      "train acc :  0.69947363194\n",
      "validation acc :  0.702183081121\n",
      "degree=1, lambda=0.100, Training RMSE=0.998, Testing RMSE=0.998\n",
      "train acc :  0.699771576125\n",
      "validation acc :  0.702232696601\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.861\n",
      "train acc :  0.78071307975\n",
      "validation acc :  0.746192011908\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.836\n",
      "train acc :  0.780613765021\n",
      "validation acc :  0.757628380055\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.821\n",
      "train acc :  0.780514450293\n",
      "validation acc :  0.764276854379\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.815\n",
      "train acc :  0.780216506108\n",
      "validation acc :  0.768866286281\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.813\n",
      "train acc :  0.780315820836\n",
      "validation acc :  0.769883403622\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.812\n",
      "train acc :  0.780415135565\n",
      "validation acc :  0.771272637063\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.812\n",
      "train acc :  0.780415135565\n",
      "validation acc :  0.771098982883\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.812\n",
      "train acc :  0.780415135565\n",
      "validation acc :  0.770826097742\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.812\n",
      "train acc :  0.780415135565\n",
      "validation acc :  0.770801290002\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.812\n",
      "train acc :  0.780415135565\n",
      "validation acc :  0.771123790623\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.812\n",
      "train acc :  0.780415135565\n",
      "validation acc :  0.771173406103\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.812\n",
      "train acc :  0.780415135565\n",
      "validation acc :  0.771247829323\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.812\n",
      "train acc :  0.780415135565\n",
      "validation acc :  0.771198213843\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.812\n",
      "train acc :  0.780315820836\n",
      "validation acc :  0.771148598363\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.811\n",
      "train acc :  0.780216506108\n",
      "validation acc :  0.771148598363\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.811\n",
      "train acc :  0.780216506108\n",
      "validation acc :  0.771049367403\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.811\n",
      "train acc :  0.780415135565\n",
      "validation acc :  0.770974944183\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.811\n",
      "train acc :  0.780117191379\n",
      "validation acc :  0.770950136443\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.811\n",
      "train acc :  0.779918561923\n",
      "validation acc :  0.770900520963\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.811\n",
      "train acc :  0.780017876651\n",
      "validation acc :  0.770925328703\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.811\n",
      "train acc :  0.779719932466\n",
      "validation acc :  0.770329942942\n",
      "degree=2, lambda=0.000, Training RMSE=0.799, Testing RMSE=0.811\n",
      "train acc :  0.779421988281\n",
      "validation acc :  0.769808980402\n",
      "degree=2, lambda=0.001, Training RMSE=0.800, Testing RMSE=0.812\n",
      "train acc :  0.778925414639\n",
      "validation acc :  0.769461672042\n",
      "degree=2, lambda=0.001, Training RMSE=0.801, Testing RMSE=0.812\n",
      "train acc :  0.776541861158\n",
      "validation acc :  0.768444554701\n",
      "degree=2, lambda=0.001, Training RMSE=0.802, Testing RMSE=0.813\n",
      "train acc :  0.776541861158\n",
      "validation acc :  0.76717935996\n",
      "degree=2, lambda=0.002, Training RMSE=0.805, Testing RMSE=0.815\n",
      "train acc :  0.774058992949\n",
      "validation acc :  0.765120317539\n",
      "degree=2, lambda=0.002, Training RMSE=0.809, Testing RMSE=0.818\n",
      "train acc :  0.7696891449\n",
      "validation acc :  0.763433391218\n",
      "degree=2, lambda=0.003, Training RMSE=0.816, Testing RMSE=0.824\n",
      "train acc :  0.763829575926\n",
      "validation acc :  0.759960307616\n",
      "degree=2, lambda=0.004, Training RMSE=0.826, Testing RMSE=0.833\n",
      "train acc :  0.75806932168\n",
      "validation acc :  0.752567601092\n",
      "degree=2, lambda=0.005, Training RMSE=0.841, Testing RMSE=0.846\n",
      "train acc :  0.750819346509\n",
      "validation acc :  0.746688166708\n",
      "degree=2, lambda=0.007, Training RMSE=0.860, Testing RMSE=0.863\n",
      "train acc :  0.745555665905\n",
      "validation acc :  0.741726618705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=2, lambda=0.009, Training RMSE=0.881, Testing RMSE=0.884\n",
      "train acc :  0.739398152746\n",
      "validation acc :  0.737707764823\n",
      "degree=2, lambda=0.013, Training RMSE=0.904, Testing RMSE=0.906\n",
      "train acc :  0.7336378985\n",
      "validation acc :  0.736045646242\n",
      "degree=2, lambda=0.017, Training RMSE=0.927, Testing RMSE=0.927\n",
      "train acc :  0.733737213229\n",
      "validation acc :  0.73406102704\n",
      "degree=2, lambda=0.023, Training RMSE=0.946, Testing RMSE=0.947\n",
      "train acc :  0.730956400834\n",
      "validation acc :  0.73287025552\n",
      "degree=2, lambda=0.031, Training RMSE=0.963, Testing RMSE=0.963\n",
      "train acc :  0.726487238057\n",
      "validation acc :  0.731704291739\n",
      "degree=2, lambda=0.041, Training RMSE=0.976, Testing RMSE=0.976\n",
      "train acc :  0.724699572947\n",
      "validation acc :  0.728231208137\n",
      "degree=2, lambda=0.055, Training RMSE=0.985, Testing RMSE=0.985\n",
      "train acc :  0.722415334194\n",
      "validation acc :  0.726742743736\n",
      "degree=2, lambda=0.074, Training RMSE=0.991, Testing RMSE=0.991\n",
      "train acc :  0.721024927997\n",
      "validation acc :  0.724237161995\n",
      "degree=2, lambda=0.100, Training RMSE=0.995, Testing RMSE=0.995\n",
      "train acc :  0.72082629854\n",
      "validation acc :  0.723815430414\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.886\n",
      "train acc :  0.791141126229\n",
      "validation acc :  0.747804515009\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.847\n",
      "train acc :  0.791538385143\n",
      "validation acc :  0.762416273877\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.825\n",
      "train acc :  0.791637699871\n",
      "validation acc :  0.772959563384\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.815\n",
      "train acc :  0.791439070414\n",
      "validation acc :  0.777276110146\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.811\n",
      "train acc :  0.791538385143\n",
      "validation acc :  0.780302654428\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.810\n",
      "train acc :  0.791439070414\n",
      "validation acc :  0.779930538328\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.810\n",
      "train acc :  0.791439070414\n",
      "validation acc :  0.780898040189\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.810\n",
      "train acc :  0.791439070414\n",
      "validation acc :  0.781667080129\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.810\n",
      "train acc :  0.791439070414\n",
      "validation acc :  0.781592656909\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.810\n",
      "train acc :  0.791439070414\n",
      "validation acc :  0.781394194989\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.810\n",
      "train acc :  0.791439070414\n",
      "validation acc :  0.781245348549\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.810\n",
      "train acc :  0.791538385143\n",
      "validation acc :  0.781543041429\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.809\n",
      "train acc :  0.791538385143\n",
      "validation acc :  0.781543041429\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.809\n",
      "train acc :  0.791737014599\n",
      "validation acc :  0.781443810469\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.809\n",
      "train acc :  0.791935644056\n",
      "validation acc :  0.781394194989\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.809\n",
      "train acc :  0.791935644056\n",
      "validation acc :  0.781617464649\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.809\n",
      "train acc :  0.792134273513\n",
      "validation acc :  0.781691887869\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.809\n",
      "train acc :  0.79233290297\n",
      "validation acc :  0.781815926569\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.809\n",
      "train acc :  0.792531532426\n",
      "validation acc :  0.781989580749\n",
      "degree=3, lambda=0.000, Training RMSE=0.784, Testing RMSE=0.809\n",
      "train acc :  0.792531532426\n",
      "validation acc :  0.781691887869\n",
      "degree=3, lambda=0.000, Training RMSE=0.785, Testing RMSE=0.808\n",
      "train acc :  0.791737014599\n",
      "validation acc :  0.781592656909\n",
      "degree=3, lambda=0.000, Training RMSE=0.785, Testing RMSE=0.808\n",
      "train acc :  0.791836329328\n",
      "validation acc :  0.781741503349\n",
      "degree=3, lambda=0.001, Training RMSE=0.786, Testing RMSE=0.807\n",
      "train acc :  0.791339755686\n",
      "validation acc :  0.780724386008\n",
      "degree=3, lambda=0.001, Training RMSE=0.787, Testing RMSE=0.807\n",
      "train acc :  0.792034958784\n",
      "validation acc :  0.780054577028\n",
      "degree=3, lambda=0.001, Training RMSE=0.789, Testing RMSE=0.806\n",
      "train acc :  0.792034958784\n",
      "validation acc :  0.780277846688\n",
      "degree=3, lambda=0.002, Training RMSE=0.791, Testing RMSE=0.807\n",
      "train acc :  0.789353461118\n",
      "validation acc :  0.779186306128\n",
      "degree=3, lambda=0.002, Training RMSE=0.796, Testing RMSE=0.809\n",
      "train acc :  0.784884298341\n",
      "validation acc :  0.776035723146\n",
      "degree=3, lambda=0.003, Training RMSE=0.804, Testing RMSE=0.815\n",
      "train acc :  0.776641175886\n",
      "validation acc :  0.770429173902\n",
      "degree=3, lambda=0.004, Training RMSE=0.816, Testing RMSE=0.824\n",
      "train acc :  0.769391200715\n",
      "validation acc :  0.761597618457\n",
      "degree=3, lambda=0.005, Training RMSE=0.831, Testing RMSE=0.837\n",
      "train acc :  0.757175489125\n",
      "validation acc :  0.752195484991\n",
      "degree=3, lambda=0.007, Training RMSE=0.848, Testing RMSE=0.854\n",
      "train acc :  0.747045386831\n",
      "validation acc :  0.744182584967\n",
      "degree=3, lambda=0.009, Training RMSE=0.868, Testing RMSE=0.872\n",
      "train acc :  0.737809117092\n",
      "validation acc :  0.735747953361\n",
      "degree=3, lambda=0.013, Training RMSE=0.888, Testing RMSE=0.892\n",
      "train acc :  0.728970106267\n",
      "validation acc :  0.729694864798\n",
      "degree=3, lambda=0.017, Training RMSE=0.908, Testing RMSE=0.911\n",
      "train acc :  0.725295461317\n",
      "validation acc :  0.724162738774\n",
      "degree=3, lambda=0.023, Training RMSE=0.927, Testing RMSE=0.930\n",
      "train acc :  0.719733836528\n",
      "validation acc :  0.720292731332\n",
      "degree=3, lambda=0.031, Training RMSE=0.944, Testing RMSE=0.947\n",
      "train acc :  0.716059191578\n",
      "validation acc :  0.715628876209\n",
      "degree=3, lambda=0.041, Training RMSE=0.959, Testing RMSE=0.961\n",
      "train acc :  0.711093455159\n",
      "validation acc :  0.712527908708\n",
      "degree=3, lambda=0.055, Training RMSE=0.970, Testing RMSE=0.972\n",
      "train acc :  0.707319495481\n",
      "validation acc :  0.709774249566\n",
      "degree=3, lambda=0.074, Training RMSE=0.978, Testing RMSE=0.980\n",
      "train acc :  0.704538683087\n",
      "validation acc :  0.707392706524\n",
      "degree=3, lambda=0.100, Training RMSE=0.984, Testing RMSE=0.986\n",
      "train acc :  0.700367464495\n",
      "validation acc :  0.703894815182\n",
      "degree=4, lambda=0.000, Training RMSE=0.776, Testing RMSE=1.153\n",
      "train acc :  0.797596583573\n",
      "validation acc :  0.724857355495\n",
      "degree=4, lambda=0.000, Training RMSE=0.776, Testing RMSE=1.105\n",
      "train acc :  0.798490416129\n",
      "validation acc :  0.746638551228\n",
      "degree=4, lambda=0.000, Training RMSE=0.776, Testing RMSE=1.072\n",
      "train acc :  0.798589730857\n",
      "validation acc :  0.763904738278\n",
      "degree=4, lambda=0.000, Training RMSE=0.776, Testing RMSE=1.054\n",
      "train acc :  0.7983911014\n",
      "validation acc :  0.776383031506\n",
      "degree=4, lambda=0.000, Training RMSE=0.776, Testing RMSE=1.045\n",
      "train acc :  0.798291786672\n",
      "validation acc :  0.78375093029\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.042\n",
      "train acc :  0.7983911014\n",
      "validation acc :  0.786132473332\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.798093157215\n",
      "validation acc :  0.787199206152\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.79779521303\n",
      "validation acc :  0.787149590672\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.797497268845\n",
      "validation acc :  0.786752666832\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.797497268845\n",
      "validation acc :  0.786653435872\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.797596583573\n",
      "validation acc :  0.786554204912\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.797695898302\n",
      "validation acc :  0.786479781692\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.797695898302\n",
      "validation acc :  0.786380550732\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.797596583573\n",
      "validation acc :  0.786330935252\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.797894527758\n",
      "validation acc :  0.786281319772\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.040\n",
      "train acc :  0.798093157215\n",
      "validation acc :  0.786231704292\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.041\n",
      "train acc :  0.798490416129\n",
      "validation acc :  0.786529397172\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.042\n",
      "train acc :  0.798986989771\n",
      "validation acc :  0.786727859092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.044\n",
      "train acc :  0.799086304499\n",
      "validation acc :  0.786827090052\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.046\n",
      "train acc :  0.799682192869\n",
      "validation acc :  0.786504589432\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.049\n",
      "train acc :  0.799682192869\n",
      "validation acc :  0.786777474572\n",
      "degree=4, lambda=0.000, Training RMSE=0.777, Testing RMSE=1.051\n",
      "train acc :  0.799384248684\n",
      "validation acc :  0.786851897792\n",
      "degree=4, lambda=0.001, Training RMSE=0.777, Testing RMSE=1.048\n",
      "train acc :  0.79779521303\n",
      "validation acc :  0.787521706773\n",
      "degree=4, lambda=0.001, Training RMSE=0.778, Testing RMSE=1.038\n",
      "train acc :  0.797695898302\n",
      "validation acc :  0.787744976433\n",
      "degree=4, lambda=0.001, Training RMSE=0.780, Testing RMSE=1.016\n",
      "train acc :  0.797000695203\n",
      "validation acc :  0.786579012652\n",
      "degree=4, lambda=0.002, Training RMSE=0.783, Testing RMSE=0.988\n",
      "train acc :  0.795610289006\n",
      "validation acc :  0.784991317291\n",
      "degree=4, lambda=0.002, Training RMSE=0.788, Testing RMSE=0.963\n",
      "train acc :  0.790346608402\n",
      "validation acc :  0.781369387249\n",
      "degree=4, lambda=0.003, Training RMSE=0.796, Testing RMSE=0.948\n",
      "train acc :  0.787267851822\n",
      "validation acc :  0.775415529645\n",
      "degree=4, lambda=0.004, Training RMSE=0.807, Testing RMSE=0.945\n",
      "train acc :  0.777237064257\n",
      "validation acc :  0.769188786902\n",
      "degree=4, lambda=0.005, Training RMSE=0.821, Testing RMSE=0.950\n",
      "train acc :  0.766213129407\n",
      "validation acc :  0.760431654676\n",
      "degree=4, lambda=0.007, Training RMSE=0.838, Testing RMSE=0.964\n",
      "train acc :  0.753302214718\n",
      "validation acc :  0.749044902009\n",
      "degree=4, lambda=0.009, Training RMSE=0.856, Testing RMSE=0.980\n",
      "train acc :  0.742774853511\n",
      "validation acc :  0.739791614984\n",
      "degree=4, lambda=0.013, Training RMSE=0.874, Testing RMSE=0.987\n",
      "train acc :  0.734333101599\n",
      "validation acc :  0.731257752419\n",
      "degree=4, lambda=0.017, Training RMSE=0.891, Testing RMSE=0.985\n",
      "train acc :  0.728970106267\n",
      "validation acc :  0.725576779955\n",
      "degree=4, lambda=0.023, Training RMSE=0.908, Testing RMSE=0.978\n",
      "train acc :  0.726089979144\n",
      "validation acc :  0.722029273133\n",
      "degree=4, lambda=0.031, Training RMSE=0.923, Testing RMSE=0.971\n",
      "train acc :  0.724004369848\n",
      "validation acc :  0.719746961052\n",
      "degree=4, lambda=0.041, Training RMSE=0.937, Testing RMSE=0.967\n",
      "train acc :  0.724103684576\n",
      "validation acc :  0.719201190772\n",
      "degree=4, lambda=0.055, Training RMSE=0.950, Testing RMSE=0.968\n",
      "train acc :  0.721620816367\n",
      "validation acc :  0.717935996031\n",
      "degree=4, lambda=0.074, Training RMSE=0.962, Testing RMSE=0.972\n",
      "train acc :  0.721521501639\n",
      "validation acc :  0.715355991069\n",
      "degree=4, lambda=0.100, Training RMSE=0.972, Testing RMSE=0.979\n",
      "train acc :  0.71774754196\n",
      "validation acc :  0.712056561647\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.630\n",
      "train acc :  0.800476710696\n",
      "validation acc :  0.704043661622\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.596\n",
      "train acc :  0.800774654881\n",
      "validation acc :  0.732200446539\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.579\n",
      "train acc :  0.801767802165\n",
      "validation acc :  0.75045894319\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.571\n",
      "train acc :  0.801867116893\n",
      "validation acc :  0.763557429918\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.567\n",
      "train acc :  0.80087396961\n",
      "validation acc :  0.772339369883\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.566\n",
      "train acc :  0.800377395968\n",
      "validation acc :  0.779310344828\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.565\n",
      "train acc :  0.799880822326\n",
      "validation acc :  0.78404862317\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.565\n",
      "train acc :  0.799980137054\n",
      "validation acc :  0.787670553213\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.565\n",
      "train acc :  0.799980137054\n",
      "validation acc :  0.789382287274\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.565\n",
      "train acc :  0.800079451783\n",
      "validation acc :  0.790002480774\n",
      "degree=5, lambda=0.000, Training RMSE=0.768, Testing RMSE=2.565\n",
      "train acc :  0.799781507598\n",
      "validation acc :  0.790324981394\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.565\n",
      "train acc :  0.799682192869\n",
      "validation acc :  0.790399404614\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.565\n",
      "train acc :  0.799781507598\n",
      "validation acc :  0.790746712974\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.563\n",
      "train acc :  0.799781507598\n",
      "validation acc :  0.790895559415\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.561\n",
      "train acc :  0.799880822326\n",
      "validation acc :  0.790945174895\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.558\n",
      "train acc :  0.800576025425\n",
      "validation acc :  0.791069213595\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.551\n",
      "train acc :  0.800675340153\n",
      "validation acc :  0.790721905234\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.540\n",
      "train acc :  0.801370543252\n",
      "validation acc :  0.790622674274\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.521\n",
      "train acc :  0.801867116893\n",
      "validation acc :  0.790647482014\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.492\n",
      "train acc :  0.802860264177\n",
      "validation acc :  0.790349789134\n",
      "degree=5, lambda=0.000, Training RMSE=0.769, Testing RMSE=2.450\n",
      "train acc :  0.803654782004\n",
      "validation acc :  0.790672289754\n",
      "degree=5, lambda=0.000, Training RMSE=0.770, Testing RMSE=2.400\n",
      "train acc :  0.804349985103\n",
      "validation acc :  0.791118829075\n",
      "degree=5, lambda=0.001, Training RMSE=0.771, Testing RMSE=2.380\n",
      "train acc :  0.803257523091\n",
      "validation acc :  0.790969982635\n",
      "degree=5, lambda=0.001, Training RMSE=0.772, Testing RMSE=2.498\n",
      "train acc :  0.801370543252\n",
      "validation acc :  0.790324981394\n",
      "degree=5, lambda=0.001, Training RMSE=0.774, Testing RMSE=2.890\n",
      "train acc :  0.798689045585\n",
      "validation acc :  0.789828826594\n",
      "degree=5, lambda=0.002, Training RMSE=0.778, Testing RMSE=3.538\n",
      "train acc :  0.794815771179\n",
      "validation acc :  0.787000744232\n",
      "degree=5, lambda=0.002, Training RMSE=0.784, Testing RMSE=4.233\n",
      "train acc :  0.790644552587\n",
      "validation acc :  0.78325477549\n",
      "degree=5, lambda=0.003, Training RMSE=0.793, Testing RMSE=4.774\n",
      "train acc :  0.786771278181\n",
      "validation acc :  0.777400148846\n",
      "degree=5, lambda=0.004, Training RMSE=0.804, Testing RMSE=5.061\n",
      "train acc :  0.781209653392\n",
      "validation acc :  0.770478789382\n",
      "degree=5, lambda=0.005, Training RMSE=0.816, Testing RMSE=5.028\n",
      "train acc :  0.771476810011\n",
      "validation acc :  0.761374348797\n",
      "degree=5, lambda=0.007, Training RMSE=0.830, Testing RMSE=4.637\n",
      "train acc :  0.762339855\n",
      "validation acc :  0.752592408832\n",
      "degree=5, lambda=0.009, Training RMSE=0.843, Testing RMSE=3.978\n",
      "train acc :  0.75260701162\n",
      "validation acc :  0.742197965765\n",
      "degree=5, lambda=0.013, Training RMSE=0.858, Testing RMSE=3.271\n",
      "train acc :  0.743072797696\n",
      "validation acc :  0.73408583478\n",
      "degree=5, lambda=0.017, Training RMSE=0.873, Testing RMSE=2.733\n",
      "train acc :  0.736617340352\n",
      "validation acc :  0.727263706276\n",
      "degree=5, lambda=0.023, Training RMSE=0.890, Testing RMSE=2.450\n",
      "train acc :  0.72936736518\n",
      "validation acc :  0.722054080873\n",
      "degree=5, lambda=0.031, Training RMSE=0.908, Testing RMSE=2.357\n",
      "train acc :  0.724302314033\n",
      "validation acc :  0.71681964773\n",
      "degree=5, lambda=0.041, Training RMSE=0.925, Testing RMSE=2.327\n",
      "train acc :  0.720329724898\n",
      "validation acc :  0.712701562888\n",
      "degree=5, lambda=0.055, Training RMSE=0.940, Testing RMSE=2.259\n",
      "train acc :  0.713774952826\n",
      "validation acc :  0.709625403126\n",
      "degree=5, lambda=0.074, Training RMSE=0.952, Testing RMSE=2.113\n",
      "train acc :  0.710000993147\n",
      "validation acc :  0.706648474324\n",
      "degree=5, lambda=0.100, Training RMSE=0.961, Testing RMSE=1.904\n",
      "train acc :  0.701658555964\n",
      "validation acc :  0.69945422972\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.603\n",
      "train acc :  0.801767802165\n",
      "validation acc :  0.724361200695\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.580\n",
      "train acc :  0.801767802165\n",
      "validation acc :  0.736641032002\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.560\n",
      "train acc :  0.80206574635\n",
      "validation acc :  0.744430662367\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.542\n",
      "train acc :  0.802165061079\n",
      "validation acc :  0.751426445051\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.526\n",
      "train acc :  0.802264375807\n",
      "validation acc :  0.759067228975\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.511\n",
      "train acc :  0.801569172708\n",
      "validation acc :  0.768767055321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.497\n",
      "train acc :  0.801767802165\n",
      "validation acc :  0.778690151327\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.485\n",
      "train acc :  0.80206574635\n",
      "validation acc :  0.785164971471\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.476\n",
      "train acc :  0.801072599066\n",
      "validation acc :  0.789084594393\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.469\n",
      "train acc :  0.80087396961\n",
      "validation acc :  0.789804018854\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.463\n",
      "train acc :  0.800774654881\n",
      "validation acc :  0.790424212354\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.456\n",
      "train acc :  0.800774654881\n",
      "validation acc :  0.790052096254\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.445\n",
      "train acc :  0.800973284338\n",
      "validation acc :  0.789630364674\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.428\n",
      "train acc :  0.800774654881\n",
      "validation acc :  0.789729595634\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.400\n",
      "train acc :  0.800576025425\n",
      "validation acc :  0.789630364674\n",
      "degree=6, lambda=0.000, Training RMSE=0.766, Testing RMSE=10.357\n",
      "train acc :  0.801370543252\n",
      "validation acc :  0.789903249814\n",
      "degree=6, lambda=0.000, Training RMSE=0.767, Testing RMSE=10.297\n",
      "train acc :  0.801966431622\n",
      "validation acc :  0.790126519474\n",
      "degree=6, lambda=0.000, Training RMSE=0.767, Testing RMSE=10.215\n",
      "train acc :  0.801966431622\n",
      "validation acc :  0.790002480774\n",
      "degree=6, lambda=0.000, Training RMSE=0.767, Testing RMSE=10.102\n",
      "train acc :  0.802363690535\n",
      "validation acc :  0.790151327214\n",
      "degree=6, lambda=0.000, Training RMSE=0.767, Testing RMSE=9.931\n",
      "train acc :  0.803058893634\n",
      "validation acc :  0.789853634334\n",
      "degree=6, lambda=0.000, Training RMSE=0.768, Testing RMSE=9.674\n",
      "train acc :  0.803456152547\n",
      "validation acc :  0.790300173654\n",
      "degree=6, lambda=0.000, Training RMSE=0.768, Testing RMSE=9.430\n",
      "train acc :  0.803952726189\n",
      "validation acc :  0.790721905234\n",
      "degree=6, lambda=0.001, Training RMSE=0.769, Testing RMSE=9.714\n",
      "train acc :  0.804647929288\n",
      "validation acc :  0.790697097494\n",
      "degree=6, lambda=0.001, Training RMSE=0.771, Testing RMSE=11.367\n",
      "train acc :  0.803853411461\n",
      "validation acc :  0.789952865294\n",
      "degree=6, lambda=0.001, Training RMSE=0.773, Testing RMSE=14.272\n",
      "train acc :  0.799980137054\n",
      "validation acc :  0.789258248574\n",
      "degree=6, lambda=0.002, Training RMSE=0.777, Testing RMSE=16.811\n",
      "train acc :  0.797397954117\n",
      "validation acc :  0.786951128752\n",
      "degree=6, lambda=0.002, Training RMSE=0.783, Testing RMSE=17.221\n",
      "train acc :  0.793921938623\n",
      "validation acc :  0.78355246837\n",
      "degree=6, lambda=0.003, Training RMSE=0.791, Testing RMSE=14.958\n",
      "train acc :  0.79044592313\n",
      "validation acc :  0.778714959067\n",
      "degree=6, lambda=0.004, Training RMSE=0.801, Testing RMSE=10.905\n",
      "train acc :  0.785480186712\n",
      "validation acc :  0.771644753163\n",
      "degree=6, lambda=0.005, Training RMSE=0.813, Testing RMSE=7.308\n",
      "train acc :  0.776045287516\n",
      "validation acc :  0.763607045398\n",
      "degree=6, lambda=0.007, Training RMSE=0.826, Testing RMSE=8.367\n",
      "train acc :  0.763928890654\n",
      "validation acc :  0.754080873232\n",
      "degree=6, lambda=0.009, Training RMSE=0.840, Testing RMSE=12.985\n",
      "train acc :  0.752904955805\n",
      "validation acc :  0.743463160506\n",
      "degree=6, lambda=0.013, Training RMSE=0.855, Testing RMSE=17.091\n",
      "train acc :  0.744363889165\n",
      "validation acc :  0.73383775738\n",
      "degree=6, lambda=0.017, Training RMSE=0.871, Testing RMSE=18.818\n",
      "train acc :  0.731651603933\n",
      "validation acc :  0.724931778715\n",
      "degree=6, lambda=0.023, Training RMSE=0.887, Testing RMSE=17.828\n",
      "train acc :  0.723209852021\n",
      "validation acc :  0.71686926321\n",
      "degree=6, lambda=0.031, Training RMSE=0.901, Testing RMSE=14.966\n",
      "train acc :  0.718840003973\n",
      "validation acc :  0.710220788886\n",
      "degree=6, lambda=0.041, Training RMSE=0.914, Testing RMSE=11.497\n",
      "train acc :  0.710199622604\n",
      "validation acc :  0.703770776482\n",
      "degree=6, lambda=0.055, Training RMSE=0.925, Testing RMSE=8.378\n",
      "train acc :  0.704439368358\n",
      "validation acc :  0.69955346068\n",
      "degree=6, lambda=0.074, Training RMSE=0.935, Testing RMSE=6.103\n",
      "train acc :  0.697586652101\n",
      "validation acc :  0.695112875217\n",
      "degree=6, lambda=0.100, Training RMSE=0.945, Testing RMSE=4.939\n",
      "train acc :  0.694607210249\n",
      "validation acc :  0.692780947656\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=169.821\n",
      "train acc :  0.809613665707\n",
      "validation acc :  0.717588687671\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.051\n",
      "train acc :  0.809613665707\n",
      "validation acc :  0.726469858596\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.267\n",
      "train acc :  0.809315721522\n",
      "validation acc :  0.735871992062\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.464\n",
      "train acc :  0.809017777336\n",
      "validation acc :  0.746936244108\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.644\n",
      "train acc :  0.80881914788\n",
      "validation acc :  0.761151079137\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.800\n",
      "train acc :  0.808918462608\n",
      "validation acc :  0.771545522203\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.917\n",
      "train acc :  0.808918462608\n",
      "validation acc :  0.780699578268\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.981\n",
      "train acc :  0.808719833151\n",
      "validation acc :  0.787596129993\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.976\n",
      "train acc :  0.809017777336\n",
      "validation acc :  0.790721905234\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.887\n",
      "train acc :  0.808918462608\n",
      "validation acc :  0.791714214835\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.681\n",
      "train acc :  0.808719833151\n",
      "validation acc :  0.792061523195\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=170.303\n",
      "train acc :  0.809017777336\n",
      "validation acc :  0.792954601836\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=169.676\n",
      "train acc :  0.808521203695\n",
      "validation acc :  0.792954601836\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=168.728\n",
      "train acc :  0.80881914788\n",
      "validation acc :  0.792681716696\n",
      "degree=7, lambda=0.000, Training RMSE=0.760, Testing RMSE=167.454\n",
      "train acc :  0.80941503625\n",
      "validation acc :  0.792557677996\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=165.990\n",
      "train acc :  0.809712980435\n",
      "validation acc :  0.793078640536\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=164.599\n",
      "train acc :  0.809613665707\n",
      "validation acc :  0.793351525676\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=163.458\n",
      "train acc :  0.809514350978\n",
      "validation acc :  0.793698834036\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=162.301\n",
      "train acc :  0.80881914788\n",
      "validation acc :  0.793425948896\n",
      "degree=7, lambda=0.000, Training RMSE=0.762, Testing RMSE=160.122\n",
      "train acc :  0.809117092065\n",
      "validation acc :  0.793723641776\n",
      "degree=7, lambda=0.000, Training RMSE=0.762, Testing RMSE=155.158\n",
      "train acc :  0.809315721522\n",
      "validation acc :  0.793425948896\n",
      "degree=7, lambda=0.000, Training RMSE=0.763, Testing RMSE=145.485\n",
      "train acc :  0.80881914788\n",
      "validation acc :  0.793897295956\n",
      "degree=7, lambda=0.001, Training RMSE=0.765, Testing RMSE=130.614\n",
      "train acc :  0.808421888966\n",
      "validation acc :  0.793723641776\n",
      "degree=7, lambda=0.001, Training RMSE=0.766, Testing RMSE=113.752\n",
      "train acc :  0.807726685868\n",
      "validation acc :  0.793450756636\n",
      "degree=7, lambda=0.001, Training RMSE=0.769, Testing RMSE=102.145\n",
      "train acc :  0.806435594399\n",
      "validation acc :  0.792309600595\n",
      "degree=7, lambda=0.002, Training RMSE=0.772, Testing RMSE=102.240\n",
      "train acc :  0.801767802165\n",
      "validation acc :  0.790076903994\n",
      "degree=7, lambda=0.002, Training RMSE=0.778, Testing RMSE=114.465\n",
      "train acc :  0.799682192869\n",
      "validation acc :  0.787620937733\n",
      "degree=7, lambda=0.003, Training RMSE=0.786, Testing RMSE=134.266\n",
      "train acc :  0.79352467971\n",
      "validation acc :  0.780972463409\n",
      "degree=7, lambda=0.004, Training RMSE=0.797, Testing RMSE=154.026\n",
      "train acc :  0.785877445625\n",
      "validation acc :  0.773381294964\n",
      "degree=7, lambda=0.005, Training RMSE=0.810, Testing RMSE=165.910\n",
      "train acc :  0.774257622405\n",
      "validation acc :  0.763483006698\n",
      "degree=7, lambda=0.007, Training RMSE=0.824, Testing RMSE=164.814\n",
      "train acc :  0.764127520111\n",
      "validation acc :  0.754452989333\n",
      "degree=7, lambda=0.009, Training RMSE=0.838, Testing RMSE=148.708\n",
      "train acc :  0.754096732545\n",
      "validation acc :  0.744703547507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=7, lambda=0.013, Training RMSE=0.851, Testing RMSE=118.591\n",
      "train acc :  0.746350183732\n",
      "validation acc :  0.735003721161\n",
      "degree=7, lambda=0.017, Training RMSE=0.863, Testing RMSE=78.926\n",
      "train acc :  0.737213228722\n",
      "validation acc :  0.727139667576\n",
      "degree=7, lambda=0.023, Training RMSE=0.876, Testing RMSE=37.611\n",
      "train acc :  0.727182441156\n",
      "validation acc :  0.720590424212\n",
      "degree=7, lambda=0.031, Training RMSE=0.889, Testing RMSE=16.027\n",
      "train acc :  0.720329724898\n",
      "validation acc :  0.715777722649\n",
      "degree=7, lambda=0.041, Training RMSE=0.902, Testing RMSE=36.339\n",
      "train acc :  0.719932465985\n",
      "validation acc :  0.710716943686\n",
      "degree=7, lambda=0.055, Training RMSE=0.914, Testing RMSE=52.325\n",
      "train acc :  0.71595987685\n",
      "validation acc :  0.706524435624\n",
      "degree=7, lambda=0.074, Training RMSE=0.924, Testing RMSE=58.444\n",
      "train acc :  0.714867414838\n",
      "validation acc :  0.703572314562\n",
      "degree=7, lambda=0.100, Training RMSE=0.933, Testing RMSE=56.089\n",
      "train acc :  0.713079749727\n",
      "validation acc :  0.704217315803\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=825.353\n",
      "train acc :  0.81736021452\n",
      "validation acc :  0.74924336393\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=825.317\n",
      "train acc :  0.817459529248\n",
      "validation acc :  0.758025303895\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=825.306\n",
      "train acc :  0.817558843977\n",
      "validation acc :  0.763731084098\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=825.344\n",
      "train acc :  0.817658158705\n",
      "validation acc :  0.76631108906\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=825.457\n",
      "train acc :  0.817856788162\n",
      "validation acc :  0.769039940461\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=825.678\n",
      "train acc :  0.817757473433\n",
      "validation acc :  0.770255519722\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=826.054\n",
      "train acc :  0.81736021452\n",
      "validation acc :  0.773554949144\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=826.631\n",
      "train acc :  0.81736021452\n",
      "validation acc :  0.777871495907\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=827.413\n",
      "train acc :  0.817161585063\n",
      "validation acc :  0.78399900769\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=828.312\n",
      "train acc :  0.817062270335\n",
      "validation acc :  0.789828826594\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=829.132\n",
      "train acc :  0.817260899791\n",
      "validation acc :  0.795112875217\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=829.624\n",
      "train acc :  0.817558843977\n",
      "validation acc :  0.798461920119\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=829.519\n",
      "train acc :  0.817658158705\n",
      "validation acc :  0.79957826842\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=828.412\n",
      "train acc :  0.817856788162\n",
      "validation acc :  0.799057305879\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=825.494\n",
      "train acc :  0.818055417618\n",
      "validation acc :  0.7993798065\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=819.195\n",
      "train acc :  0.817658158705\n",
      "validation acc :  0.79942942198\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=806.911\n",
      "train acc :  0.817658158705\n",
      "validation acc :  0.79928057554\n",
      "degree=8, lambda=0.000, Training RMSE=0.749, Testing RMSE=784.901\n",
      "train acc :  0.81676432615\n",
      "validation acc :  0.7993798065\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=748.544\n",
      "train acc :  0.816665011421\n",
      "validation acc :  0.79918134458\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=693.337\n",
      "train acc :  0.817062270335\n",
      "validation acc :  0.79990076904\n",
      "degree=8, lambda=0.000, Training RMSE=0.751, Testing RMSE=617.011\n",
      "train acc :  0.816565696693\n",
      "validation acc :  0.7997519226\n",
      "degree=8, lambda=0.000, Training RMSE=0.752, Testing RMSE=522.430\n",
      "train acc :  0.816466381964\n",
      "validation acc :  0.79965269164\n",
      "degree=8, lambda=0.001, Training RMSE=0.754, Testing RMSE=419.046\n",
      "train acc :  0.815175290496\n",
      "validation acc :  0.799156536839\n",
      "degree=8, lambda=0.001, Training RMSE=0.756, Testing RMSE=320.505\n",
      "train acc :  0.81368556957\n",
      "validation acc :  0.798635574299\n",
      "degree=8, lambda=0.001, Training RMSE=0.760, Testing RMSE=249.110\n",
      "train acc :  0.812295163373\n",
      "validation acc :  0.797122302158\n",
      "degree=8, lambda=0.002, Training RMSE=0.765, Testing RMSE=241.141\n",
      "train acc :  0.808223259509\n",
      "validation acc :  0.793847680476\n",
      "degree=8, lambda=0.002, Training RMSE=0.773, Testing RMSE=278.873\n",
      "train acc :  0.802860264177\n",
      "validation acc :  0.789159017613\n",
      "degree=8, lambda=0.003, Training RMSE=0.783, Testing RMSE=296.705\n",
      "train acc :  0.794021253352\n",
      "validation acc :  0.781493425949\n",
      "degree=8, lambda=0.004, Training RMSE=0.795, Testing RMSE=269.820\n",
      "train acc :  0.78617538981\n",
      "validation acc :  0.772835524684\n",
      "degree=8, lambda=0.005, Training RMSE=0.808, Testing RMSE=242.988\n",
      "train acc :  0.776243916973\n",
      "validation acc :  0.763011659638\n",
      "degree=8, lambda=0.007, Training RMSE=0.822, Testing RMSE=319.868\n",
      "train acc :  0.766908332506\n",
      "validation acc :  0.754006450012\n",
      "degree=8, lambda=0.009, Training RMSE=0.835, Testing RMSE=487.279\n",
      "train acc :  0.757870692224\n",
      "validation acc :  0.745596626147\n",
      "degree=8, lambda=0.013, Training RMSE=0.848, Testing RMSE=657.121\n",
      "train acc :  0.747641275201\n",
      "validation acc :  0.736219300422\n",
      "degree=8, lambda=0.017, Training RMSE=0.860, Testing RMSE=771.297\n",
      "train acc :  0.738603634919\n",
      "validation acc :  0.727263706276\n",
      "degree=8, lambda=0.023, Training RMSE=0.872, Testing RMSE=800.507\n",
      "train acc :  0.729069420995\n",
      "validation acc :  0.720739270652\n",
      "degree=8, lambda=0.031, Training RMSE=0.884, Testing RMSE=740.330\n",
      "train acc :  0.722316019466\n",
      "validation acc :  0.713817911188\n",
      "degree=8, lambda=0.041, Training RMSE=0.894, Testing RMSE=608.780\n",
      "train acc :  0.714370841196\n",
      "validation acc :  0.706499627884\n",
      "degree=8, lambda=0.055, Training RMSE=0.904, Testing RMSE=438.842\n",
      "train acc :  0.707716754395\n",
      "validation acc :  0.70064500124\n",
      "degree=8, lambda=0.074, Training RMSE=0.914, Testing RMSE=266.200\n",
      "train acc :  0.700864038137\n",
      "validation acc :  0.696005953858\n",
      "degree=8, lambda=0.100, Training RMSE=0.923, Testing RMSE=119.691\n",
      "train acc :  0.694309266064\n",
      "validation acc :  0.690573058794\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5762.779\n",
      "train acc :  0.823219783494\n",
      "validation acc :  0.515529645249\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5766.408\n",
      "train acc :  0.824014301321\n",
      "validation acc :  0.531059290499\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5769.431\n",
      "train acc :  0.824014301321\n",
      "validation acc :  0.594070950136\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5771.632\n",
      "train acc :  0.823914986593\n",
      "validation acc :  0.590449020094\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5773.093\n",
      "train acc :  0.823914986593\n",
      "validation acc :  0.660406846936\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5773.442\n",
      "train acc :  0.822921839309\n",
      "validation acc :  0.724162738774\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5771.568\n",
      "train acc :  0.823219783494\n",
      "validation acc :  0.670726866782\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5764.715\n",
      "train acc :  0.823716357136\n",
      "validation acc :  0.71686926321\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5748.223\n",
      "train acc :  0.823716357136\n",
      "validation acc :  0.779211113868\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5717.663\n",
      "train acc :  0.823418412951\n",
      "validation acc :  0.769139171421\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5670.876\n",
      "train acc :  0.823418412951\n",
      "validation acc :  0.78260977425\n",
      "degree=9, lambda=0.000, Training RMSE=0.740, Testing RMSE=5610.376\n",
      "train acc :  0.823219783494\n",
      "validation acc :  0.786355742992\n",
      "degree=9, lambda=0.000, Training RMSE=0.741, Testing RMSE=5542.152\n",
      "train acc :  0.82282252458\n",
      "validation acc :  0.780178615728\n",
      "degree=9, lambda=0.000, Training RMSE=0.741, Testing RMSE=5471.828\n",
      "train acc :  0.82222663621\n",
      "validation acc :  0.793798064996\n",
      "degree=9, lambda=0.000, Training RMSE=0.741, Testing RMSE=5400.320\n",
      "train acc :  0.82282252458\n",
      "validation acc :  0.772016869263\n",
      "degree=9, lambda=0.000, Training RMSE=0.741, Testing RMSE=5322.027\n",
      "train acc :  0.82222663621\n",
      "validation acc :  0.768320516001\n",
      "degree=9, lambda=0.000, Training RMSE=0.741, Testing RMSE=5229.097\n",
      "train acc :  0.82222663621\n",
      "validation acc :  0.785289010171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=9, lambda=0.000, Training RMSE=0.741, Testing RMSE=5117.909\n",
      "train acc :  0.822524580395\n",
      "validation acc :  0.788588439593\n",
      "degree=9, lambda=0.000, Training RMSE=0.742, Testing RMSE=4998.335\n",
      "train acc :  0.82222663621\n",
      "validation acc :  0.802331927561\n",
      "degree=9, lambda=0.000, Training RMSE=0.742, Testing RMSE=4901.929\n",
      "train acc :  0.822127321482\n",
      "validation acc :  0.802704043662\n",
      "degree=9, lambda=0.000, Training RMSE=0.743, Testing RMSE=4881.802\n",
      "train acc :  0.821829377297\n",
      "validation acc :  0.803398660382\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=4993.091\n",
      "train acc :  0.821928692025\n",
      "validation acc :  0.803572314562\n",
      "degree=9, lambda=0.001, Training RMSE=0.746, Testing RMSE=5241.416\n",
      "train acc :  0.819545138544\n",
      "validation acc :  0.802778466882\n",
      "degree=9, lambda=0.001, Training RMSE=0.749, Testing RMSE=5501.122\n",
      "train acc :  0.817757473433\n",
      "validation acc :  0.80017365418\n",
      "degree=9, lambda=0.001, Training RMSE=0.754, Testing RMSE=5460.926\n",
      "train acc :  0.8130896812\n",
      "validation acc :  0.797841726619\n",
      "degree=9, lambda=0.002, Training RMSE=0.761, Testing RMSE=4763.577\n",
      "train acc :  0.809712980435\n",
      "validation acc :  0.794343835277\n",
      "degree=9, lambda=0.002, Training RMSE=0.771, Testing RMSE=3358.434\n",
      "train acc :  0.80266163472\n",
      "validation acc :  0.789729595634\n",
      "degree=9, lambda=0.003, Training RMSE=0.782, Testing RMSE=1808.413\n",
      "train acc :  0.795213030092\n",
      "validation acc :  0.780997271149\n",
      "degree=9, lambda=0.004, Training RMSE=0.795, Testing RMSE=1680.967\n",
      "train acc :  0.787367166551\n",
      "validation acc :  0.772438600843\n",
      "degree=9, lambda=0.005, Training RMSE=0.807, Testing RMSE=2667.704\n",
      "train acc :  0.778130896812\n",
      "validation acc :  0.764177623419\n",
      "degree=9, lambda=0.007, Training RMSE=0.820, Testing RMSE=3294.495\n",
      "train acc :  0.768100109246\n",
      "validation acc :  0.754527412553\n",
      "degree=9, lambda=0.009, Training RMSE=0.832, Testing RMSE=3282.075\n",
      "train acc :  0.757771377495\n",
      "validation acc :  0.745075663607\n",
      "degree=9, lambda=0.013, Training RMSE=0.844, Testing RMSE=2685.831\n",
      "train acc :  0.750024828682\n",
      "validation acc :  0.735549491441\n",
      "degree=9, lambda=0.017, Training RMSE=0.855, Testing RMSE=1800.135\n",
      "train acc :  0.740589929487\n",
      "validation acc :  0.727710245597\n",
      "degree=9, lambda=0.023, Training RMSE=0.865, Testing RMSE=1487.655\n",
      "train acc :  0.732942695402\n",
      "validation acc :  0.719598114612\n",
      "degree=9, lambda=0.031, Training RMSE=0.876, Testing RMSE=2303.976\n",
      "train acc :  0.725792034959\n",
      "validation acc :  0.714611758869\n",
      "degree=9, lambda=0.041, Training RMSE=0.887, Testing RMSE=3253.807\n",
      "train acc :  0.720131095441\n",
      "validation acc :  0.710245596626\n",
      "degree=9, lambda=0.055, Training RMSE=0.898, Testing RMSE=3857.332\n",
      "train acc :  0.715165359023\n",
      "validation acc :  0.704291739023\n",
      "degree=9, lambda=0.074, Training RMSE=0.908, Testing RMSE=4035.161\n",
      "train acc :  0.709802363691\n",
      "validation acc :  0.69990076904\n",
      "degree=9, lambda=0.100, Training RMSE=0.917, Testing RMSE=3834.380\n",
      "train acc :  0.706028404012\n",
      "validation acc :  0.694170181097\n",
      "degree=10, lambda=0.000, Training RMSE=0.741, Testing RMSE=50804.290\n",
      "train acc :  0.820736915285\n",
      "validation acc :  0.49920615232\n",
      "degree=10, lambda=0.000, Training RMSE=0.740, Testing RMSE=50847.948\n",
      "train acc :  0.819048564902\n",
      "validation acc :  0.50032250062\n",
      "degree=10, lambda=0.000, Training RMSE=0.766, Testing RMSE=50902.830\n",
      "train acc :  0.801867116893\n",
      "validation acc :  0.497246340858\n",
      "degree=10, lambda=0.000, Training RMSE=0.751, Testing RMSE=50942.736\n",
      "train acc :  0.812791737015\n",
      "validation acc :  0.501240387001\n",
      "degree=10, lambda=0.000, Training RMSE=0.755, Testing RMSE=50970.318\n",
      "train acc :  0.811699275002\n",
      "validation acc :  0.495708260977\n",
      "degree=10, lambda=0.000, Training RMSE=0.740, Testing RMSE=50981.037\n",
      "train acc :  0.820339656371\n",
      "validation acc :  0.503448275862\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=50977.216\n",
      "train acc :  0.819247194359\n",
      "validation acc :  0.485487472091\n",
      "degree=10, lambda=0.000, Training RMSE=0.738, Testing RMSE=50950.650\n",
      "train acc :  0.820836230013\n",
      "validation acc :  0.492954601836\n",
      "degree=10, lambda=0.000, Training RMSE=0.749, Testing RMSE=50896.327\n",
      "train acc :  0.823021154037\n",
      "validation acc :  0.55031009675\n",
      "degree=10, lambda=0.000, Training RMSE=0.744, Testing RMSE=50816.129\n",
      "train acc :  0.818254047075\n",
      "validation acc :  0.461721657157\n",
      "degree=10, lambda=0.000, Training RMSE=0.765, Testing RMSE=50724.964\n",
      "train acc :  0.803058893634\n",
      "validation acc :  0.543785661126\n",
      "degree=10, lambda=0.000, Training RMSE=0.746, Testing RMSE=50673.041\n",
      "train acc :  0.815671864137\n",
      "validation acc :  0.43296948648\n",
      "degree=10, lambda=0.000, Training RMSE=0.742, Testing RMSE=50697.217\n",
      "train acc :  0.81855199126\n",
      "validation acc :  0.577995534607\n",
      "degree=10, lambda=0.000, Training RMSE=0.738, Testing RMSE=50811.111\n",
      "train acc :  0.823021154037\n",
      "validation acc :  0.594964028777\n",
      "degree=10, lambda=0.000, Training RMSE=0.740, Testing RMSE=50981.786\n",
      "train acc :  0.822921839309\n",
      "validation acc :  0.521061771273\n",
      "degree=10, lambda=0.000, Training RMSE=0.740, Testing RMSE=51151.903\n",
      "train acc :  0.826397854802\n",
      "validation acc :  0.452914909452\n",
      "degree=10, lambda=0.000, Training RMSE=0.739, Testing RMSE=51304.957\n",
      "train acc :  0.821332803655\n",
      "validation acc :  0.788538824113\n",
      "degree=10, lambda=0.000, Training RMSE=0.747, Testing RMSE=51527.171\n",
      "train acc :  0.815870493594\n",
      "validation acc :  0.491739022575\n",
      "degree=10, lambda=0.000, Training RMSE=0.739, Testing RMSE=51971.048\n",
      "train acc :  0.820736915285\n",
      "validation acc :  0.669387248822\n",
      "degree=10, lambda=0.000, Training RMSE=0.740, Testing RMSE=52695.353\n",
      "train acc :  0.818949250174\n",
      "validation acc :  0.524113123294\n",
      "degree=10, lambda=0.000, Training RMSE=0.742, Testing RMSE=53496.076\n",
      "train acc :  0.822723209852\n",
      "validation acc :  0.538972959563\n",
      "degree=10, lambda=0.000, Training RMSE=0.746, Testing RMSE=53785.926\n",
      "train acc :  0.81676432615\n",
      "validation acc :  0.618060034731\n",
      "degree=10, lambda=0.001, Training RMSE=0.744, Testing RMSE=52399.263\n",
      "train acc :  0.82103485947\n",
      "validation acc :  0.601488464401\n",
      "degree=10, lambda=0.001, Training RMSE=0.748, Testing RMSE=47502.505\n",
      "train acc :  0.815771178866\n",
      "validation acc :  0.79980153808\n",
      "degree=10, lambda=0.001, Training RMSE=0.753, Testing RMSE=37473.818\n",
      "train acc :  0.81428145794\n",
      "validation acc :  0.796675762838\n",
      "degree=10, lambda=0.002, Training RMSE=0.761, Testing RMSE=23154.912\n",
      "train acc :  0.810904757175\n",
      "validation acc :  0.793946911436\n",
      "degree=10, lambda=0.002, Training RMSE=0.771, Testing RMSE=11126.014\n",
      "train acc :  0.803754096733\n",
      "validation acc :  0.788985363433\n",
      "degree=10, lambda=0.003, Training RMSE=0.782, Testing RMSE=13430.378\n",
      "train acc :  0.795014400636\n",
      "validation acc :  0.781220540809\n",
      "degree=10, lambda=0.004, Training RMSE=0.794, Testing RMSE=17317.754\n",
      "train acc :  0.788161684378\n",
      "validation acc :  0.773058794344\n",
      "degree=10, lambda=0.005, Training RMSE=0.806, Testing RMSE=15061.230\n",
      "train acc :  0.776740490615\n",
      "validation acc :  0.763483006698\n",
      "degree=10, lambda=0.007, Training RMSE=0.818, Testing RMSE=7490.545\n",
      "train acc :  0.766610388321\n",
      "validation acc :  0.754701066733\n",
      "degree=10, lambda=0.009, Training RMSE=0.829, Testing RMSE=3406.474\n",
      "train acc :  0.757672062767\n",
      "validation acc :  0.745497395187\n",
      "degree=10, lambda=0.013, Training RMSE=0.840, Testing RMSE=13058.772\n",
      "train acc :  0.750720031781\n",
      "validation acc :  0.736194492682\n",
      "degree=10, lambda=0.017, Training RMSE=0.851, Testing RMSE=19896.050\n",
      "train acc :  0.7409871884\n",
      "validation acc :  0.727065244356\n",
      "degree=10, lambda=0.023, Training RMSE=0.862, Testing RMSE=22823.525\n",
      "train acc :  0.734829675241\n",
      "validation acc :  0.720367154552\n",
      "degree=10, lambda=0.031, Training RMSE=0.873, Testing RMSE=22168.515\n",
      "train acc :  0.727679014798\n",
      "validation acc :  0.714016373108\n",
      "degree=10, lambda=0.041, Training RMSE=0.883, Testing RMSE=18584.199\n",
      "train acc :  0.719833151256\n",
      "validation acc :  0.708012900025\n",
      "degree=10, lambda=0.055, Training RMSE=0.894, Testing RMSE=12828.587\n",
      "train acc :  0.714569470652\n",
      "validation acc :  0.701240387001\n",
      "degree=10, lambda=0.074, Training RMSE=0.903, Testing RMSE=6319.768\n",
      "train acc :  0.704637997815\n",
      "validation acc :  0.695484991317\n",
      "degree=10, lambda=0.100, Training RMSE=0.911, Testing RMSE=4746.008\n",
      "train acc :  0.697983911014\n",
      "validation acc :  0.689283056314\n",
      "degree=11, lambda=0.000, Training RMSE=1.756, Testing RMSE=710868.015\n",
      "train acc :  0.584169232297\n",
      "validation acc :  0.508459439345\n",
      "degree=11, lambda=0.000, Training RMSE=2.533, Testing RMSE=718220.019\n",
      "train acc :  0.584069917569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc :  0.495013644257\n",
      "degree=11, lambda=0.000, Training RMSE=1.443, Testing RMSE=711767.780\n",
      "train acc :  0.621908829079\n",
      "validation acc :  0.497147109898\n",
      "degree=11, lambda=0.000, Training RMSE=2.095, Testing RMSE=712745.208\n",
      "train acc :  0.569768596683\n",
      "validation acc :  0.505135202183\n",
      "degree=11, lambda=0.000, Training RMSE=1.855, Testing RMSE=715225.846\n",
      "train acc :  0.599761644652\n",
      "validation acc :  0.502356735301\n",
      "degree=11, lambda=0.000, Training RMSE=2.203, Testing RMSE=718007.776\n",
      "train acc :  0.488628463601\n",
      "validation acc :  0.494319027537\n",
      "degree=11, lambda=0.000, Training RMSE=3.758, Testing RMSE=721378.229\n",
      "train acc :  0.576919257126\n",
      "validation acc :  0.502307119821\n",
      "degree=11, lambda=0.000, Training RMSE=1.402, Testing RMSE=725023.677\n",
      "train acc :  0.607011619823\n",
      "validation acc :  0.488836516993\n",
      "degree=11, lambda=0.000, Training RMSE=1.732, Testing RMSE=728016.418\n",
      "train acc :  0.664812791737\n",
      "validation acc :  0.480327462168\n",
      "degree=11, lambda=0.000, Training RMSE=2.832, Testing RMSE=729286.828\n",
      "train acc :  0.492005164366\n",
      "validation acc :  0.498213842719\n",
      "degree=11, lambda=0.000, Training RMSE=1.933, Testing RMSE=728629.687\n",
      "train acc :  0.590326745456\n",
      "validation acc :  0.512205408087\n",
      "degree=11, lambda=0.000, Training RMSE=1.972, Testing RMSE=725955.454\n",
      "train acc :  0.620121163969\n",
      "validation acc :  0.457032994294\n",
      "degree=11, lambda=0.000, Training RMSE=1.968, Testing RMSE=721745.056\n",
      "train acc :  0.560234382759\n",
      "validation acc :  0.554229719673\n",
      "degree=11, lambda=0.000, Training RMSE=2.220, Testing RMSE=715521.575\n",
      "train acc :  0.626080047671\n",
      "validation acc :  0.425651203175\n",
      "degree=11, lambda=0.000, Training RMSE=1.524, Testing RMSE=705662.143\n",
      "train acc :  0.642169033668\n",
      "validation acc :  0.522649466634\n",
      "degree=11, lambda=0.000, Training RMSE=2.523, Testing RMSE=688847.599\n",
      "train acc :  0.488827093058\n",
      "validation acc :  0.423145621434\n",
      "degree=11, lambda=0.000, Training RMSE=1.021, Testing RMSE=660259.948\n",
      "train acc :  0.717648227232\n",
      "validation acc :  0.576085338626\n",
      "degree=11, lambda=0.000, Training RMSE=1.378, Testing RMSE=614878.319\n",
      "train acc :  0.649518323567\n",
      "validation acc :  0.520516000992\n",
      "degree=11, lambda=0.000, Training RMSE=1.115, Testing RMSE=549780.241\n",
      "train acc :  0.65915185222\n",
      "validation acc :  0.53269660134\n",
      "degree=11, lambda=0.000, Training RMSE=1.497, Testing RMSE=467544.259\n",
      "train acc :  0.586254841593\n",
      "validation acc :  0.578838997767\n",
      "degree=11, lambda=0.000, Training RMSE=1.814, Testing RMSE=377560.114\n",
      "train acc :  0.586950044692\n",
      "validation acc :  0.445472587447\n",
      "degree=11, lambda=0.000, Training RMSE=1.515, Testing RMSE=295328.303\n",
      "train acc :  0.609494488033\n",
      "validation acc :  0.578590920367\n",
      "degree=11, lambda=0.001, Training RMSE=0.951, Testing RMSE=236452.434\n",
      "train acc :  0.700566093952\n",
      "validation acc :  0.578342842967\n",
      "degree=11, lambda=0.001, Training RMSE=0.936, Testing RMSE=207703.318\n",
      "train acc :  0.681001092462\n",
      "validation acc :  0.45080625155\n",
      "degree=11, lambda=0.001, Training RMSE=1.222, Testing RMSE=200661.617\n",
      "train acc :  0.631939616645\n",
      "validation acc :  0.571892830563\n",
      "degree=11, lambda=0.002, Training RMSE=0.956, Testing RMSE=193552.480\n",
      "train acc :  0.740887873672\n",
      "validation acc :  0.435623914661\n",
      "degree=11, lambda=0.002, Training RMSE=1.039, Testing RMSE=162474.822\n",
      "train acc :  0.635217002681\n",
      "validation acc :  0.532324485239\n",
      "degree=11, lambda=0.003, Training RMSE=0.899, Testing RMSE=97910.797\n",
      "train acc :  0.690634621114\n",
      "validation acc :  0.685611510791\n",
      "degree=11, lambda=0.004, Training RMSE=0.856, Testing RMSE=32166.637\n",
      "train acc :  0.724103684576\n",
      "validation acc :  0.714686182089\n",
      "degree=11, lambda=0.005, Training RMSE=0.836, Testing RMSE=101366.587\n",
      "train acc :  0.737709802364\n",
      "validation acc :  0.730116596378\n",
      "degree=11, lambda=0.007, Training RMSE=0.837, Testing RMSE=175416.797\n",
      "train acc :  0.739298838018\n",
      "validation acc :  0.725179856115\n",
      "degree=11, lambda=0.009, Training RMSE=0.837, Testing RMSE=213963.325\n",
      "train acc :  0.751017975966\n",
      "validation acc :  0.743785661126\n",
      "degree=11, lambda=0.013, Training RMSE=0.842, Testing RMSE=207232.444\n",
      "train acc :  0.740788558943\n",
      "validation acc :  0.730587943438\n",
      "degree=11, lambda=0.017, Training RMSE=0.857, Testing RMSE=158908.025\n",
      "train acc :  0.729565994637\n",
      "validation acc :  0.718184073431\n",
      "degree=11, lambda=0.023, Training RMSE=0.863, Testing RMSE=85713.162\n",
      "train acc :  0.723805740391\n",
      "validation acc :  0.709129248325\n",
      "degree=11, lambda=0.031, Training RMSE=0.868, Testing RMSE=9671.594\n",
      "train acc :  0.721024927997\n",
      "validation acc :  0.707740014885\n",
      "degree=11, lambda=0.041, Training RMSE=0.879, Testing RMSE=52578.306\n",
      "train acc :  0.710795510974\n",
      "validation acc :  0.6998759613\n",
      "degree=11, lambda=0.055, Training RMSE=0.888, Testing RMSE=94049.697\n",
      "train acc :  0.713278379184\n",
      "validation acc :  0.700893078641\n",
      "degree=11, lambda=0.074, Training RMSE=0.897, Testing RMSE=115476.330\n",
      "train acc :  0.708114013308\n",
      "validation acc :  0.696675762838\n",
      "degree=11, lambda=0.100, Training RMSE=0.906, Testing RMSE=120224.286\n",
      "train acc :  0.697586652101\n",
      "validation acc :  0.687794591913\n",
      "Best params for Ridge regression : degree =  9 , lambda =  0.000492388263171 , accuracy =  0.803572314562\n",
      "************ Model 4 ************* \n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724721001015\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.72534099876\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724833727877\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724439183858\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724495547289\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724439183858\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724439183858\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724439183858\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.72455191072\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724495547289\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.722976028946\n",
      "validation acc :  0.724382820426\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.723202170963\n",
      "validation acc :  0.724439183858\n",
      "degree=1, lambda=0.000, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.722523744912\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.001, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.722071460877\n",
      "validation acc :  0.724495547289\n",
      "degree=1, lambda=0.001, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.722976028946\n",
      "validation acc :  0.725284635329\n",
      "degree=1, lambda=0.001, Training RMSE=0.846, Testing RMSE=0.852\n",
      "train acc :  0.722523744912\n",
      "validation acc :  0.725002818172\n",
      "degree=1, lambda=0.002, Training RMSE=0.847, Testing RMSE=0.853\n",
      "train acc :  0.722976028946\n",
      "validation acc :  0.724608274152\n",
      "degree=1, lambda=0.002, Training RMSE=0.849, Testing RMSE=0.855\n",
      "train acc :  0.724106739032\n",
      "validation acc :  0.724326456995\n",
      "degree=1, lambda=0.003, Training RMSE=0.853, Testing RMSE=0.858\n",
      "train acc :  0.723654454998\n",
      "validation acc :  0.725059181603\n",
      "degree=1, lambda=0.004, Training RMSE=0.859, Testing RMSE=0.863\n",
      "train acc :  0.727951153324\n",
      "validation acc :  0.724721001015\n",
      "degree=1, lambda=0.005, Training RMSE=0.869, Testing RMSE=0.872\n",
      "train acc :  0.724785165084\n",
      "validation acc :  0.724439183858\n",
      "degree=1, lambda=0.007, Training RMSE=0.885, Testing RMSE=0.887\n",
      "train acc :  0.721393034826\n",
      "validation acc :  0.722973734641\n",
      "degree=1, lambda=0.009, Training RMSE=0.906, Testing RMSE=0.908\n",
      "train acc :  0.713478064224\n",
      "validation acc :  0.717055574343\n",
      "degree=1, lambda=0.013, Training RMSE=0.930, Testing RMSE=0.931\n",
      "train acc :  0.709859791949\n",
      "validation acc :  0.710855596889\n",
      "degree=1, lambda=0.017, Training RMSE=0.952, Testing RMSE=0.952\n",
      "train acc :  0.701266395296\n",
      "validation acc :  0.702401082178\n",
      "degree=1, lambda=0.023, Training RMSE=0.969, Testing RMSE=0.969\n",
      "train acc :  0.696065128901\n",
      "validation acc :  0.697948371097\n",
      "degree=1, lambda=0.031, Training RMSE=0.981, Testing RMSE=0.981\n",
      "train acc :  0.691090004523\n",
      "validation acc :  0.693157479427\n",
      "degree=1, lambda=0.041, Training RMSE=0.989, Testing RMSE=0.989\n",
      "train acc :  0.691090004523\n",
      "validation acc :  0.690113854131\n",
      "degree=1, lambda=0.055, Training RMSE=0.994, Testing RMSE=0.994\n",
      "train acc :  0.688376300317\n",
      "validation acc :  0.687915680307\n",
      "degree=1, lambda=0.074, Training RMSE=0.996, Testing RMSE=0.996\n",
      "train acc :  0.688150158299\n",
      "validation acc :  0.686281140796\n",
      "degree=1, lambda=0.100, Training RMSE=0.998, Testing RMSE=0.998\n",
      "train acc :  0.687245590231\n",
      "validation acc :  0.68566114305\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.860\n",
      "train acc :  0.762098597919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc :  0.729906436704\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.849\n",
      "train acc :  0.762550881954\n",
      "validation acc :  0.734190057491\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.839\n",
      "train acc :  0.762777023971\n",
      "validation acc :  0.739882764063\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.832\n",
      "train acc :  0.762550881954\n",
      "validation acc :  0.745631834066\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.828\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.748562732499\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.826\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.749295457107\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.751099086912\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.751606357795\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.751831811521\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.752226355541\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.752789989855\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.752677262992\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.75262089956\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.752733626423\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.752789989855\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.752733626423\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762324739937\n",
      "validation acc :  0.75262089956\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.825\n",
      "train acc :  0.762324739937\n",
      "validation acc :  0.752677262992\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.824\n",
      "train acc :  0.762550881954\n",
      "validation acc :  0.752846353286\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.824\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.752733626423\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.824\n",
      "train acc :  0.762550881954\n",
      "validation acc :  0.752733626423\n",
      "degree=2, lambda=0.000, Training RMSE=0.815, Testing RMSE=0.824\n",
      "train acc :  0.762550881954\n",
      "validation acc :  0.752677262992\n",
      "degree=2, lambda=0.001, Training RMSE=0.815, Testing RMSE=0.824\n",
      "train acc :  0.761646313885\n",
      "validation acc :  0.752395445835\n",
      "degree=2, lambda=0.001, Training RMSE=0.815, Testing RMSE=0.824\n",
      "train acc :  0.761872455902\n",
      "validation acc :  0.751155450344\n",
      "degree=2, lambda=0.001, Training RMSE=0.816, Testing RMSE=0.824\n",
      "train acc :  0.760515603799\n",
      "validation acc :  0.750704542893\n",
      "degree=2, lambda=0.002, Training RMSE=0.817, Testing RMSE=0.825\n",
      "train acc :  0.758706467662\n",
      "validation acc :  0.750760906324\n",
      "degree=2, lambda=0.002, Training RMSE=0.819, Testing RMSE=0.827\n",
      "train acc :  0.754409769335\n",
      "validation acc :  0.749633637696\n",
      "degree=2, lambda=0.003, Training RMSE=0.822, Testing RMSE=0.829\n",
      "train acc :  0.750565355043\n",
      "validation acc :  0.748168188479\n",
      "degree=2, lambda=0.004, Training RMSE=0.827, Testing RMSE=0.833\n",
      "train acc :  0.743781094527\n",
      "validation acc :  0.743151843084\n",
      "degree=2, lambda=0.005, Training RMSE=0.833, Testing RMSE=0.840\n",
      "train acc :  0.736092265943\n",
      "validation acc :  0.738360951415\n",
      "degree=2, lambda=0.007, Training RMSE=0.842, Testing RMSE=0.848\n",
      "train acc :  0.730212573496\n",
      "validation acc :  0.733570059745\n",
      "degree=2, lambda=0.009, Training RMSE=0.852, Testing RMSE=0.858\n",
      "train acc :  0.722523744912\n",
      "validation acc :  0.725960996505\n",
      "degree=2, lambda=0.013, Training RMSE=0.863, Testing RMSE=0.869\n",
      "train acc :  0.715965626413\n",
      "validation acc :  0.719479201894\n",
      "degree=2, lambda=0.017, Training RMSE=0.875, Testing RMSE=0.881\n",
      "train acc :  0.708729081863\n",
      "validation acc :  0.712715590125\n",
      "degree=2, lambda=0.023, Training RMSE=0.889, Testing RMSE=0.895\n",
      "train acc :  0.703527815468\n",
      "validation acc :  0.707248337279\n",
      "degree=2, lambda=0.031, Training RMSE=0.905, Testing RMSE=0.910\n",
      "train acc :  0.700814111262\n",
      "validation acc :  0.702626535904\n",
      "degree=2, lambda=0.041, Training RMSE=0.922, Testing RMSE=0.927\n",
      "train acc :  0.699231117142\n",
      "validation acc :  0.698342915117\n",
      "degree=2, lambda=0.055, Training RMSE=0.940, Testing RMSE=0.943\n",
      "train acc :  0.697648123021\n",
      "validation acc :  0.696990192763\n",
      "degree=2, lambda=0.074, Training RMSE=0.956, Testing RMSE=0.959\n",
      "train acc :  0.696517412935\n",
      "validation acc :  0.696313831586\n",
      "degree=2, lambda=0.100, Training RMSE=0.970, Testing RMSE=0.972\n",
      "train acc :  0.696517412935\n",
      "validation acc :  0.696257468155\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.943\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.773531732612\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.774433547514\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.774433547514\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.774546274377\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.774771728103\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.775166272123\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.775222635554\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.775109908691\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.775222635554\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.775166272123\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786748077793\n",
      "validation acc :  0.775166272123\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.78697421981\n",
      "validation acc :  0.775166272123\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.78697421981\n",
      "validation acc :  0.775166272123\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786521935776\n",
      "validation acc :  0.775222635554\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786295793758\n",
      "validation acc :  0.775109908691\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786295793758\n",
      "validation acc :  0.775222635554\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.786069651741\n",
      "validation acc :  0.775391725848\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.942\n",
      "train acc :  0.785165083673\n",
      "validation acc :  0.77544808928\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.943\n",
      "train acc :  0.785165083673\n",
      "validation acc :  0.775617179574\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.944\n",
      "train acc :  0.78539122569\n",
      "validation acc :  0.775786269868\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.945\n",
      "train acc :  0.786069651741\n",
      "validation acc :  0.774884454966\n",
      "degree=3, lambda=0.000, Training RMSE=0.792, Testing RMSE=0.948\n",
      "train acc :  0.787200361827\n",
      "validation acc :  0.775109908691\n",
      "degree=3, lambda=0.001, Training RMSE=0.792, Testing RMSE=0.951\n",
      "train acc :  0.785843509724\n",
      "validation acc :  0.775560816142\n",
      "degree=3, lambda=0.001, Training RMSE=0.793, Testing RMSE=0.955\n",
      "train acc :  0.78539122569\n",
      "validation acc :  0.775560816142\n",
      "degree=3, lambda=0.001, Training RMSE=0.794, Testing RMSE=0.958\n",
      "train acc :  0.784486657621\n",
      "validation acc :  0.77347536918\n",
      "degree=3, lambda=0.002, Training RMSE=0.796, Testing RMSE=0.958\n",
      "train acc :  0.783582089552\n",
      "validation acc :  0.772348100552\n",
      "degree=3, lambda=0.002, Training RMSE=0.798, Testing RMSE=0.953\n",
      "train acc :  0.779963817277\n",
      "validation acc :  0.769811746139\n",
      "degree=3, lambda=0.003, Training RMSE=0.802, Testing RMSE=0.943\n",
      "train acc :  0.773179556762\n",
      "validation acc :  0.763104497802\n",
      "degree=3, lambda=0.004, Training RMSE=0.808, Testing RMSE=0.928\n",
      "train acc :  0.767299864315\n",
      "validation acc :  0.756002705445\n",
      "degree=3, lambda=0.005, Training RMSE=0.816, Testing RMSE=0.912\n",
      "train acc :  0.754635911352\n",
      "validation acc :  0.747942734754\n",
      "degree=3, lambda=0.007, Training RMSE=0.827, Testing RMSE=0.897\n",
      "train acc :  0.742424242424\n",
      "validation acc :  0.740897305828\n",
      "degree=3, lambda=0.009, Training RMSE=0.839, Testing RMSE=0.885\n",
      "train acc :  0.73473541384\n",
      "validation acc :  0.73255551798\n",
      "degree=3, lambda=0.013, Training RMSE=0.852, Testing RMSE=0.878\n",
      "train acc :  0.726142017187\n",
      "validation acc :  0.726299177094\n",
      "degree=3, lambda=0.017, Training RMSE=0.864, Testing RMSE=0.879\n",
      "train acc :  0.716870194482\n",
      "validation acc :  0.720155563071\n",
      "degree=3, lambda=0.023, Training RMSE=0.878, Testing RMSE=0.886\n",
      "train acc :  0.708729081863\n",
      "validation acc :  0.715590125127\n",
      "degree=3, lambda=0.031, Training RMSE=0.894, Testing RMSE=0.900\n",
      "train acc :  0.70737222976\n",
      "validation acc :  0.711081050614\n",
      "degree=3, lambda=0.041, Training RMSE=0.911, Testing RMSE=0.921\n",
      "train acc :  0.704432383537\n",
      "validation acc :  0.709051967084\n",
      "degree=3, lambda=0.055, Training RMSE=0.930, Testing RMSE=0.944\n",
      "train acc :  0.704432383537\n",
      "validation acc :  0.708319242475\n",
      "degree=3, lambda=0.074, Training RMSE=0.948, Testing RMSE=0.965\n",
      "train acc :  0.704658525554\n",
      "validation acc :  0.708657423064\n",
      "degree=3, lambda=0.100, Training RMSE=0.963, Testing RMSE=0.980\n",
      "train acc :  0.70420624152\n",
      "validation acc :  0.709671964829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=4, lambda=0.000, Training RMSE=0.782, Testing RMSE=2.342\n",
      "train acc :  0.792175486205\n",
      "validation acc :  0.586292413482\n",
      "degree=4, lambda=0.000, Training RMSE=0.782, Testing RMSE=2.253\n",
      "train acc :  0.792853912257\n",
      "validation acc :  0.594465111036\n",
      "degree=4, lambda=0.000, Training RMSE=0.782, Testing RMSE=2.141\n",
      "train acc :  0.79262777024\n",
      "validation acc :  0.609570510653\n",
      "degree=4, lambda=0.000, Training RMSE=0.782, Testing RMSE=2.026\n",
      "train acc :  0.792401628223\n",
      "validation acc :  0.636512230865\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.932\n",
      "train acc :  0.79262777024\n",
      "validation acc :  0.669259384511\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.874\n",
      "train acc :  0.791949344188\n",
      "validation acc :  0.711531958066\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.847\n",
      "train acc :  0.791723202171\n",
      "validation acc :  0.750535452598\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.836\n",
      "train acc :  0.790592492085\n",
      "validation acc :  0.770600834179\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.833\n",
      "train acc :  0.790818634102\n",
      "validation acc :  0.776688084771\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.832\n",
      "train acc :  0.790818634102\n",
      "validation acc :  0.778548078007\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.832\n",
      "train acc :  0.790818634102\n",
      "validation acc :  0.77826626085\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.832\n",
      "train acc :  0.790366350068\n",
      "validation acc :  0.77905534889\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.832\n",
      "train acc :  0.790140208051\n",
      "validation acc :  0.778717168301\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.833\n",
      "train acc :  0.790366350068\n",
      "validation acc :  0.779224439184\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.833\n",
      "train acc :  0.790818634102\n",
      "validation acc :  0.779280802615\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.834\n",
      "train acc :  0.791270918137\n",
      "validation acc :  0.779111712321\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.836\n",
      "train acc :  0.791497060154\n",
      "validation acc :  0.778886258595\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.840\n",
      "train acc :  0.791949344188\n",
      "validation acc :  0.778604441438\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.845\n",
      "train acc :  0.792401628223\n",
      "validation acc :  0.77866080487\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.854\n",
      "train acc :  0.793532338308\n",
      "validation acc :  0.778886258595\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.867\n",
      "train acc :  0.79421076436\n",
      "validation acc :  0.778491714576\n",
      "degree=4, lambda=0.000, Training RMSE=0.783, Testing RMSE=1.885\n",
      "train acc :  0.793758480326\n",
      "validation acc :  0.778604441438\n",
      "degree=4, lambda=0.001, Training RMSE=0.784, Testing RMSE=1.905\n",
      "train acc :  0.793080054274\n",
      "validation acc :  0.778435351144\n",
      "degree=4, lambda=0.001, Training RMSE=0.784, Testing RMSE=1.922\n",
      "train acc :  0.791044776119\n",
      "validation acc :  0.77866080487\n",
      "degree=4, lambda=0.001, Training RMSE=0.785, Testing RMSE=1.925\n",
      "train acc :  0.791497060154\n",
      "validation acc :  0.778435351144\n",
      "degree=4, lambda=0.002, Training RMSE=0.787, Testing RMSE=1.904\n",
      "train acc :  0.789461781999\n",
      "validation acc :  0.777589899673\n",
      "degree=4, lambda=0.002, Training RMSE=0.790, Testing RMSE=1.853\n",
      "train acc :  0.786295793758\n",
      "validation acc :  0.773419005749\n",
      "degree=4, lambda=0.003, Training RMSE=0.794, Testing RMSE=1.772\n",
      "train acc :  0.77973767526\n",
      "validation acc :  0.768120843197\n",
      "degree=4, lambda=0.004, Training RMSE=0.800, Testing RMSE=1.680\n",
      "train acc :  0.771144278607\n",
      "validation acc :  0.7602299628\n",
      "degree=4, lambda=0.005, Training RMSE=0.809, Testing RMSE=1.604\n",
      "train acc :  0.75961103573\n",
      "validation acc :  0.750929996618\n",
      "degree=4, lambda=0.007, Training RMSE=0.821, Testing RMSE=1.567\n",
      "train acc :  0.74920850294\n",
      "validation acc :  0.741517303573\n",
      "degree=4, lambda=0.009, Training RMSE=0.833, Testing RMSE=1.551\n",
      "train acc :  0.739258254184\n",
      "validation acc :  0.735091872393\n",
      "degree=4, lambda=0.013, Training RMSE=0.845, Testing RMSE=1.515\n",
      "train acc :  0.731117141565\n",
      "validation acc :  0.729060985233\n",
      "degree=4, lambda=0.017, Training RMSE=0.855, Testing RMSE=1.431\n",
      "train acc :  0.721393034826\n",
      "validation acc :  0.722579190621\n",
      "degree=4, lambda=0.023, Training RMSE=0.864, Testing RMSE=1.303\n",
      "train acc :  0.717096336499\n",
      "validation acc :  0.718464660129\n",
      "degree=4, lambda=0.031, Training RMSE=0.873, Testing RMSE=1.163\n",
      "train acc :  0.714382632293\n",
      "validation acc :  0.71530830797\n",
      "degree=4, lambda=0.041, Training RMSE=0.883, Testing RMSE=1.047\n",
      "train acc :  0.712121212121\n",
      "validation acc :  0.712433772968\n",
      "degree=4, lambda=0.055, Training RMSE=0.896, Testing RMSE=0.978\n",
      "train acc :  0.709859791949\n",
      "validation acc :  0.710291962575\n",
      "degree=4, lambda=0.074, Training RMSE=0.911, Testing RMSE=0.954\n",
      "train acc :  0.708729081863\n",
      "validation acc :  0.708939240221\n",
      "degree=4, lambda=0.100, Training RMSE=0.929, Testing RMSE=0.957\n",
      "train acc :  0.707598371777\n",
      "validation acc :  0.70730470071\n",
      "degree=5, lambda=0.000, Training RMSE=0.777, Testing RMSE=5.601\n",
      "train acc :  0.792853912257\n",
      "validation acc :  0.564310675234\n",
      "degree=5, lambda=0.000, Training RMSE=0.777, Testing RMSE=5.494\n",
      "train acc :  0.79262777024\n",
      "validation acc :  0.589392402209\n",
      "degree=5, lambda=0.000, Training RMSE=0.777, Testing RMSE=5.412\n",
      "train acc :  0.792853912257\n",
      "validation acc :  0.628452260174\n",
      "degree=5, lambda=0.000, Training RMSE=0.777, Testing RMSE=5.361\n",
      "train acc :  0.79262777024\n",
      "validation acc :  0.669372111374\n",
      "degree=5, lambda=0.000, Training RMSE=0.777, Testing RMSE=5.333\n",
      "train acc :  0.791949344188\n",
      "validation acc :  0.710179235712\n",
      "degree=5, lambda=0.000, Training RMSE=0.777, Testing RMSE=5.320\n",
      "train acc :  0.792401628223\n",
      "validation acc :  0.737120955924\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.314\n",
      "train acc :  0.793080054274\n",
      "validation acc :  0.755777251719\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.311\n",
      "train acc :  0.793080054274\n",
      "validation acc :  0.765528125352\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.311\n",
      "train acc :  0.793080054274\n",
      "validation acc :  0.772291737121\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.311\n",
      "train acc :  0.793758480326\n",
      "validation acc :  0.774997181828\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.311\n",
      "train acc :  0.793758480326\n",
      "validation acc :  0.775222635554\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.310\n",
      "train acc :  0.793758480326\n",
      "validation acc :  0.775673543005\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.310\n",
      "train acc :  0.793532338308\n",
      "validation acc :  0.776631721339\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.309\n",
      "train acc :  0.793758480326\n",
      "validation acc :  0.776800811633\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.307\n",
      "train acc :  0.793306196291\n",
      "validation acc :  0.776969901928\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.302\n",
      "train acc :  0.793532338308\n",
      "validation acc :  0.776913538496\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.295\n",
      "train acc :  0.793532338308\n",
      "validation acc :  0.776857175065\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.282\n",
      "train acc :  0.793532338308\n",
      "validation acc :  0.777138992222\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.259\n",
      "train acc :  0.793306196291\n",
      "validation acc :  0.777364445947\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.219\n",
      "train acc :  0.79262777024\n",
      "validation acc :  0.778491714576\n",
      "degree=5, lambda=0.000, Training RMSE=0.778, Testing RMSE=5.150\n",
      "train acc :  0.793306196291\n",
      "validation acc :  0.778829895164\n",
      "degree=5, lambda=0.000, Training RMSE=0.779, Testing RMSE=5.034\n",
      "train acc :  0.793984622343\n",
      "validation acc :  0.778773531733\n",
      "degree=5, lambda=0.001, Training RMSE=0.779, Testing RMSE=4.844\n",
      "train acc :  0.792175486205\n",
      "validation acc :  0.780126254086\n",
      "degree=5, lambda=0.001, Training RMSE=0.780, Testing RMSE=4.539\n",
      "train acc :  0.79262777024\n",
      "validation acc :  0.780971705557\n",
      "degree=5, lambda=0.001, Training RMSE=0.782, Testing RMSE=4.067\n",
      "train acc :  0.789914066033\n",
      "validation acc :  0.779618983204\n",
      "degree=5, lambda=0.002, Training RMSE=0.784, Testing RMSE=3.378\n",
      "train acc :  0.790366350068\n",
      "validation acc :  0.776969901928\n",
      "degree=5, lambda=0.002, Training RMSE=0.787, Testing RMSE=2.488\n",
      "train acc :  0.786069651741\n",
      "validation acc :  0.77426445722\n",
      "degree=5, lambda=0.003, Training RMSE=0.791, Testing RMSE=1.614\n",
      "train acc :  0.780416101312\n",
      "validation acc :  0.769868109571\n",
      "degree=5, lambda=0.004, Training RMSE=0.798, Testing RMSE=1.324\n",
      "train acc :  0.771370420624\n",
      "validation acc :  0.762259046331\n",
      "degree=5, lambda=0.005, Training RMSE=0.806, Testing RMSE=1.645\n",
      "train acc :  0.763229308005\n",
      "validation acc :  0.754029985346\n",
      "degree=5, lambda=0.007, Training RMSE=0.815, Testing RMSE=1.887\n",
      "train acc :  0.754183627318\n",
      "validation acc :  0.747322737008\n",
      "degree=5, lambda=0.009, Training RMSE=0.826, Testing RMSE=1.940\n",
      "train acc :  0.746720940751\n",
      "validation acc :  0.739826400631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=5, lambda=0.013, Training RMSE=0.836, Testing RMSE=2.081\n",
      "train acc :  0.739936680235\n",
      "validation acc :  0.732780971706\n",
      "degree=5, lambda=0.017, Training RMSE=0.846, Testing RMSE=2.451\n",
      "train acc :  0.732926277702\n",
      "validation acc :  0.728328260625\n",
      "degree=5, lambda=0.023, Training RMSE=0.856, Testing RMSE=2.895\n",
      "train acc :  0.72591587517\n",
      "validation acc :  0.724101003269\n",
      "degree=5, lambda=0.031, Training RMSE=0.866, Testing RMSE=3.266\n",
      "train acc :  0.722071460877\n",
      "validation acc :  0.719761019051\n",
      "degree=5, lambda=0.041, Training RMSE=0.877, Testing RMSE=3.517\n",
      "train acc :  0.715061058345\n",
      "validation acc :  0.715759215421\n",
      "degree=5, lambda=0.055, Training RMSE=0.889, Testing RMSE=3.625\n",
      "train acc :  0.712799638173\n",
      "validation acc :  0.714350129636\n",
      "degree=5, lambda=0.074, Training RMSE=0.903, Testing RMSE=3.548\n",
      "train acc :  0.712347354138\n",
      "validation acc :  0.712321046105\n",
      "degree=5, lambda=0.100, Training RMSE=0.918, Testing RMSE=3.259\n",
      "train acc :  0.710312075984\n",
      "validation acc :  0.711362867771\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.434\n",
      "train acc :  0.796472184532\n",
      "validation acc :  0.597565099763\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.453\n",
      "train acc :  0.796472184532\n",
      "validation acc :  0.599988727314\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.481\n",
      "train acc :  0.796698326549\n",
      "validation acc :  0.603821440649\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.516\n",
      "train acc :  0.796246042515\n",
      "validation acc :  0.609626874084\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.564\n",
      "train acc :  0.797150610583\n",
      "validation acc :  0.61909593056\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.627\n",
      "train acc :  0.797829036635\n",
      "validation acc :  0.632566790666\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.708\n",
      "train acc :  0.797376752601\n",
      "validation acc :  0.65077217901\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.803\n",
      "train acc :  0.798281320669\n",
      "validation acc :  0.680701161087\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.897\n",
      "train acc :  0.797829036635\n",
      "validation acc :  0.712771953557\n",
      "degree=6, lambda=0.000, Training RMSE=0.773, Testing RMSE=61.977\n",
      "train acc :  0.798055178652\n",
      "validation acc :  0.742700935633\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=62.037\n",
      "train acc :  0.797602894618\n",
      "validation acc :  0.759328147898\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=62.076\n",
      "train acc :  0.797602894618\n",
      "validation acc :  0.767895389471\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=62.099\n",
      "train acc :  0.798281320669\n",
      "validation acc :  0.772742644572\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=62.111\n",
      "train acc :  0.798055178652\n",
      "validation acc :  0.773926276632\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=62.115\n",
      "train acc :  0.798055178652\n",
      "validation acc :  0.774377184083\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=62.109\n",
      "train acc :  0.797829036635\n",
      "validation acc :  0.775673543005\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=62.090\n",
      "train acc :  0.798281320669\n",
      "validation acc :  0.775673543005\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=62.048\n",
      "train acc :  0.798281320669\n",
      "validation acc :  0.775673543005\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=61.966\n",
      "train acc :  0.798733604704\n",
      "validation acc :  0.775898996731\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=61.810\n",
      "train acc :  0.797376752601\n",
      "validation acc :  0.776237177319\n",
      "degree=6, lambda=0.000, Training RMSE=0.774, Testing RMSE=61.532\n",
      "train acc :  0.797602894618\n",
      "validation acc :  0.776631721339\n",
      "degree=6, lambda=0.000, Training RMSE=0.775, Testing RMSE=61.050\n",
      "train acc :  0.797150610583\n",
      "validation acc :  0.777533536242\n",
      "degree=6, lambda=0.001, Training RMSE=0.775, Testing RMSE=60.256\n",
      "train acc :  0.796019900498\n",
      "validation acc :  0.777251719085\n",
      "degree=6, lambda=0.001, Training RMSE=0.776, Testing RMSE=59.020\n",
      "train acc :  0.796472184532\n",
      "validation acc :  0.77747717281\n",
      "degree=6, lambda=0.001, Training RMSE=0.778, Testing RMSE=57.180\n",
      "train acc :  0.793532338308\n",
      "validation acc :  0.777533536242\n",
      "degree=6, lambda=0.002, Training RMSE=0.780, Testing RMSE=54.469\n",
      "train acc :  0.791497060154\n",
      "validation acc :  0.778153533987\n",
      "degree=6, lambda=0.002, Training RMSE=0.783, Testing RMSE=50.387\n",
      "train acc :  0.790366350068\n",
      "validation acc :  0.77505354526\n",
      "degree=6, lambda=0.003, Training RMSE=0.787, Testing RMSE=44.281\n",
      "train acc :  0.783355947535\n",
      "validation acc :  0.771728102807\n",
      "degree=6, lambda=0.004, Training RMSE=0.793, Testing RMSE=35.832\n",
      "train acc :  0.773857982813\n",
      "validation acc :  0.765979032804\n",
      "degree=6, lambda=0.005, Training RMSE=0.800, Testing RMSE=25.620\n",
      "train acc :  0.768882858435\n",
      "validation acc :  0.758369969564\n",
      "degree=6, lambda=0.007, Training RMSE=0.809, Testing RMSE=15.183\n",
      "train acc :  0.759384893713\n",
      "validation acc :  0.750535452598\n",
      "degree=6, lambda=0.009, Training RMSE=0.820, Testing RMSE=7.164\n",
      "train acc :  0.75079149706\n",
      "validation acc :  0.742700935633\n",
      "degree=6, lambda=0.013, Training RMSE=0.831, Testing RMSE=6.943\n",
      "train acc :  0.744233378562\n",
      "validation acc :  0.735937323864\n",
      "degree=6, lambda=0.017, Training RMSE=0.843, Testing RMSE=10.498\n",
      "train acc :  0.733604703754\n",
      "validation acc :  0.729004621801\n",
      "degree=6, lambda=0.023, Training RMSE=0.853, Testing RMSE=12.758\n",
      "train acc :  0.729534147445\n",
      "validation acc :  0.724777364446\n",
      "degree=6, lambda=0.031, Training RMSE=0.861, Testing RMSE=13.759\n",
      "train acc :  0.723654454998\n",
      "validation acc :  0.722241010033\n",
      "degree=6, lambda=0.041, Training RMSE=0.869, Testing RMSE=14.211\n",
      "train acc :  0.718227046585\n",
      "validation acc :  0.717619208657\n",
      "degree=6, lambda=0.055, Training RMSE=0.876, Testing RMSE=14.422\n",
      "train acc :  0.715287200362\n",
      "validation acc :  0.715421034833\n",
      "degree=6, lambda=0.074, Training RMSE=0.884, Testing RMSE=14.260\n",
      "train acc :  0.713930348259\n",
      "validation acc :  0.713842858753\n",
      "degree=6, lambda=0.100, Training RMSE=0.893, Testing RMSE=13.526\n",
      "train acc :  0.711668928087\n",
      "validation acc :  0.712828316988\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=768.172\n",
      "train acc :  0.807779285391\n",
      "validation acc :  0.585954232894\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=768.164\n",
      "train acc :  0.807553143374\n",
      "validation acc :  0.586517867208\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=768.138\n",
      "train acc :  0.807553143374\n",
      "validation acc :  0.591252395446\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=768.087\n",
      "train acc :  0.806874717322\n",
      "validation acc :  0.600777815353\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=768.009\n",
      "train acc :  0.806874717322\n",
      "validation acc :  0.615657761245\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=767.905\n",
      "train acc :  0.80710085934\n",
      "validation acc :  0.637695862924\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=767.788\n",
      "train acc :  0.806648575305\n",
      "validation acc :  0.670781197159\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=767.674\n",
      "train acc :  0.805291723202\n",
      "validation acc :  0.71209559238\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=767.573\n",
      "train acc :  0.805065581185\n",
      "validation acc :  0.745631834066\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=767.481\n",
      "train acc :  0.805065581185\n",
      "validation acc :  0.765077217901\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=767.382\n",
      "train acc :  0.805744007237\n",
      "validation acc :  0.773362642318\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=767.247\n",
      "train acc :  0.805517865219\n",
      "validation acc :  0.776293540751\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=767.032\n",
      "train acc :  0.805744007237\n",
      "validation acc :  0.777195355653\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=766.668\n",
      "train acc :  0.805517865219\n",
      "validation acc :  0.778998985458\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=766.047\n",
      "train acc :  0.804839439168\n",
      "validation acc :  0.779224439184\n",
      "degree=7, lambda=0.000, Training RMSE=0.761, Testing RMSE=765.006\n",
      "train acc :  0.804839439168\n",
      "validation acc :  0.780069890655\n",
      "degree=7, lambda=0.000, Training RMSE=0.762, Testing RMSE=763.319\n",
      "train acc :  0.805065581185\n",
      "validation acc :  0.780520798106\n",
      "degree=7, lambda=0.000, Training RMSE=0.762, Testing RMSE=760.671\n",
      "train acc :  0.803934871099\n",
      "validation acc :  0.780915342126\n",
      "degree=7, lambda=0.000, Training RMSE=0.762, Testing RMSE=756.545\n",
      "train acc :  0.803708729082\n",
      "validation acc :  0.781648066734\n",
      "degree=7, lambda=0.000, Training RMSE=0.763, Testing RMSE=749.995\n",
      "train acc :  0.80303030303\n",
      "validation acc :  0.782042610754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=7, lambda=0.000, Training RMSE=0.763, Testing RMSE=739.292\n",
      "train acc :  0.80144730891\n",
      "validation acc :  0.782098974186\n",
      "degree=7, lambda=0.000, Training RMSE=0.764, Testing RMSE=721.501\n",
      "train acc :  0.800316598824\n",
      "validation acc :  0.781422613009\n",
      "degree=7, lambda=0.001, Training RMSE=0.765, Testing RMSE=692.112\n",
      "train acc :  0.79986431479\n",
      "validation acc :  0.781817157029\n",
      "degree=7, lambda=0.001, Training RMSE=0.766, Testing RMSE=645.118\n",
      "train acc :  0.798733604704\n",
      "validation acc :  0.7806898884\n",
      "degree=7, lambda=0.001, Training RMSE=0.768, Testing RMSE=574.270\n",
      "train acc :  0.798055178652\n",
      "validation acc :  0.779675346635\n",
      "degree=7, lambda=0.002, Training RMSE=0.771, Testing RMSE=476.070\n",
      "train acc :  0.79579375848\n",
      "validation acc :  0.779168075752\n",
      "degree=7, lambda=0.002, Training RMSE=0.775, Testing RMSE=353.942\n",
      "train acc :  0.791723202171\n",
      "validation acc :  0.776518994476\n",
      "degree=7, lambda=0.003, Training RMSE=0.781, Testing RMSE=221.426\n",
      "train acc :  0.784260515604\n",
      "validation acc :  0.771953556533\n",
      "degree=7, lambda=0.004, Training RMSE=0.789, Testing RMSE=103.478\n",
      "train acc :  0.778380823157\n",
      "validation acc :  0.76744448202\n",
      "degree=7, lambda=0.005, Training RMSE=0.799, Testing RMSE=60.691\n",
      "train acc :  0.768656716418\n",
      "validation acc :  0.758764513584\n",
      "degree=7, lambda=0.007, Training RMSE=0.809, Testing RMSE=104.454\n",
      "train acc :  0.759158751696\n",
      "validation acc :  0.751268177207\n",
      "degree=7, lambda=0.009, Training RMSE=0.819, Testing RMSE=138.849\n",
      "train acc :  0.751922207146\n",
      "validation acc :  0.742926389359\n",
      "degree=7, lambda=0.013, Training RMSE=0.829, Testing RMSE=153.534\n",
      "train acc :  0.744911804613\n",
      "validation acc :  0.738248224552\n",
      "degree=7, lambda=0.017, Training RMSE=0.838, Testing RMSE=152.212\n",
      "train acc :  0.739258254184\n",
      "validation acc :  0.732780971706\n",
      "degree=7, lambda=0.023, Training RMSE=0.846, Testing RMSE=138.836\n",
      "train acc :  0.734056987788\n",
      "validation acc :  0.728497350919\n",
      "degree=7, lambda=0.031, Training RMSE=0.854, Testing RMSE=117.695\n",
      "train acc :  0.728629579376\n",
      "validation acc :  0.723931912975\n",
      "degree=7, lambda=0.041, Training RMSE=0.862, Testing RMSE=93.491\n",
      "train acc :  0.724785165084\n",
      "validation acc :  0.72094465111\n",
      "degree=7, lambda=0.055, Training RMSE=0.870, Testing RMSE=69.904\n",
      "train acc :  0.720488466757\n",
      "validation acc :  0.719197384737\n",
      "degree=7, lambda=0.074, Training RMSE=0.879, Testing RMSE=48.534\n",
      "train acc :  0.717548620534\n",
      "validation acc :  0.716548303461\n",
      "degree=7, lambda=0.100, Training RMSE=0.888, Testing RMSE=31.216\n",
      "train acc :  0.715287200362\n",
      "validation acc :  0.715477398264\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5191.709\n",
      "train acc :  0.812075983718\n",
      "validation acc :  0.596663284861\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5190.442\n",
      "train acc :  0.812302125735\n",
      "validation acc :  0.593112388682\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5189.053\n",
      "train acc :  0.812528267752\n",
      "validation acc :  0.617517754481\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5187.681\n",
      "train acc :  0.812302125735\n",
      "validation acc :  0.62912862135\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5186.415\n",
      "train acc :  0.812302125735\n",
      "validation acc :  0.637019501747\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5185.195\n",
      "train acc :  0.812528267752\n",
      "validation acc :  0.64316311577\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5183.826\n",
      "train acc :  0.812754409769\n",
      "validation acc :  0.648630368617\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5182.021\n",
      "train acc :  0.812528267752\n",
      "validation acc :  0.656633975876\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5179.482\n",
      "train acc :  0.811849841701\n",
      "validation acc :  0.668864840491\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5176.047\n",
      "train acc :  0.812075983718\n",
      "validation acc :  0.686393867659\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5171.903\n",
      "train acc :  0.812754409769\n",
      "validation acc :  0.710291962575\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5167.669\n",
      "train acc :  0.812302125735\n",
      "validation acc :  0.737402773081\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5164.107\n",
      "train acc :  0.812302125735\n",
      "validation acc :  0.7602299628\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5161.652\n",
      "train acc :  0.812075983718\n",
      "validation acc :  0.773137188592\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5160.081\n",
      "train acc :  0.812754409769\n",
      "validation acc :  0.778829895164\n",
      "degree=8, lambda=0.000, Training RMSE=0.750, Testing RMSE=5158.273\n",
      "train acc :  0.813206693804\n",
      "validation acc :  0.782098974186\n",
      "degree=8, lambda=0.000, Training RMSE=0.751, Testing RMSE=5153.640\n",
      "train acc :  0.811171415649\n",
      "validation acc :  0.784015330853\n",
      "degree=8, lambda=0.000, Training RMSE=0.751, Testing RMSE=5140.975\n",
      "train acc :  0.810945273632\n",
      "validation acc :  0.785706233796\n",
      "degree=8, lambda=0.000, Training RMSE=0.751, Testing RMSE=5110.705\n",
      "train acc :  0.810719131615\n",
      "validation acc :  0.786100777815\n",
      "degree=8, lambda=0.000, Training RMSE=0.752, Testing RMSE=5046.214\n",
      "train acc :  0.811171415649\n",
      "validation acc :  0.786100777815\n",
      "degree=8, lambda=0.000, Training RMSE=0.752, Testing RMSE=4919.682\n",
      "train acc :  0.811171415649\n",
      "validation acc :  0.785988050953\n",
      "degree=8, lambda=0.000, Training RMSE=0.753, Testing RMSE=4687.449\n",
      "train acc :  0.809362279512\n",
      "validation acc :  0.786213504678\n",
      "degree=8, lambda=0.001, Training RMSE=0.754, Testing RMSE=4289.993\n",
      "train acc :  0.80710085934\n",
      "validation acc :  0.786438958404\n",
      "degree=8, lambda=0.001, Training RMSE=0.756, Testing RMSE=3666.868\n",
      "train acc :  0.805970149254\n",
      "validation acc :  0.784466238305\n",
      "degree=8, lambda=0.001, Training RMSE=0.760, Testing RMSE=2796.174\n",
      "train acc :  0.803256445047\n",
      "validation acc :  0.782944425657\n",
      "degree=8, lambda=0.002, Training RMSE=0.765, Testing RMSE=1746.468\n",
      "train acc :  0.798055178652\n",
      "validation acc :  0.779280802615\n",
      "degree=8, lambda=0.002, Training RMSE=0.772, Testing RMSE=696.647\n",
      "train acc :  0.793984622343\n",
      "validation acc :  0.776011723594\n",
      "degree=8, lambda=0.003, Training RMSE=0.781, Testing RMSE=312.931\n",
      "train acc :  0.78697421981\n",
      "validation acc :  0.770037199865\n",
      "degree=8, lambda=0.004, Training RMSE=0.789, Testing RMSE=849.094\n",
      "train acc :  0.777476255088\n",
      "validation acc :  0.764569947018\n",
      "degree=8, lambda=0.005, Training RMSE=0.798, Testing RMSE=1126.314\n",
      "train acc :  0.771144278607\n",
      "validation acc :  0.757637244955\n",
      "degree=8, lambda=0.007, Training RMSE=0.806, Testing RMSE=1159.771\n",
      "train acc :  0.762098597919\n",
      "validation acc :  0.752000901815\n",
      "degree=8, lambda=0.009, Training RMSE=0.815, Testing RMSE=1024.492\n",
      "train acc :  0.755766621438\n",
      "validation acc :  0.746082741517\n",
      "degree=8, lambda=0.013, Training RMSE=0.823, Testing RMSE=797.985\n",
      "train acc :  0.74920850294\n",
      "validation acc :  0.742813662496\n",
      "degree=8, lambda=0.017, Training RMSE=0.832, Testing RMSE=554.354\n",
      "train acc :  0.744459520579\n",
      "validation acc :  0.738248224552\n",
      "degree=8, lambda=0.023, Training RMSE=0.840, Testing RMSE=352.818\n",
      "train acc :  0.738127544098\n",
      "validation acc :  0.734302784354\n",
      "degree=8, lambda=0.031, Training RMSE=0.847, Testing RMSE=238.555\n",
      "train acc :  0.732926277702\n",
      "validation acc :  0.729906436704\n",
      "degree=8, lambda=0.041, Training RMSE=0.855, Testing RMSE=240.760\n",
      "train acc :  0.726820443238\n",
      "validation acc :  0.7261300868\n",
      "degree=8, lambda=0.055, Training RMSE=0.863, Testing RMSE=302.023\n",
      "train acc :  0.725011307101\n",
      "validation acc :  0.721508285424\n",
      "degree=8, lambda=0.074, Training RMSE=0.871, Testing RMSE=353.782\n",
      "train acc :  0.720488466757\n",
      "validation acc :  0.718633750423\n",
      "degree=8, lambda=0.100, Training RMSE=0.879, Testing RMSE=370.381\n",
      "train acc :  0.717774762551\n",
      "validation acc :  0.716773757186\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=22981.482\n",
      "train acc :  0.814789687924\n",
      "validation acc :  0.49639274039\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=22986.916\n",
      "train acc :  0.815015829941\n",
      "validation acc :  0.50321271559\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=22993.550\n",
      "train acc :  0.81592039801\n",
      "validation acc :  0.497125464998\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=23000.636\n",
      "train acc :  0.816372682044\n",
      "validation acc :  0.501747266374\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=23007.879\n",
      "train acc :  0.816146540027\n",
      "validation acc :  0.504847255101\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=23015.999\n",
      "train acc :  0.815015829941\n",
      "validation acc :  0.54170893924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=23026.202\n",
      "train acc :  0.814563545907\n",
      "validation acc :  0.514316311577\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=23039.674\n",
      "train acc :  0.815015829941\n",
      "validation acc :  0.545992560027\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=23056.395\n",
      "train acc :  0.81592039801\n",
      "validation acc :  0.596268740841\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=23073.688\n",
      "train acc :  0.815694255993\n",
      "validation acc :  0.65195581107\n",
      "degree=9, lambda=0.000, Training RMSE=0.744, Testing RMSE=23085.612\n",
      "train acc :  0.815468113976\n",
      "validation acc :  0.638879494984\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=23084.447\n",
      "train acc :  0.815241971958\n",
      "validation acc :  0.712997407282\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=23063.113\n",
      "train acc :  0.814789687924\n",
      "validation acc :  0.762202682899\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=23016.310\n",
      "train acc :  0.814563545907\n",
      "validation acc :  0.774828091534\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=22937.899\n",
      "train acc :  0.813432835821\n",
      "validation acc :  0.776631721339\n",
      "degree=9, lambda=0.000, Training RMSE=0.745, Testing RMSE=22813.731\n",
      "train acc :  0.812302125735\n",
      "validation acc :  0.78147897644\n",
      "degree=9, lambda=0.000, Training RMSE=0.746, Testing RMSE=22611.957\n",
      "train acc :  0.812075983718\n",
      "validation acc :  0.784578965167\n",
      "degree=9, lambda=0.000, Training RMSE=0.746, Testing RMSE=22271.484\n",
      "train acc :  0.813658977838\n",
      "validation acc :  0.782718971931\n",
      "degree=9, lambda=0.000, Training RMSE=0.746, Testing RMSE=21686.596\n",
      "train acc :  0.813885119855\n",
      "validation acc :  0.78469169203\n",
      "degree=9, lambda=0.000, Training RMSE=0.746, Testing RMSE=20687.986\n",
      "train acc :  0.813885119855\n",
      "validation acc :  0.785424416638\n",
      "degree=9, lambda=0.000, Training RMSE=0.747, Testing RMSE=19031.238\n",
      "train acc :  0.811623699683\n",
      "validation acc :  0.785593506933\n",
      "degree=9, lambda=0.000, Training RMSE=0.748, Testing RMSE=16426.528\n",
      "train acc :  0.81026684758\n",
      "validation acc :  0.78626986811\n",
      "degree=9, lambda=0.001, Training RMSE=0.750, Testing RMSE=12669.108\n",
      "train acc :  0.809362279512\n",
      "validation acc :  0.785593506933\n",
      "degree=9, lambda=0.001, Training RMSE=0.752, Testing RMSE=7891.022\n",
      "train acc :  0.809588421529\n",
      "validation acc :  0.784466238305\n",
      "degree=9, lambda=0.001, Training RMSE=0.757, Testing RMSE=2759.302\n",
      "train acc :  0.806422433288\n",
      "validation acc :  0.781760793597\n",
      "degree=9, lambda=0.002, Training RMSE=0.763, Testing RMSE=1719.671\n",
      "train acc :  0.803708729082\n",
      "validation acc :  0.778829895164\n",
      "degree=9, lambda=0.002, Training RMSE=0.770, Testing RMSE=4680.132\n",
      "train acc :  0.797829036635\n",
      "validation acc :  0.774715364671\n",
      "degree=9, lambda=0.003, Training RMSE=0.778, Testing RMSE=5832.957\n",
      "train acc :  0.790366350068\n",
      "validation acc :  0.768909931237\n",
      "degree=9, lambda=0.004, Training RMSE=0.787, Testing RMSE=5369.004\n",
      "train acc :  0.781772953415\n",
      "validation acc :  0.761639048585\n",
      "degree=9, lambda=0.005, Training RMSE=0.795, Testing RMSE=3855.006\n",
      "train acc :  0.77250113071\n",
      "validation acc :  0.756735430053\n",
      "degree=9, lambda=0.007, Training RMSE=0.804, Testing RMSE=2269.332\n",
      "train acc :  0.764812302126\n",
      "validation acc :  0.752057265246\n",
      "degree=9, lambda=0.009, Training RMSE=0.813, Testing RMSE=2066.838\n",
      "train acc :  0.75802804161\n",
      "validation acc :  0.745350016909\n",
      "degree=9, lambda=0.013, Training RMSE=0.822, Testing RMSE=2884.691\n",
      "train acc :  0.75079149706\n",
      "validation acc :  0.741742757299\n",
      "degree=9, lambda=0.017, Training RMSE=0.830, Testing RMSE=3519.861\n",
      "train acc :  0.744007236545\n",
      "validation acc :  0.737402773081\n",
      "degree=9, lambda=0.023, Training RMSE=0.837, Testing RMSE=3743.927\n",
      "train acc :  0.739710538218\n",
      "validation acc :  0.732893698568\n",
      "degree=9, lambda=0.031, Training RMSE=0.844, Testing RMSE=3548.291\n",
      "train acc :  0.735639981909\n",
      "validation acc :  0.728891894939\n",
      "degree=9, lambda=0.041, Training RMSE=0.851, Testing RMSE=2971.874\n",
      "train acc :  0.729534147445\n",
      "validation acc :  0.725848269643\n",
      "degree=9, lambda=0.055, Training RMSE=0.859, Testing RMSE=2143.194\n",
      "train acc :  0.725463591135\n",
      "validation acc :  0.722748280915\n",
      "degree=9, lambda=0.074, Training RMSE=0.866, Testing RMSE=1287.346\n",
      "train acc :  0.722071460877\n",
      "validation acc :  0.7193101116\n",
      "degree=9, lambda=0.100, Training RMSE=0.874, Testing RMSE=702.143\n",
      "train acc :  0.720940750791\n",
      "validation acc :  0.717562845226\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=92247.148\n",
      "train acc :  0.81433740389\n",
      "validation acc :  0.499830909706\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=92278.818\n",
      "train acc :  0.814563545907\n",
      "validation acc :  0.498703641078\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=92189.500\n",
      "train acc :  0.814111261872\n",
      "validation acc :  0.526265359035\n",
      "degree=10, lambda=0.000, Training RMSE=0.744, Testing RMSE=92172.727\n",
      "train acc :  0.813885119855\n",
      "validation acc :  0.512512681772\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=92150.685\n",
      "train acc :  0.817051108096\n",
      "validation acc :  0.506594521474\n",
      "degree=10, lambda=0.000, Training RMSE=0.744, Testing RMSE=92119.502\n",
      "train acc :  0.815694255993\n",
      "validation acc :  0.502649081276\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=92070.559\n",
      "train acc :  0.814563545907\n",
      "validation acc :  0.492165483035\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=91991.047\n",
      "train acc :  0.815241971958\n",
      "validation acc :  0.497632735881\n",
      "degree=10, lambda=0.000, Training RMSE=0.742, Testing RMSE=91862.544\n",
      "train acc :  0.815241971958\n",
      "validation acc :  0.512456318341\n",
      "degree=10, lambda=0.000, Training RMSE=0.745, Testing RMSE=91663.668\n",
      "train acc :  0.813432835821\n",
      "validation acc :  0.508736331868\n",
      "degree=10, lambda=0.000, Training RMSE=0.744, Testing RMSE=91369.645\n",
      "train acc :  0.815468113976\n",
      "validation acc :  0.537988952767\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=90965.230\n",
      "train acc :  0.815468113976\n",
      "validation acc :  0.503099988727\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=90456.857\n",
      "train acc :  0.816598824062\n",
      "validation acc :  0.525363544133\n",
      "degree=10, lambda=0.000, Training RMSE=0.744, Testing RMSE=89879.388\n",
      "train acc :  0.816146540027\n",
      "validation acc :  0.44104385075\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=89283.131\n",
      "train acc :  0.815468113976\n",
      "validation acc :  0.41539848946\n",
      "degree=10, lambda=0.000, Training RMSE=0.743, Testing RMSE=88704.056\n",
      "train acc :  0.819086386251\n",
      "validation acc :  0.431912974862\n",
      "degree=10, lambda=0.000, Training RMSE=0.744, Testing RMSE=88116.376\n",
      "train acc :  0.814789687924\n",
      "validation acc :  0.47835644234\n",
      "degree=10, lambda=0.000, Training RMSE=0.744, Testing RMSE=87394.655\n",
      "train acc :  0.81592039801\n",
      "validation acc :  0.689606583249\n",
      "degree=10, lambda=0.000, Training RMSE=0.744, Testing RMSE=86286.087\n",
      "train acc :  0.815015829941\n",
      "validation acc :  0.677432082065\n",
      "degree=10, lambda=0.000, Training RMSE=0.745, Testing RMSE=84366.834\n",
      "train acc :  0.81592039801\n",
      "validation acc :  0.673993912749\n",
      "degree=10, lambda=0.000, Training RMSE=0.746, Testing RMSE=80914.177\n",
      "train acc :  0.81433740389\n",
      "validation acc :  0.406831247886\n",
      "degree=10, lambda=0.000, Training RMSE=0.747, Testing RMSE=74635.823\n",
      "train acc :  0.811171415649\n",
      "validation acc :  0.742080937887\n",
      "degree=10, lambda=0.001, Training RMSE=0.749, Testing RMSE=63434.883\n",
      "train acc :  0.81026684758\n",
      "validation acc :  0.686675684816\n",
      "degree=10, lambda=0.001, Training RMSE=0.751, Testing RMSE=44965.646\n",
      "train acc :  0.808909995477\n",
      "validation acc :  0.780971705557\n",
      "degree=10, lambda=0.001, Training RMSE=0.755, Testing RMSE=18725.218\n",
      "train acc :  0.809814563546\n",
      "validation acc :  0.7806898884\n",
      "degree=10, lambda=0.002, Training RMSE=0.762, Testing RMSE=11937.256\n",
      "train acc :  0.805744007237\n",
      "validation acc :  0.777420809379\n",
      "degree=10, lambda=0.002, Training RMSE=0.769, Testing RMSE=40161.307\n",
      "train acc :  0.796019900498\n",
      "validation acc :  0.772517190847\n",
      "degree=10, lambda=0.003, Training RMSE=0.777, Testing RMSE=60594.447\n",
      "train acc :  0.791270918137\n",
      "validation acc :  0.767951752903\n",
      "degree=10, lambda=0.004, Training RMSE=0.785, Testing RMSE=71190.809\n",
      "train acc :  0.782451379466\n",
      "validation acc :  0.761469958291\n",
      "degree=10, lambda=0.005, Training RMSE=0.793, Testing RMSE=72742.668\n",
      "train acc :  0.775440976934\n",
      "validation acc :  0.756115432308\n",
      "degree=10, lambda=0.007, Training RMSE=0.801, Testing RMSE=67058.816\n",
      "train acc :  0.767752148349\n",
      "validation acc :  0.751324540638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=10, lambda=0.009, Training RMSE=0.810, Testing RMSE=56008.976\n",
      "train acc :  0.761194029851\n",
      "validation acc :  0.746026378086\n",
      "degree=10, lambda=0.013, Training RMSE=0.819, Testing RMSE=41448.152\n",
      "train acc :  0.752600633198\n",
      "validation acc :  0.741742757299\n",
      "degree=10, lambda=0.017, Training RMSE=0.828, Testing RMSE=25744.696\n",
      "train acc :  0.745364088648\n",
      "validation acc :  0.738811858866\n",
      "degree=10, lambda=0.023, Training RMSE=0.835, Testing RMSE=13023.866\n",
      "train acc :  0.739710538218\n",
      "validation acc :  0.734866418668\n",
      "degree=10, lambda=0.031, Training RMSE=0.842, Testing RMSE=11420.267\n",
      "train acc :  0.733378561737\n",
      "validation acc :  0.731822793372\n",
      "degree=10, lambda=0.041, Training RMSE=0.848, Testing RMSE=16795.386\n",
      "train acc :  0.731117141565\n",
      "validation acc :  0.728271897193\n",
      "degree=10, lambda=0.055, Training RMSE=0.853, Testing RMSE=20714.475\n",
      "train acc :  0.729760289462\n",
      "validation acc :  0.725002818172\n",
      "degree=10, lambda=0.074, Training RMSE=0.859, Testing RMSE=22383.528\n",
      "train acc :  0.727951153324\n",
      "validation acc :  0.722861007778\n",
      "degree=10, lambda=0.100, Training RMSE=0.865, Testing RMSE=22265.800\n",
      "train acc :  0.724106739032\n",
      "validation acc :  0.718859204148\n",
      "degree=11, lambda=0.000, Training RMSE=1.268, Testing RMSE=1384008.664\n",
      "train acc :  0.518317503392\n",
      "validation acc :  0.507721790103\n",
      "degree=11, lambda=0.000, Training RMSE=2.166, Testing RMSE=1367682.017\n",
      "train acc :  0.455676164631\n",
      "validation acc :  0.500056363431\n",
      "degree=11, lambda=0.000, Training RMSE=1.818, Testing RMSE=1362759.062\n",
      "train acc :  0.610809588422\n",
      "validation acc :  0.501916356668\n",
      "degree=11, lambda=0.000, Training RMSE=1.281, Testing RMSE=1361602.418\n",
      "train acc :  0.577566711895\n",
      "validation acc :  0.503325442453\n",
      "degree=11, lambda=0.000, Training RMSE=1.330, Testing RMSE=1362438.467\n",
      "train acc :  0.696743554953\n",
      "validation acc :  0.503438169316\n",
      "degree=11, lambda=0.000, Training RMSE=1.854, Testing RMSE=1363410.389\n",
      "train acc :  0.698326549073\n",
      "validation acc :  0.496956374704\n",
      "degree=11, lambda=0.000, Training RMSE=2.415, Testing RMSE=1366343.406\n",
      "train acc :  0.436906377205\n",
      "validation acc :  0.507890880397\n",
      "degree=11, lambda=0.000, Training RMSE=1.468, Testing RMSE=1370963.825\n",
      "train acc :  0.566485753053\n",
      "validation acc :  0.503381805884\n",
      "degree=11, lambda=0.000, Training RMSE=3.011, Testing RMSE=1377263.744\n",
      "train acc :  0.347354138399\n",
      "validation acc :  0.477341900575\n",
      "degree=11, lambda=0.000, Training RMSE=2.459, Testing RMSE=1384523.832\n",
      "train acc :  0.33921302578\n",
      "validation acc :  0.505410889415\n",
      "degree=11, lambda=0.000, Training RMSE=2.522, Testing RMSE=1391582.754\n",
      "train acc :  0.345771144279\n",
      "validation acc :  0.488163679405\n",
      "degree=11, lambda=0.000, Training RMSE=1.965, Testing RMSE=1397294.429\n",
      "train acc :  0.452057892356\n",
      "validation acc :  0.491940029309\n",
      "degree=11, lambda=0.000, Training RMSE=1.744, Testing RMSE=1400509.639\n",
      "train acc :  0.47263681592\n",
      "validation acc :  0.37211137414\n",
      "degree=11, lambda=0.000, Training RMSE=2.294, Testing RMSE=1399830.240\n",
      "train acc :  0.393939393939\n",
      "validation acc :  0.584432420246\n",
      "degree=11, lambda=0.000, Training RMSE=1.624, Testing RMSE=1393099.617\n",
      "train acc :  0.693351424695\n",
      "validation acc :  0.408860331417\n",
      "degree=11, lambda=0.000, Training RMSE=2.950, Testing RMSE=1377315.658\n",
      "train acc :  0.705110809588\n",
      "validation acc :  0.630255889979\n",
      "degree=11, lambda=0.000, Training RMSE=2.653, Testing RMSE=1347401.554\n",
      "train acc :  0.333785617368\n",
      "validation acc :  0.520629015894\n",
      "degree=11, lambda=0.000, Training RMSE=1.433, Testing RMSE=1294727.628\n",
      "train acc :  0.653098145635\n",
      "validation acc :  0.606752339082\n",
      "degree=11, lambda=0.000, Training RMSE=1.411, Testing RMSE=1205919.202\n",
      "train acc :  0.638398914518\n",
      "validation acc :  0.631326795175\n",
      "degree=11, lambda=0.000, Training RMSE=1.749, Testing RMSE=1060221.010\n",
      "train acc :  0.698326549073\n",
      "validation acc :  0.37250591816\n",
      "degree=11, lambda=0.000, Training RMSE=1.395, Testing RMSE=832045.737\n",
      "train acc :  0.671641791045\n",
      "validation acc :  0.634201330177\n",
      "degree=11, lambda=0.000, Training RMSE=1.231, Testing RMSE=503040.205\n",
      "train acc :  0.74197195839\n",
      "validation acc :  0.457332882426\n",
      "degree=11, lambda=0.001, Training RMSE=1.156, Testing RMSE=83629.880\n",
      "train acc :  0.5289461782\n",
      "validation acc :  0.370984105512\n",
      "degree=11, lambda=0.001, Training RMSE=0.855, Testing RMSE=379945.046\n",
      "train acc :  0.750339213026\n",
      "validation acc :  0.37250591816\n",
      "degree=11, lambda=0.001, Training RMSE=0.841, Testing RMSE=797221.120\n",
      "train acc :  0.75079149706\n",
      "validation acc :  0.396629466802\n",
      "degree=11, lambda=0.002, Training RMSE=0.885, Testing RMSE=1088510.324\n",
      "train acc :  0.75237449118\n",
      "validation acc :  0.380058617969\n",
      "degree=11, lambda=0.002, Training RMSE=0.916, Testing RMSE=1206968.894\n",
      "train acc :  0.66960651289\n",
      "validation acc :  0.64874309548\n",
      "degree=11, lambda=0.003, Training RMSE=0.845, Testing RMSE=1155187.777\n",
      "train acc :  0.732700135685\n",
      "validation acc :  0.716435576598\n",
      "degree=11, lambda=0.004, Training RMSE=0.790, Testing RMSE=967917.288\n",
      "train acc :  0.78132066938\n",
      "validation acc :  0.757693608387\n",
      "degree=11, lambda=0.005, Training RMSE=0.816, Testing RMSE=702566.972\n",
      "train acc :  0.753505201266\n",
      "validation acc :  0.739037312592\n",
      "degree=11, lambda=0.007, Training RMSE=0.802, Testing RMSE=421220.808\n",
      "train acc :  0.759384893713\n",
      "validation acc :  0.746871829557\n",
      "degree=11, lambda=0.009, Training RMSE=0.812, Testing RMSE=176373.480\n",
      "train acc :  0.755540479421\n",
      "validation acc :  0.742024574456\n",
      "degree=11, lambda=0.013, Training RMSE=0.818, Testing RMSE=44920.262\n",
      "train acc :  0.750339213026\n",
      "validation acc :  0.73898094916\n",
      "degree=11, lambda=0.017, Training RMSE=0.831, Testing RMSE=142082.035\n",
      "train acc :  0.748530076888\n",
      "validation acc :  0.734415511216\n",
      "degree=11, lambda=0.023, Training RMSE=0.832, Testing RMSE=205271.442\n",
      "train acc :  0.744007236545\n",
      "validation acc :  0.73497914553\n",
      "degree=11, lambda=0.031, Training RMSE=0.838, Testing RMSE=226958.434\n",
      "train acc :  0.737901402081\n",
      "validation acc :  0.733175515725\n",
      "degree=11, lambda=0.041, Training RMSE=0.845, Testing RMSE=215899.524\n",
      "train acc :  0.732926277702\n",
      "validation acc :  0.730131890429\n",
      "degree=11, lambda=0.055, Training RMSE=0.851, Testing RMSE=180404.078\n",
      "train acc :  0.732021709634\n",
      "validation acc :  0.726468267388\n",
      "degree=11, lambda=0.074, Training RMSE=0.854, Testing RMSE=130418.540\n",
      "train acc :  0.729986431479\n",
      "validation acc :  0.723988276406\n",
      "degree=11, lambda=0.100, Training RMSE=0.859, Testing RMSE=78117.943\n",
      "train acc :  0.729534147445\n",
      "validation acc :  0.722241010033\n",
      "Best params for Ridge regression : degree =  8 , lambda =  0.000661474064123 , accuracy =  0.786438958404\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCpKkjqEgSeoYCpKkjqEgSercBBiYjBJLMYDKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20413f4c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for i, model in enumerate(models):\n",
    "    print('************ Model {} ************* '.format(i+1))\n",
    "    weights_no_nan, degree_no_nan, lambda_no_nan = test_ridge_regression(\n",
    "        model['x_train'], model['y_train'], model['x_validation'], model['y_validation'], \n",
    "        degrees = np.linspace(1,11,11), lambdas=np.logspace(-6,-1, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8106238605375409\n"
     ]
    }
   ],
   "source": [
    "# Model 1:\n",
    "# Best params for Ridge regression : degree =  9 , lambda =  0.000206913808111 , accuracy =  0.842567272273\n",
    "# Best params for Ridge regression : degree =  9 , lambda =  0.00016681005372 , accuracy =  0.842804520197\n",
    "# Feat Aug\n",
    "# Best params for Ridge regression : degree =  8 , lambda =  1.91448197617e-05 , accuracy =  0.842417431479\n",
    "# Model 2:\n",
    "# Best params for Ridge regression : degree =  7 , lambda =  0.000615848211066 , accuracy =  0.779464300147\n",
    "# Best params for Ridge regression : degree =  7 , lambda =  0.000661474064123 , accuracy =  0.779415805894\n",
    "# Feat Aug\n",
    "# Best params for Ridge regression : degree =  7 , lambda =  0.000661474064123 , accuracy =  0.778866204355\n",
    "# Model 3:\n",
    "# Best params for Ridge regression : degree =  9 , lambda =  0.000615848211066 , accuracy =  0.803870007442\n",
    "# Best params for Ridge regression : degree =  9 , lambda =  0.000272833337649 , accuracy =  0.805284048623\n",
    "# Feat Aug\n",
    "# Best params for Ridge regression : degree =  9 , lambda =  0.000492388263171 , accuracy =  0.803572314562\n",
    "# Model 4:\n",
    "# Best params for Ridge regression : degree =  8 , lambda =  0.000615848211066 , accuracy =  0.789595310563\n",
    "# Best params for Ridge regression : degree =  8 , lambda =  0.000366524123708 , accuracy =  0.790046218014\n",
    "# Feat Aug\n",
    "# Best params for Ridge regression : degree =  8 , lambda =  0.000661474064123 , accuracy =  0.786438958404\n",
    "\n",
    "acc = np.array([0.842567272273, 0.779464300147, 0.805284048623, 0.790046218014])\n",
    "print('Accuracy = {}'.format(acc.dot(repart_jet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.233524097725\n",
      "568238 568238\n",
      "0.312458238802\n",
      "568238 568238\n",
      "0.276830947233\n",
      "568238 568238\n",
      "0.285257530654\n",
      "568238 568238\n"
     ]
    }
   ],
   "source": [
    "from scripts.implementations import build_poly, least_squares, accuracy\n",
    "from scripts.proj1_helpers import predict_labels\n",
    "\n",
    "y_pred = np.zeros(len(y_test))\n",
    "best_degrees = [9, 7, 9, 8]\n",
    "best_lambda = [0.000206913808111, 0.000615848211066, 0.000272833337649, 0.000366524123708]\n",
    "for i, model in enumerate(models):\n",
    "    \n",
    "    phi_train = lib.build_poly(model['x_train'], best_degrees[i])\n",
    "    phi_test = lib.build_poly(model['x_test'], best_degrees[i])\n",
    "    mse_tr, weights = lib.ridge_regression(model['y_train'], phi_train, best_lambda[i])\n",
    "    y_pred[model['id_test']] = predict_labels(weights, phi_test)\n",
    "    print(mse_tr)\n",
    "    print(len(y_pred), len(model['id_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved ...\n"
     ]
    }
   ],
   "source": [
    "from scripts.proj1_helpers import create_csv_submission\n",
    "\n",
    "create_csv_submission(ids_test, y_pred, 'submission.csv')\n",
    "print('Results saved ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
