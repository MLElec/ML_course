{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scripts.implementations as lib  # Add personal library\n",
    "import scripts.proj1_helpers as helper  # Add personal library\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DATA_TRAIN = os.path.join(DATA_FOLDER, 'train.csv')\n",
    "DATA_TEST = os.path.join(DATA_FOLDER, 'test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning \n",
    "\n",
    "\n",
    "## 1.1 Data Exploration\n",
    "\n",
    "We first load the data to see what are the repartition of the data. In our case prediction gives `s` for signal and `b` for backgroud. In this case around 2/3 of the data (65.73%) are labeled as background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, x_train, ids, header = helper.load_csv_data(DATA_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Repartition of {} labels, s: {:.2f}%, b: {:.2f}%'.format(\n",
    "    len(y_train), np.mean(y_train==1)*100, np.mean(y_train==-1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [the Higgs boson machine learning challenge](https://higgsml.lal.in2p3.fr/files/2014/04/documentation_v1.8.pdf) some variable are indicated as \"may be undefined\" when it can happen that they are meaning-\n",
    "less or cannot be computed. In this case, their value is -999.0, which is outside the normal range of all variables. Let's put them to NaN so they will be easier to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[x_train == -999] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the repartition of the NaN along the features. We can see that some features seems to have the same amount of NaN value. The second graph shows that some features seems to have NaNs values axactly at the same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "plt.bar(np.arange(len(header)), np.sum(np.isnan(x_train), axis=0))\n",
    "plt.xticks(np.arange(len(header)), header, rotation='vertical')\n",
    "plt.ylim(0, len(y_train)); plt.xlabel('Features'); plt.xlabel('#Sample'); plt.title('NaN sum per feature')\n",
    "plt.grid(); plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 20))\n",
    "plt.matshow(np.isnan(x_train)[:100, :].T)\n",
    "plt.yticks(np.arange(len(header)), header)\n",
    "plt.xlabel('Features'); plt.xlabel('#Sample'); plt.title('NAN sum per feature')\n",
    "plt.show(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the NaN value gave us any information (`s` or `b`) ? We can see that is NaN is not present we are more likely to find a signal `s`. If NaN is present it seems that we are close to the initial distribution with 34%-66% ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NaN is present, s: {:.2f}, b: {:.2f}'.format(\n",
    "    np.mean(y_train[np.any(np.isnan(x_train), axis=1)] == 1), \n",
    "    np.mean(y_train[np.any(np.isnan(x_train), axis=1)] == -1)))\n",
    "print('NaN is not present, s: {:.2f}, b: {:.2f}'.format(\n",
    "    np.mean(y_train[~np.any(np.isnan(x_train), axis=1)] == 1), \n",
    "    np.mean(y_train[~np.any(np.isnan(x_train), axis=1)] == -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also take a look at the feature ranges. it can give us insights of the data. We can see that features (16), (19), (21), (26) and (29) are actually angles (range in $[-\\pi, \\pi]$). To be certain we checked it directly on the documentation. Note that we are ignoring the NaN values to compute the min and max.\n",
    "\n",
    "We have to be careful with those results since the output gives us no imformation about the data distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feature in enumerate(x_train.T):\n",
    "    print('Feature {} - {} has range: [{:.4f}, {:.4f}]'.format(\n",
    "        i+1, header[i], np.nanmin(feature), np.nanmax(feature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Cleaning - Naive\n",
    "\n",
    "One solution to deal with NaN is to remove feature vectors that contains NaN values. After this we can normalize the data (0 mean and 1 standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with NaN\n",
    "keep_id = np.nonzero(np.sum(np.isnan(x_train), axis=0) == 0)[0]\n",
    "x_naive = x_train[:, keep_id]\n",
    "# normalize features\n",
    "x_naive = (x_naive - np.mean(x_naive, axis=0))/np.std(x_naive, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.ml import cross_validation_ls\n",
    "\n",
    "degrees = np.linspace(1, 6, 6).astype(int)\n",
    "for i, degree in enumerate(degrees):\n",
    "    acc, _, _ = cross_validation_ls(y_train, x_naive, degree=degree)\n",
    "    print('{}/{} Least square deg {} with acc {:.4f}'.format(i+1, len(degrees), degree, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Cleaning - Dealing with NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize features\n",
    "x_no_nan = x_train.copy()\n",
    "x_no_nan = (x_no_nan - np.nanmean(x_no_nan, axis=0))/np.nanstd(x_no_nan, axis=0)\n",
    "x_no_nan = np.nan_to_num(x_no_nan)\n",
    "print('\\nStd:', np.std(x_no_nan, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, degree in enumerate(degrees):\n",
    "    acc, _, _ = cross_validation_ls(y_train, x_no_nan, degree=degree)\n",
    "    print('{}/{} Least square deg {} with acc {:.4f}'.format(i+1, len(degrees), degree, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data Cleaning - Dealing with physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags, count = np.unique(x_train[:, 22], return_counts=True)\n",
    "print('Repartition of #jet {} along data {}%'.format(tags, 100*count/len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 4))\n",
    "nan_j = []\n",
    "for i in range(4):\n",
    "    nan_j.append(np.array([ np.sum(np.logical_and(x_train[:, 22] == i, np.isnan(f))) for f in x_train.T]))\n",
    "    plt.bar(np.arange(len(header)), nan_j[-1], \n",
    "            label='#Jet={}'.format(i), bottom = np.sum(np.array(nan_j[:-1]), axis=0))\n",
    "plt.xticks(np.arange(len(header)), header, rotation='vertical'); \n",
    "plt.ylim(0, len(y_train));\n",
    "plt.xlabel('Features'); plt.xlabel('#Sample'); plt.title('NaN sum per feature and Jet number')\n",
    "plt.legend(); plt.grid(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_jet(y, x, id_current_jet, id_jet=22):\n",
    "    id_select = (x[:, id_jet] == id_current_jet)\n",
    "    x_jet = x[id_select, :]\n",
    "    y_jet = y[id_select]\n",
    "    return id_select, y_jet, x_jet[:, np.logical_and(~np.any(np.isnan(x_jet), axis=0), np.arange(x.shape[1]) != id_jet)]\n",
    "   \n",
    "models = []\n",
    "for i in range(4):\n",
    "    _, y_train_j, x_train_j = get_data_jet(y_train, x_train, i)\n",
    "    # Last vector is only 0's for #jet == 0, we remove it\n",
    "    if i != 0:\n",
    "        x_train_j = (x_train_j - np.mean(x_train_j, axis=0))/np.std(x_train_j, axis=0)\n",
    "    else:\n",
    "        x_train_j = (x_train_j[:, :-1] - np.mean(x_train_j[:, :-1], axis=0))/np.std(x_train_j[:, :-1], axis=0)\n",
    "    models.append( {'y_train': y_train_j, 'x_train': x_train_j} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.linspace(1, 12, 12).astype(int)\n",
    "best_acc = np.zeros(len(models))\n",
    "for j, model in enumerate(models):\n",
    "    acc = np.zeros(len(degrees))\n",
    "    tr = np.zeros(len(degrees))\n",
    "    te = np.zeros(len(degrees))\n",
    "    for i, degree in enumerate(degrees):\n",
    "        acc[i], tr[i], te[i] = cross_validation_ls(model['y_train'], model['x_train'], degree=degree)\n",
    "        print('Jet-{}-{}: Least square max acc: {:.4f}, {:.4f}, {:.4f}'.format(\n",
    "            j, degree, acc[i], tr[i], te[i]))\n",
    "    best_acc[j] = np.max(acc)\n",
    "    #print('Jet-{}: Least square max acc: {:.4f}, {:.4f}, {:.4f}, for deg: {}'.format(\n",
    "    #    j, np.max(acc), tr[np.argmax(acc)], te[np.argmax(acc)], degrees[np.argmax(acc)]))\n",
    "# Sum accuracy with the actual repartition of #jet over dataset\n",
    "print('\\n-> Theorical overal accuracy: {:.4f}'.format(np.sum(best_acc * count/len(y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Submission test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, x_test, ids, header = helper.load_csv_data(DATA_TEST)\n",
    "x_test[x_test == -999] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ = 0\n",
    "for i in range(4):\n",
    "    id_test_j, y_test_j, x_test_j = get_data_jet(y_test, x_test, i)\n",
    "    # Last vector is only 0's for #jet == 0, we remove it\n",
    "    if i != 0:\n",
    "        x_test_j = (x_test_j - np.mean(x_test_j, axis=0))/np.std(x_test_j, axis=0)\n",
    "    else:\n",
    "        x_test_j = (x_test_j[:, :-1] - np.mean(x_test_j[:, :-1], axis=0))/np.std(x_test_j[:, :-1], axis=0)\n",
    "    sum_ += np.sum(id_test_j)\n",
    "    models[i]['id_test'] = id_test_j \n",
    "    models[i]['x_test'] = x_test_j\n",
    "    \n",
    "print(sum_, len(y_test))\n",
    "print(np.nonzero(models[3]['id_test']))\n",
    "print(np.nonzero(x_test[:, 22]==3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.implementations import build_poly, least_squares, accuracy\n",
    "from scripts.proj1_helpers import predict_labels\n",
    "\n",
    "y_pred = np.zeros(len(y_test))\n",
    "best_degrees = [1, 4, 3, 4]\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    # Build polynomial matrix\n",
    "    _phi_train = build_poly(model['x_train'], best_degrees[i])\n",
    "    _phi_test = build_poly(model['x_test'], best_degrees[i])\n",
    "    print(np.shape(model['x_train']), np.shape(model['x_test']))\n",
    "    loss, weights = least_squares(model['y_train'], _phi_train)\n",
    "    y_pred_tmp = predict_labels(weights, _phi_test)\n",
    "    y_pred[model['id_test']] = y_pred_tmp\n",
    "    print(accuracy(model['y_train'], _phi_train.dot(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.proj1_helpers import create_csv_submission\n",
    "\n",
    "create_csv_submission(ids, y_pred, 'submission.csv')\n",
    "print('Results saved ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model \n",
    "..... meeeeehhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.implementations import build_poly, least_squares, least_squares_GD, accuracy\n",
    "\n",
    "xt =  build_poly(x, 3)\n",
    "loss, w = least_squares(yb, xt)\n",
    "print(loss, np.shape(w))\n",
    "#loss, w = least_squares_GD(yb, xt, max_iters=700, loss_name='mae')\n",
    "#print(loss)\n",
    "\n",
    "print(np.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(yb, xt.dot(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.ml import cross_validation_ls\n",
    "\n",
    "_acc = []\n",
    "_loss_tr = []\n",
    "_loss_te = []\n",
    "\n",
    "for degree in range(1,6):\n",
    "    print('Least square, deg: {}'.format(degree))\n",
    "    acc, loss_tr, loss_te = cross_validation_ls(yb, x, degree=degree)\n",
    "    _acc.append(acc); _loss_te.append(loss_te), _loss_tr.append(loss_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.ml import cross_validation_ridge\n",
    "\n",
    "_acc = []\n",
    "_loss_tr = []\n",
    "_loss_te = []\n",
    "\n",
    "for degree in range(1,6):\n",
    "    print('Ridge, deg: {}'.format(degree))\n",
    "    acc, loss_tr, loss_te = cross_validation_ridge(yb, x, degree=degree)\n",
    "    _acc.append(acc); _loss_te.append(loss_te), _loss_tr.append(loss_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
