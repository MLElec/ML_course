{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ridge regression \n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scripts.implementations as lib  # Add personal library\n",
    "import scripts.proj1_helpers as helper  # Add personal library\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DATA_TRAIN = os.path.join(DATA_FOLDER, 'train.csv')\n",
    "DATA_TEST = os.path.join(DATA_FOLDER, 'test.csv')\n",
    "\n",
    "y, x, ids, header = helper.load_csv_data(DATA_TRAIN)\n",
    "y_train, x_train,  y_validation, x_validation = lib.sep_valid_train_data(x,y, 0.8);\n",
    "N = 1000\n",
    "\n",
    "y_train = np.expand_dims(y_train[:N], axis=1)\n",
    "x_train = x_train[:N]\n",
    "y_validation = np.expand_dims(y_validation[:N], axis=1)\n",
    "x_validation = x_validation[:N]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[x_train == -999] = np.nan\n",
    "x_validation[x_validation == -999] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 - DER_mass_MMC has range: [15.3870, 685.7010]\n",
      "Feature 2 - DER_mass_transverse_met_lep has range: [0.0710, 426.2370]\n",
      "Feature 3 - DER_mass_vis has range: [13.0000, 529.5010]\n",
      "Feature 4 - DER_pt_h has range: [0.1020, 509.6950]\n",
      "Feature 5 - DER_deltaeta_jet_jet has range: [0.0170, 7.3970]\n",
      "Feature 6 - DER_mass_jet_jet has range: [33.9890, 2285.6570]\n",
      "Feature 7 - DER_prodeta_jet_jet has range: [-13.3980, 11.3740]\n",
      "Feature 8 - DER_deltar_tau_lep has range: [0.4520, 5.3210]\n",
      "Feature 9 - DER_pt_tot has range: [0.0370, 215.5600]\n",
      "Feature 10 - DER_sum_pt has range: [47.1170, 1282.5230]\n",
      "Feature 11 - DER_pt_ratio_lep_tau has range: [0.1380, 10.1490]\n",
      "Feature 12 - DER_met_phi_centrality has range: [-1.4140, 1.4140]\n",
      "Feature 13 - DER_lep_eta_centrality has range: [0.0000, 1.0000]\n",
      "Feature 14 - PRI_tau_pt has range: [20.0010, 381.6290]\n",
      "Feature 15 - PRI_tau_eta has range: [-2.4860, 2.4630]\n",
      "Feature 16 - PRI_tau_phi has range: [-3.1350, 3.1350]\n",
      "Feature 17 - PRI_lep_pt has range: [26.1050, 306.6340]\n",
      "Feature 18 - PRI_lep_eta has range: [-2.4370, 2.4560]\n",
      "Feature 19 - PRI_lep_phi has range: [-3.1400, 3.1190]\n",
      "Feature 20 - PRI_met has range: [0.7960, 358.2660]\n",
      "Feature 21 - PRI_met_phi has range: [-3.1310, 3.1360]\n",
      "Feature 22 - PRI_met_sumet has range: [28.3940, 1364.6320]\n",
      "Feature 23 - PRI_jet_num has range: [0.0000, 3.0000]\n",
      "Feature 24 - PRI_jet_leading_pt has range: [30.0680, 681.2320]\n",
      "Feature 25 - PRI_jet_leading_eta has range: [-4.1330, 4.2790]\n",
      "Feature 26 - PRI_jet_leading_phi has range: [-3.1320, 3.1410]\n",
      "Feature 27 - PRI_jet_subleading_pt has range: [30.0890, 358.7010]\n",
      "Feature 28 - PRI_jet_subleading_eta has range: [-4.4720, 4.3370]\n",
      "Feature 29 - PRI_jet_subleading_phi has range: [-3.0850, 3.0830]\n",
      "Feature 30 - PRI_jet_all_pt has range: [0.0000, 1039.9320]\n"
     ]
    }
   ],
   "source": [
    "for i, feature in enumerate(x_train.T):\n",
    "    print('Feature {} - {} has range: [{:.4f}, {:.4f}]'.format(\n",
    "        i+1, header[i], np.nanmin(feature), np.nanmax(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.1565\n",
      "[  22.175    27.3595   16.1565   32.294     1.156   139.223     1.451\n",
      "    0.591     8.693    49.5995    0.434     0.9215    0.418     8.314\n",
      "    0.909     1.593     9.4345    1.0265    1.6145   16.0235    1.6605\n",
      "   60.0805    1.       24.974     1.309     1.485    10.352     1.646\n",
      "    1.523    39.939 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkhpn0EtS4wx6SWqcQS9JjftfUquHYAzeRQ8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c7ec70dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mad(arr):\n",
    "    \"\"\" Median Absolute Deviation: a \"Robust\" version of standard deviation.\n",
    "        Indices variabililty of the sample.\n",
    "        https://en.wikipedia.org/wiki/Median_absolute_deviation \n",
    "    \"\"\"\n",
    "    arr = np.ma.array(arr).compressed() # should be faster to not use masked arrays.\n",
    "    med = np.median(arr)\n",
    "    return np.median(np.abs(arr - med))\n",
    "\n",
    "def mad_3(x_feat):\n",
    "    \"\"\" Median Absolute Deviation: a \"Robust\" version of standard deviation.\n",
    "        Indices variabililty of the sample.\n",
    "        https://en.wikipedia.org/wiki/Median_absolute_deviation \n",
    "    \"\"\"\n",
    "    mad_res = np.ones(x_feat.shape[1])\n",
    "    for i, col in enumerate(x_feat.T):\n",
    "        arr = np.ma.array(col).compressed() # should be faster to not use masked arrays.\n",
    "        med = np.nanmedian(arr)\n",
    "        mad_res[i] = np.nanmedian(np.abs(arr - med))\n",
    "\n",
    "    return mad_res\n",
    "\n",
    "# plt.hist(x_train[:,2], bins=50)\n",
    "x_t = x_train[:,2].copy()\n",
    "x_t[x_t > 250] = 0\n",
    "#print(np.std(x_t))\n",
    "#print(np.std(x_train[:,2]))\n",
    "print(mad(x_train[:,2]))\n",
    "print(mad_3(x_train))\n",
    "#print('')\n",
    "#print(np.mean(x_t))\n",
    "#print(np.mean(x_train[:,2]))\n",
    "#print(np.median(x_train[:,2]))\n",
    "\n",
    "plt.hist((x_train[:,2]-np.median(x_train[:,2]))/mad(x_train[:,2]), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c7e7f4f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((x_train[:,2]-np.mean(x_train[:,2]))/np.std(x_train[:,2]), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge with no_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Std: [ 0.9127  1.      1.      1.      0.5357  0.5357  0.5357  1.      1.      1.\n",
      "  1.      1.      0.5357  1.      1.      1.      1.      1.      1.      1.\n",
      "  1.      1.      1.      0.7829  0.7829  0.7829  0.5357  0.5357  0.5357\n",
      "  1.    ]\n",
      "\n",
      "Std: [ 0.9149  1.      1.      1.      0.5357  0.5357  0.5357  1.      1.      1.\n",
      "  1.      1.      0.5357  1.      1.      1.      1.      1.      1.      1.\n",
      "  1.      1.      1.      0.7701  0.7701  0.7701  0.5357  0.5357  0.5357\n",
      "  1.    ]\n"
     ]
    }
   ],
   "source": [
    "# normalize features\n",
    "x_no_nan = x_train.copy()\n",
    "x_no_nan = (x_no_nan - np.nanmean(x_no_nan, axis=0))/np.nanstd(x_no_nan, axis=0)\n",
    "x_no_nan = np.nan_to_num(x_no_nan)\n",
    "print('\\nStd:', np.std(x_no_nan, axis=0))\n",
    "\n",
    "# normalize features\n",
    "x_no_nan_val = x_validation.copy()\n",
    "x_no_nan_val = (x_no_nan_val - np.nanmean(x_no_nan_val, axis=0))/np.nanstd(x_no_nan_val, axis=0)\n",
    "x_no_nan_val = np.nan_to_num(x_no_nan_val)\n",
    "print('\\nStd:', np.std(x_no_nan_val, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc deg=1 lam=10.0 loss=551.5785396238977: 0.5560, 0.7450\n",
      "validation acc deg=1 lam=20.6913808111479 loss=584.3543076815613: 0.5600, 0.7420\n",
      "validation acc deg=1 lam=42.81332398719393 loss=621.6680062599812: 0.5390, 0.7420\n",
      "validation acc deg=1 lam=88.58667904100822 loss=649.1770871668065: 0.5620, 0.7300\n",
      "validation acc deg=1 lam=183.29807108324357 loss=667.229694159798: 0.5200, 0.7290\n",
      "validation acc deg=1 lam=379.26901907322497 loss=681.9457627407514: 0.5740, 0.7020\n",
      "validation acc deg=1 lam=784.7599703514607 loss=692.8326523649348: 0.5350, 0.6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:8: RuntimeWarning: invalid value encountered in multiply\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:17: RuntimeWarning: overflow encountered in double_scalars\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:21: RuntimeWarning: overflow encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc deg=1 lam=1623.776739188721 loss=nan: 0.0000, 0.0000\n",
      "validation acc deg=1 lam=3359.818286283781 loss=nan: 0.0000, 0.0000\n",
      "validation acc deg=1 lam=6951.927961775606 loss=nan: 0.0000, 0.0000\n",
      "validation acc deg=1 lam=14384.498882876629 loss=nan: 0.0000, 0.0000\n",
      "validation acc deg=1 lam=29763.51441631313 loss=nan: 0.0000, 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fbeddec04703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'validation MAX acc: {:.6f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mlogistic_regression_gradient_descent_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_no_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_no_nan_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegrees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-fbeddec04703>\u001b[0m in \u001b[0;36mlogistic_regression_gradient_descent_demo\u001b[0;34m(x, y, x_val, y_val, degrees)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_penalized_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# converge criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fbeddec04703>\u001b[0m in \u001b[0;36mlearning_by_penalized_gradient\u001b[0;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fbeddec04703>\u001b[0m in \u001b[0;36mpenalized_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"\"\"return the loss and gradient.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msqueeze\u001b[0;34m(a, axis)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# First try to use the new axis= parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# For backwards compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1+np.exp(-t))\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    ypred = sigmoid(tx.dot(w))\n",
    "    return -np.sum(y*np.log(ypred) + (1-y)*np.log(1-ypred))\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    pred = sigmoid(tx.dot(w))\n",
    "    return tx.T.dot(pred-y)\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\"\"\"\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ * np.squeeze(w.T.dot(w))\n",
    "    \n",
    "    batch_size = 100\n",
    "    (y_st, tx_st) = [batch for batch in lib._batch_iter(y, tx, batch_size)][0]\n",
    "    grad = calculate_gradient(y_st, tx_st, w) + 2 * lambda_ * w  # Derivate of term\n",
    "    return loss, grad\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, grad = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w -= gamma * grad\n",
    "    return loss, w\n",
    "\n",
    "def logistic_regression_gradient_descent_demo(x, y, x_val, y_val, degrees):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 1e-3\n",
    "    \n",
    "    degrees = np.linspace(1,2,2).astype(int)\n",
    "    lambdas = np.logspace(1,7,20)\n",
    "    \n",
    "    acc_train = np.zeros((len(degrees), len(lambdas)))\n",
    "    acc_val = np.zeros((len(degrees), len(lambdas)))\n",
    "    acc_loss = np.zeros((len(degrees), len(lambdas)))\n",
    "    \n",
    "    _y = (y+1)/2\n",
    "    _y_val = (y_val+1)/2\n",
    "\n",
    "    \n",
    "    # Get ploynomial\n",
    "\n",
    "    for i, degree in enumerate(degrees):\n",
    "        \n",
    "        phi_train = lib.build_poly(x, degree)\n",
    "        phi_test = lib.build_poly(x_val, degree)\n",
    "        w = np.zeros((phi_train.shape[1], 1))\n",
    "        \n",
    "        for j, lambda_ in enumerate(lambdas):\n",
    "            losses = []\n",
    "            # start the logistic regression\n",
    "            for iter in range(max_iter):\n",
    "                # get loss and update w.\n",
    "                loss, w = learning_by_penalized_gradient(_y, phi_train, w, gamma, lambda_)\n",
    "                # converge criterion\n",
    "                losses.append(loss)\n",
    "                if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "                    break\n",
    "            acc_train[i, j] = lib.accuracy(y, phi_train.dot(w))\n",
    "            acc_val[i, j] = lib.accuracy(y, phi_test.dot(w))\n",
    "            acc_loss[i, j] = losses[-1]\n",
    "            print('validation acc deg={} lam={} loss={}: {:.4f}, {:.4f}'.format(\n",
    "                degree, lambda_, acc_loss[i, j], acc_val[i, j], acc_train[i, j]))\n",
    "\n",
    "    print('validation MAX acc: {:.6f}'.format(np.max(acc_val)))\n",
    "logistic_regression_gradient_descent_demo(x_no_nan, y_train, x_no_nan_val, y_validation, degrees=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, x_test, ids_test, header = helper.load_csv_data(DATA_TEST)\n",
    "x_test[x_test == -999] = np.nan\n",
    "\n",
    "x_no_nan_test = x_test.copy()\n",
    "x_no_nan_test = (x_no_nan_test - np.nanmean(x_no_nan_test, axis=0))/np.nanstd(x_no_nan_test, axis=0)\n",
    "x_no_nan_test = np.nan_to_num(x_no_nan_test)\n",
    "print('\\nStd:', np.std(x_no_nan_test, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_opt = degree_no_nan\n",
    "weights_opt = weights_no_nan\n",
    "\n",
    "_phi_test = lib.build_poly(x_no_nan_test, degree_opt)\n",
    "y_pred = helper.predict_labels(weights_opt, _phi_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.create_csv_submission(ids_test, y_pred, 'ridge_no_nan1.csv')\n",
    "print('Results saved ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
