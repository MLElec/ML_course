{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,)\n"
     ]
    }
   ],
   "source": [
    "##Ridge regression \n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scripts.implementations as lib  # Add personal library\n",
    "import scripts.proj1_helpers as helper  # Add personal library\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DATA_TRAIN = os.path.join(DATA_FOLDER, 'train.csv')\n",
    "DATA_TEST = os.path.join(DATA_FOLDER, 'test.csv')\n",
    "\n",
    "y, x, ids, header = helper.load_csv_data(DATA_TRAIN)\n",
    "y_train, x_train,  y_validation, x_validation = lib.sep_valid_train_data(x,y, 0.8);\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[x_train == -999] = np.nan\n",
    "x_validation[x_validation == -999] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 - DER_mass_MMC has range: [9.0440, 1192.0260]\n",
      "Feature 2 - DER_mass_transverse_met_lep has range: [0.0000, 595.8190]\n",
      "Feature 3 - DER_mass_vis has range: [6.4620, 1329.9130]\n",
      "Feature 4 - DER_pt_h has range: [0.0000, 1053.8070]\n",
      "Feature 5 - DER_deltaeta_jet_jet has range: [0.0000, 8.5030]\n",
      "Feature 6 - DER_mass_jet_jet has range: [13.6020, 4974.9790]\n",
      "Feature 7 - DER_prodeta_jet_jet has range: [-18.0660, 16.6900]\n",
      "Feature 8 - DER_deltar_tau_lep has range: [0.2080, 5.6840]\n",
      "Feature 9 - DER_pt_tot has range: [0.0000, 513.6590]\n",
      "Feature 10 - DER_sum_pt has range: [46.1040, 1852.4620]\n",
      "Feature 11 - DER_pt_ratio_lep_tau has range: [0.0470, 19.7730]\n",
      "Feature 12 - DER_met_phi_centrality has range: [-1.4140, 1.4140]\n",
      "Feature 13 - DER_lep_eta_centrality has range: [0.0000, 1.0000]\n",
      "Feature 14 - PRI_tau_pt has range: [20.0000, 622.8620]\n",
      "Feature 15 - PRI_tau_eta has range: [-2.4990, 2.4970]\n",
      "Feature 16 - PRI_tau_phi has range: [-3.1420, 3.1420]\n",
      "Feature 17 - PRI_lep_pt has range: [26.0000, 461.8960]\n",
      "Feature 18 - PRI_lep_eta has range: [-2.5050, 2.5020]\n",
      "Feature 19 - PRI_lep_phi has range: [-3.1420, 3.1420]\n",
      "Feature 20 - PRI_met has range: [0.1090, 951.3630]\n",
      "Feature 21 - PRI_met_phi has range: [-3.1420, 3.1420]\n",
      "Feature 22 - PRI_met_sumet has range: [13.6780, 2003.9760]\n",
      "Feature 23 - PRI_jet_num has range: [0.0000, 3.0000]\n",
      "Feature 24 - PRI_jet_leading_pt has range: [30.0000, 1120.5730]\n",
      "Feature 25 - PRI_jet_leading_eta has range: [-4.4990, 4.4990]\n",
      "Feature 26 - PRI_jet_leading_phi has range: [-3.1420, 3.1410]\n",
      "Feature 27 - PRI_jet_subleading_pt has range: [30.0010, 721.4560]\n",
      "Feature 28 - PRI_jet_subleading_eta has range: [-4.5000, 4.5000]\n",
      "Feature 29 - PRI_jet_subleading_phi has range: [-3.1420, 3.1420]\n",
      "Feature 30 - PRI_jet_all_pt has range: [0.0000, 1633.4330]\n"
     ]
    }
   ],
   "source": [
    "for i, feature in enumerate(x_train.T):\n",
    "    print('Feature {} - {} has range: [{:.4f}, {:.4f}]'.format(\n",
    "        i+1, header[i], np.nanmin(feature), np.nanmax(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,60))\n",
    "\n",
    "for i, feature in enumerate(x_train.T):\n",
    "    plt.subplot(15, 2, i+1)\n",
    "    id_keep = ~np.isnan(feature)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.hist(feature[id_keep], bins=100, alpha=0.4, label='total')\n",
    "    plt.hist(feature[id_b], alpha=0.4, bins=100, label='back')\n",
    "    plt.hist(feature[id_s], alpha=0.4, bins=100, label='signal')\n",
    "    plt.title('Feature {} - {}'.format(i+1, header[i]))\n",
    "    plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,24))\n",
    "\n",
    "for i, feature in enumerate(x_train.T):\n",
    "    plt.subplot(10, 3, i+1)\n",
    "    id_keep = ~np.isnan(feature)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.boxplot([feature[id_keep], feature[id_b], feature[id_s]], whis=2.5, \n",
    "                vert=False, labels=['total', 'back', 'signal'])\n",
    "    plt.title('Feature {} - {}'.format(i+1, header[i]))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.ml import augmented_feat_angle\n",
    "\n",
    "id_angle_feat = np.array([15, 18, 20, 25, 28])\n",
    "id_left = [ i for i in range(x_train.shape[1]) if i not in id_angle_feat]\n",
    "\n",
    "# Augment features\n",
    "x_aug_cos, x_aug_sin = augmented_feat_angle(x_train, id_angle_feat)\n",
    "#x_aug[:, 0] = recenter_feature(x_aug[:, 0])\n",
    "#x_aug[:, 1] = recenter_feature(x_aug[:, 1])\n",
    "#x_aug[:, 2] = recenter_feature(x_aug[:, 2])\n",
    "#x_aug[:, 3] = recenter_feature(x_aug[:, 3])\n",
    "#x_aug[:, 4] = recenter_feature(x_aug[:, 4])\n",
    "#x_aug[:, 5] = recenter_feature(x_aug[:, 5])\n",
    "#x_aug[:, 6] = recenter_feature(x_aug[:, 6])\n",
    "#x_aug[:, 7] = recenter_feature(x_aug[:, 7])\n",
    "#x_aug[:, 8] = recenter_feature(x_aug[:, 8])\n",
    "#x_aug[:, 9] = recenter_feature(x_aug[:, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,20))\n",
    "\n",
    "for i, (f1, f2) in enumerate(zip(x_aug_cos.T, x_aug_sin.T)):\n",
    "    ft = f1*f2\n",
    "    plt.subplot(5, 2, 2*(i+1)-1)\n",
    "    id_keep = ~np.isnan(ft)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.hist(ft[id_keep], bins=100, alpha=0.4, label='total')\n",
    "    plt.hist(ft[id_b], alpha=0.4, bins=100, label='back')\n",
    "    plt.hist(ft[id_s], alpha=0.4, bins=100, label='signal')\n",
    "    plt.legend()\n",
    "    # plt.title('Feature {} - {}'.format(int(i/2)+1, header[id_angle_feat[int(i/2)]]))\n",
    "    plt.subplot(5, 2, 2*(i+1))\n",
    "    id_keep = ~np.isnan(f2)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.hist(f2[id_keep], bins=100, alpha=0.4, label='total')\n",
    "    plt.hist(f2[id_b], alpha=0.4, bins=100, label='back')\n",
    "    plt.hist(f2[id_s], alpha=0.4, bins=100, label='signal')\n",
    "    plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "for i, feature in enumerate(x_aug.T):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    id_keep = ~np.isnan(feature)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.boxplot([feature[id_keep], feature[id_b], feature[id_s]], whis=2.5, \n",
    "                vert=False, labels=['total', 'back', 'signal'])\n",
    "    plt.title('Feature {} - {}'.format(int(i/2)+1, header[id_angle_feat[int(i/2)]]))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"ridge_regression\")\n",
    "    \n",
    "def test_ridge_regression(x, y, x_val, y_val, degrees, lambdas):\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_degree = 0\n",
    "    best_lambda = 0\n",
    "    best_rmse_tr = []\n",
    "    best_rmse_te = []\n",
    "    best_weights = []\n",
    "    for degree in degrees:\n",
    "        degree = int(degree)\n",
    "        #lambdas = np.logspace(-7, 2, 20)\n",
    "\n",
    "        # Split sets\n",
    "        #x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "        # Get ploynomial\n",
    "        phi_train = lib.build_poly(x, degree)\n",
    "        phi_test = lib.build_poly(x_val, degree)\n",
    "\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        update_rmse = False\n",
    "\n",
    "        for ind, lambda_ in enumerate(lambdas):\n",
    "\n",
    "            mse_tr, weights = lib.ridge_regression(y, phi_train, lambda_)\n",
    "            mse_te = lib.compute_loss(y_val, phi_test.dot(weights))\n",
    "            rmse_tr.append(np.sqrt(2*mse_tr))\n",
    "            rmse_te.append(np.sqrt(2*mse_te))\n",
    "\n",
    "            print(\"degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "                    d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "            print('train acc : ', lib.accuracy(y, phi_train.dot(weights)))\n",
    "            val_acc = lib.accuracy(y_val, phi_test.dot(weights))\n",
    "            print('validation acc : ', val_acc)\n",
    "\n",
    "            if(val_acc > best_acc):\n",
    "                best_acc = val_acc\n",
    "                best_degree = degree\n",
    "                best_lambda = lambda_\n",
    "                best_weights = weights\n",
    "                update_rmse = True\n",
    "        \n",
    "        if(update_rmse):\n",
    "            best_rmse_tr = rmse_tr\n",
    "            best_rmse_te = rmse_te\n",
    "\n",
    "        # Plot the best obtained results\n",
    "    plot_train_test(best_rmse_tr, best_rmse_te, lambdas, best_degree)\n",
    "\n",
    "    print('Best params for Ridge regression : degree = ',best_degree, ', lambda = ',best_lambda,', accuracy = ', best_acc)\n",
    "    \n",
    "    return best_weights, best_degree, best_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge with no_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distrib = [ 'p', 'i', 'p', 'p', 'i', 'p', 'g', 'g', 'p', 'p',\n",
    "#            'p', 'u', 'u', 'i', 'g', 'f', 'i', 'g', 'f', 'p',\n",
    "#            'f', 'p', 'd', 'i', 'g', 'f', 'i', 'g', 'f', 'p']\n",
    "\n",
    "def norm_poisson(feature, perc_threshold=0.01):\n",
    "    length = feature.shape[0];\n",
    "    idx_val = np.int(np.ceil(length*perc_threshold))\n",
    "    maxval = np.sort(feature)[-idx_val]\n",
    "    \n",
    "    #idx_outliers = np.argsort(feature)[-idx_val:]\n",
    "    #maxval = feature[np.argsort(feature)[ -(idx_val+1) ]]\n",
    "    \n",
    "    mean = np.nanmean(feature[feature < maxval])\n",
    "    std = np.nanstd(feature[feature < maxval])\n",
    "    feature[feature > maxval] = maxval\n",
    "    feature -= mean\n",
    "    feature /= std\n",
    "    return feature, mean, std, maxval\n",
    "\n",
    "def norm_poisson_feed(feature, mean_ref, std_ref, maxval):    \n",
    "    feature[feature > maxval] = maxval\n",
    "    feature -= mean_ref\n",
    "    feature /= std_ref\n",
    "    return feature\n",
    "\n",
    "def norm_gaussian(feature, n_std=2.5):\n",
    "    \n",
    "    feat_cent = feature-np.nanmean(feature)\n",
    "    std_thresh = np.nanstd(feat_cent, axis=0)\n",
    "    maxval = n_std*std_thresh\n",
    "    \n",
    "    mean_update = np.nanmean(feature[np.abs(feat_cent) < maxval])\n",
    "    std_update = np.nanstd(feature[np.abs(feat_cent) < maxval])\n",
    "    feat_final = feature-mean_update\n",
    "    feat_final[feat_final > maxval] = maxval\n",
    "    feat_final[feat_final < -maxval] = -maxval\n",
    "    feat_final /= std_update\n",
    "    \n",
    "    #feature[feature > maxval] = maxval\n",
    "    #feature[feature < -maxval] = -maxval\n",
    "    \n",
    "    #mean_update = np.nanmean(feature)\n",
    "    #std_update = np.nanstd(feature)\n",
    "    #feature = (feature-mean_update)/std_update\n",
    "        \n",
    "    #return feature, mean_update, std_update, maxval\n",
    "\n",
    "    return feat_final, mean_update, std_update, maxval\n",
    "\n",
    "def norm_gaussian_feed(feature, mean_ref, std_ref, maxval):    \n",
    "    feat_final = feature - mean_ref\n",
    "    feat_final[feat_final > maxval]  = maxval\n",
    "    feat_final[feat_final < -maxval] = -maxval\n",
    "    feat_final /= std_ref\n",
    "    return feat_final\n",
    "\n",
    "def normalize_outliers(x_in, dist_type):\n",
    "    # 1. Substract mean\n",
    "    # 2. Compute std and detect ouliers\n",
    "    # 3. Compute std and mean witout ouliers\n",
    "    mean_corr = []\n",
    "    std_corr = []\n",
    "    max_val_corr = []\n",
    "                \n",
    "    for i, feat in enumerate(x_in.T):\n",
    "        # Normalize according to distribution\n",
    "        if dist_type[i] == 'g' or dist_type[i] == 'i' or dist_type[i] == 'u' \\\n",
    "                or dist_type[i] == 'f' or dist_type[i] == 'd':\n",
    "            feat_new, mean_new, std_new, max_val_new = norm_gaussian(feat)\n",
    "        elif dist_type[i] == 'p':\n",
    "            feat_new, mean_new, std_new, max_val_new = norm_poisson(feat)\n",
    "        else:\n",
    "            feat_new, mean_new, std_new, max_val_new = (feat, 0, 1, np.inf)\n",
    "        # Affect new values\n",
    "        mean_corr.append(mean_new)\n",
    "        std_corr.append(std_new)\n",
    "        max_val_corr.append(max_val_new)\n",
    "        x_in[:, i] = feat_new\n",
    "    return x_in, mean_corr, std_corr, max_val_corr\n",
    "\n",
    "def normalize_outliers_feed(x_in, mean_ref, std_ref, max_ref, dist_type):\n",
    "    # 1. Substract mean\n",
    "    # 2. Compute std and detect ouliers\n",
    "    # 3. Compute std and mean witout ouliers\n",
    "    \n",
    "    for i, feat in enumerate(x_in.T):\n",
    "        # Normalize according to distribution\n",
    "        if dist_type[i] == 'g' or dist_type[i] == 'i' or dist_type[i] == 'u' \\\n",
    "                or dist_type[i] == 'f' or dist_type[i] == 'd':\n",
    "            feat_new = norm_gaussian_feed(feat, mean_ref[i], std_ref[i], max_ref[i])\n",
    "        elif dist_type[i] == 'p':\n",
    "            feat_new = norm_poisson_feed(feat, mean_ref[i], std_ref[i], max_ref[i])\n",
    "        else:\n",
    "            feat_new = feat\n",
    "        # Affect new value\n",
    "        x_in[:, i] = feat_new\n",
    "    return x_in\n",
    "\n",
    "def add_feature(x_in, id_feat1, id_feat2):\n",
    "    new_feat = np.expand_dims(x_in[:, id_feat1]*x_in[:, id_feat2], axis=1)\n",
    "    return np.concatenate((x_in, new_feat), axis=1)\n",
    "\n",
    "def add_features(x_in, id_feats):\n",
    "    for id_feat in id_feats:\n",
    "        x_in = add_feature(x_in, id_feat[0], id_feat[1])\n",
    "    return x_in\n",
    "\n",
    "### UNUSED \n",
    "def remove_useless(x_in, id_useless=[15, 18, 20, 25, 28]):\n",
    "    id_left = [ i for i in range(x_train.shape[1]) if i not in id_useless]\n",
    "    return x_in[:, id_left]\n",
    "\n",
    "def recenter_feature(feature):\n",
    "    val_max = np.nanmax(feature)\n",
    "    val_min = np.nanmin(feature)\n",
    "    feature[feature < np.nanmean(feature)] += (val_max - val_min)\n",
    "    return feature\n",
    "### UNUSED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:32: RuntimeWarning: invalid value encountered in less\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:33: RuntimeWarning: invalid value encountered in less\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:35: RuntimeWarning: invalid value encountered in greater\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:36: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Std: [ 1.074   1.0408  1.1532  1.1364  0.5519  0.6304  0.5907  1.0192  1.1146\n",
      "  1.1341  1.1064  1.      0.5399  1.1454  1.      1.      1.1456  1.      1.\n",
      "  1.1392  1.      1.1057  1.      0.89    0.7758  0.7753  0.6201  0.5399\n",
      "  0.5399  1.1287  1.0661  1.0565  0.587   0.5589  0.5923  0.9597  1.0436\n",
      "  1.0003  1.0689  1.0475  1.0366  1.0791  1.0413  1.0823  0.5444  0.5653\n",
      "  0.5493  0.5796  0.5932  0.572   0.5399  0.5878] \n",
      "n_feat 52\n",
      "\n",
      "Std: [ 1.0809  1.0384  1.1548  1.134   0.5455  0.6189  0.5829  1.0176  1.106\n",
      "  1.1246  1.1044  1.0038  0.5337  1.1372  1.0017  1.0002  1.1371  1.0049\n",
      "  1.0041  1.1418  0.9975  1.103   0.9936  0.8915  0.7739  0.7704  0.6114\n",
      "  0.5313  0.5326  1.1198  1.0659  1.0561  0.5769  0.5531  0.5797  0.9645\n",
      "  1.0496  1.0066  1.0647  1.0471  1.0368  1.0809  1.0385  1.0742  0.5403\n",
      "  0.557   0.5396  0.575   0.5846  0.5661  0.5329  0.5813] \n",
      "n_feat 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:52: RuntimeWarning: invalid value encountered in greater\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:53: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "distrib = ['g']*36\n",
    "\n",
    "new_feats = [[ 1,  7], [ 3,  7], [ 4,  7], [ 4, 22], [ 4, 26], [11,  0], [11,  2], \n",
    "             [11,  7], [11,  9], [11, 13], [11, 16], [11, 19], [11, 21], [11, 22],\n",
    "             [12,  0], [12,  2], [12,  7], [12,  9], [12, 16], [12, 21], [12, 22],\n",
    "             [12, 26]]\n",
    "\n",
    "distrib = ['g']*(30+np.shape(new_feats)[0])\n",
    "#new_feats = [[3, 7], [11, 2], [11, 7], [11, 13], [11, 19], [12, 0]]\n",
    "\n",
    "x_no_nan = x_train.copy()\n",
    "x_no_nan = add_features(x_no_nan, new_feats)\n",
    "x_no_nan, mean_train, std_train, max_train = normalize_outliers(x_no_nan, distrib)\n",
    "#x_no_nan = remove_useless(x_no_nan)\n",
    "x_no_nan = np.nan_to_num(x_no_nan)\n",
    "print('\\nStd:', np.std(x_no_nan, axis=0), '\\nn_feat', x_no_nan.shape[1])\n",
    "\n",
    "# normalize features\n",
    "x_no_nan_val = x_validation.copy()\n",
    "x_no_nan_val = add_features(x_no_nan_val, new_feats)\n",
    "x_no_nan_val = normalize_outliers_feed(x_no_nan_val, mean_train, std_train, max_train, distrib)\n",
    "#x_no_nan_val = remove_useless(x_no_nan_val)\n",
    "x_no_nan_val = np.nan_to_num(x_no_nan_val)\n",
    "print('\\nStd:', np.std(x_no_nan_val, axis=0), '\\nn_feat', x_no_nan_val.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=8, lambda=0.000, Training RMSE=0.711, Testing RMSE=0.711\n",
      "train acc :  0.8303\n",
      "validation acc :  0.8306\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.830555\n",
      "validation acc :  0.8306\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.830565\n",
      "validation acc :  0.83074\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.83056\n",
      "validation acc :  0.83076\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.830625\n",
      "validation acc :  0.83068\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.830605\n",
      "validation acc :  0.8307\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.83062\n",
      "validation acc :  0.83072\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.830605\n",
      "validation acc :  0.8307\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.83059\n",
      "validation acc :  0.83072\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.830585\n",
      "validation acc :  0.83074\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.83062\n",
      "validation acc :  0.83082\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.83064\n",
      "validation acc :  0.83092\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.83065\n",
      "validation acc :  0.8308\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.830645\n",
      "validation acc :  0.83076\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.83063\n",
      "validation acc :  0.83082\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.830605\n",
      "validation acc :  0.83072\n",
      "degree=8, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.710\n",
      "train acc :  0.830515\n",
      "validation acc :  0.83086\n",
      "degree=8, lambda=0.000, Training RMSE=0.710, Testing RMSE=0.710\n",
      "train acc :  0.830135\n",
      "validation acc :  0.8312\n",
      "degree=8, lambda=0.000, Training RMSE=0.710, Testing RMSE=0.711\n",
      "train acc :  0.829735\n",
      "validation acc :  0.83062\n",
      "degree=8, lambda=0.000, Training RMSE=0.712, Testing RMSE=0.712\n",
      "train acc :  0.82923\n",
      "validation acc :  0.83002\n",
      "degree=8, lambda=0.000, Training RMSE=0.714, Testing RMSE=0.714\n",
      "train acc :  0.82853\n",
      "validation acc :  0.8292\n",
      "degree=8, lambda=0.000, Training RMSE=0.719, Testing RMSE=0.719\n",
      "train acc :  0.82716\n",
      "validation acc :  0.82688\n",
      "degree=8, lambda=0.000, Training RMSE=0.726, Testing RMSE=0.725\n",
      "train acc :  0.824545\n",
      "validation acc :  0.82338\n",
      "degree=8, lambda=0.001, Training RMSE=0.735, Testing RMSE=0.734\n",
      "train acc :  0.820395\n",
      "validation acc :  0.81966\n",
      "degree=8, lambda=0.001, Training RMSE=0.746, Testing RMSE=0.746\n",
      "train acc :  0.81421\n",
      "validation acc :  0.8139\n",
      "degree=8, lambda=0.001, Training RMSE=0.763, Testing RMSE=0.762\n",
      "train acc :  0.80351\n",
      "validation acc :  0.80392\n",
      "degree=8, lambda=0.002, Training RMSE=0.785, Testing RMSE=0.783\n",
      "train acc :  0.787915\n",
      "validation acc :  0.78914\n",
      "degree=8, lambda=0.004, Training RMSE=0.809, Testing RMSE=0.807\n",
      "train acc :  0.77295\n",
      "validation acc :  0.77652\n",
      "degree=8, lambda=0.006, Training RMSE=0.831, Testing RMSE=0.828\n",
      "train acc :  0.76164\n",
      "validation acc :  0.76584\n",
      "degree=8, lambda=0.010, Training RMSE=0.850, Testing RMSE=0.847\n",
      "train acc :  0.75281\n",
      "validation acc :  0.75778\n",
      "degree=9, lambda=0.000, Training RMSE=0.713, Testing RMSE=0.713\n",
      "train acc :  0.82875\n",
      "validation acc :  0.82918\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83084\n",
      "validation acc :  0.83088\n",
      "degree=9, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.710\n",
      "train acc :  0.830595\n",
      "validation acc :  0.83078\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83077\n",
      "validation acc :  0.83084\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83072\n",
      "validation acc :  0.8306\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83074\n",
      "validation acc :  0.8308\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.830705\n",
      "validation acc :  0.83078\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83069\n",
      "validation acc :  0.83078\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.830685\n",
      "validation acc :  0.83078\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.8307\n",
      "validation acc :  0.8307\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83073\n",
      "validation acc :  0.83072\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.830725\n",
      "validation acc :  0.83092\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83078\n",
      "validation acc :  0.8309\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83075\n",
      "validation acc :  0.83092\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83063\n",
      "validation acc :  0.8307\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83052\n",
      "validation acc :  0.83062\n",
      "degree=9, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83026\n",
      "validation acc :  0.83078\n",
      "degree=9, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.830095\n",
      "validation acc :  0.83062\n",
      "degree=9, lambda=0.000, Training RMSE=0.710, Testing RMSE=0.710\n",
      "train acc :  0.82995\n",
      "validation acc :  0.83068\n",
      "degree=9, lambda=0.000, Training RMSE=0.711, Testing RMSE=0.711\n",
      "train acc :  0.82946\n",
      "validation acc :  0.83024\n",
      "degree=9, lambda=0.000, Training RMSE=0.714, Testing RMSE=0.714\n",
      "train acc :  0.828455\n",
      "validation acc :  0.82968\n",
      "degree=9, lambda=0.000, Training RMSE=0.718, Testing RMSE=0.718\n",
      "train acc :  0.827115\n",
      "validation acc :  0.82716\n",
      "degree=9, lambda=0.000, Training RMSE=0.724, Testing RMSE=0.724\n",
      "train acc :  0.825105\n",
      "validation acc :  0.82474\n",
      "degree=9, lambda=0.001, Training RMSE=0.732, Testing RMSE=0.732\n",
      "train acc :  0.821155\n",
      "validation acc :  0.82026\n",
      "degree=9, lambda=0.001, Training RMSE=0.744, Testing RMSE=0.744\n",
      "train acc :  0.814935\n",
      "validation acc :  0.81442\n",
      "degree=9, lambda=0.001, Training RMSE=0.762, Testing RMSE=0.761\n",
      "train acc :  0.80349\n",
      "validation acc :  0.80378\n",
      "degree=9, lambda=0.002, Training RMSE=0.784, Testing RMSE=0.782\n",
      "train acc :  0.78802\n",
      "validation acc :  0.78922\n",
      "degree=9, lambda=0.004, Training RMSE=0.804, Testing RMSE=0.803\n",
      "train acc :  0.77501\n",
      "validation acc :  0.77776\n",
      "degree=9, lambda=0.006, Training RMSE=0.823, Testing RMSE=0.821\n",
      "train acc :  0.76477\n",
      "validation acc :  0.7688\n",
      "degree=9, lambda=0.010, Training RMSE=0.841, Testing RMSE=0.839\n",
      "train acc :  0.75646\n",
      "validation acc :  0.76066\n",
      "degree=10, lambda=0.000, Training RMSE=1.083, Testing RMSE=1.085\n",
      "train acc :  0.676445\n",
      "validation acc :  0.67622\n",
      "degree=10, lambda=0.000, Training RMSE=0.797, Testing RMSE=0.798\n",
      "train acc :  0.78787\n",
      "validation acc :  0.78688\n",
      "degree=10, lambda=0.000, Training RMSE=0.710, Testing RMSE=0.712\n",
      "train acc :  0.82918\n",
      "validation acc :  0.8291\n",
      "degree=10, lambda=0.000, Training RMSE=0.719, Testing RMSE=0.719\n",
      "train acc :  0.825085\n",
      "validation acc :  0.82618\n",
      "degree=10, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.708\n",
      "train acc :  0.830565\n",
      "validation acc :  0.83086\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.830815\n",
      "validation acc :  0.83106\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.83081\n",
      "validation acc :  0.83052\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.830805\n",
      "validation acc :  0.8307\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.830895\n",
      "validation acc :  0.83074\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.83091\n",
      "validation acc :  0.8308\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.830875\n",
      "validation acc :  0.83084\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.830915\n",
      "validation acc :  0.83086\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.830965\n",
      "validation acc :  0.83072\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.83094\n",
      "validation acc :  0.83086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.830915\n",
      "validation acc :  0.83074\n",
      "degree=10, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.83093\n",
      "validation acc :  0.83072\n",
      "degree=10, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.708\n",
      "train acc :  0.830645\n",
      "validation acc :  0.83092\n",
      "degree=10, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.830275\n",
      "validation acc :  0.8312\n",
      "degree=10, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.710\n",
      "train acc :  0.82997\n",
      "validation acc :  0.83116\n",
      "degree=10, lambda=0.000, Training RMSE=0.711, Testing RMSE=0.711\n",
      "train acc :  0.829135\n",
      "validation acc :  0.82992\n",
      "degree=10, lambda=0.000, Training RMSE=0.714, Testing RMSE=0.714\n",
      "train acc :  0.82847\n",
      "validation acc :  0.82932\n",
      "degree=10, lambda=0.000, Training RMSE=0.717, Testing RMSE=0.717\n",
      "train acc :  0.82734\n",
      "validation acc :  0.8279\n",
      "degree=10, lambda=0.000, Training RMSE=0.723, Testing RMSE=0.722\n",
      "train acc :  0.82551\n",
      "validation acc :  0.82478\n",
      "degree=10, lambda=0.001, Training RMSE=0.731, Testing RMSE=0.731\n",
      "train acc :  0.82187\n",
      "validation acc :  0.82134\n",
      "degree=10, lambda=0.001, Training RMSE=0.744, Testing RMSE=0.743\n",
      "train acc :  0.81497\n",
      "validation acc :  0.81444\n",
      "degree=10, lambda=0.001, Training RMSE=0.761, Testing RMSE=0.761\n",
      "train acc :  0.803935\n",
      "validation acc :  0.80286\n",
      "degree=10, lambda=0.002, Training RMSE=0.780, Testing RMSE=0.779\n",
      "train acc :  0.79122\n",
      "validation acc :  0.79158\n",
      "degree=10, lambda=0.004, Training RMSE=0.800, Testing RMSE=0.798\n",
      "train acc :  0.77901\n",
      "validation acc :  0.78146\n",
      "degree=10, lambda=0.006, Training RMSE=0.819, Testing RMSE=0.817\n",
      "train acc :  0.767485\n",
      "validation acc :  0.77196\n",
      "degree=10, lambda=0.010, Training RMSE=0.838, Testing RMSE=0.836\n",
      "train acc :  0.758365\n",
      "validation acc :  0.76286\n",
      "degree=12, lambda=0.000, Training RMSE=3.108, Testing RMSE=3.121\n",
      "train acc :  0.56891\n",
      "validation acc :  0.56938\n",
      "degree=12, lambda=0.000, Training RMSE=5.108, Testing RMSE=5.095\n",
      "train acc :  0.52751\n",
      "validation acc :  0.52806\n",
      "degree=12, lambda=0.000, Training RMSE=6.270, Testing RMSE=6.223\n",
      "train acc :  0.528775\n",
      "validation acc :  0.52624\n",
      "degree=12, lambda=0.000, Training RMSE=1.477, Testing RMSE=1.477\n",
      "train acc :  0.65009\n",
      "validation acc :  0.64784\n",
      "degree=12, lambda=0.000, Training RMSE=2.001, Testing RMSE=1.996\n",
      "train acc :  0.59415\n",
      "validation acc :  0.59444\n",
      "degree=12, lambda=0.000, Training RMSE=0.847, Testing RMSE=0.846\n",
      "train acc :  0.77174\n",
      "validation acc :  0.77178\n",
      "degree=12, lambda=0.000, Training RMSE=0.771, Testing RMSE=0.771\n",
      "train acc :  0.804175\n",
      "validation acc :  0.80356\n",
      "degree=12, lambda=0.000, Training RMSE=0.779, Testing RMSE=0.781\n",
      "train acc :  0.80367\n",
      "validation acc :  0.80212\n",
      "degree=12, lambda=0.000, Training RMSE=0.762, Testing RMSE=0.762\n",
      "train acc :  0.81021\n",
      "validation acc :  0.81002\n",
      "degree=12, lambda=0.000, Training RMSE=0.714, Testing RMSE=0.715\n",
      "train acc :  0.828545\n",
      "validation acc :  0.82756\n",
      "degree=12, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.830585\n",
      "validation acc :  0.83048\n",
      "degree=12, lambda=0.000, Training RMSE=0.706, Testing RMSE=0.707\n",
      "train acc :  0.830665\n",
      "validation acc :  0.83134\n",
      "degree=12, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.83084\n",
      "validation acc :  0.83144\n",
      "degree=12, lambda=0.000, Training RMSE=0.706, Testing RMSE=0.708\n",
      "train acc :  0.831125\n",
      "validation acc :  0.831\n",
      "degree=12, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.83117\n",
      "validation acc :  0.83116\n",
      "degree=12, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.831005\n",
      "validation acc :  0.8311\n",
      "degree=12, lambda=0.000, Training RMSE=0.707, Testing RMSE=0.708\n",
      "train acc :  0.83071\n",
      "validation acc :  0.83086\n",
      "degree=12, lambda=0.000, Training RMSE=0.708, Testing RMSE=0.709\n",
      "train acc :  0.83042\n",
      "validation acc :  0.8312\n",
      "degree=12, lambda=0.000, Training RMSE=0.709, Testing RMSE=0.709\n",
      "train acc :  0.829745\n",
      "validation acc :  0.83094\n",
      "degree=12, lambda=0.000, Training RMSE=0.710, Testing RMSE=0.711\n",
      "train acc :  0.829285\n",
      "validation acc :  0.83046\n",
      "degree=12, lambda=0.000, Training RMSE=0.713, Testing RMSE=0.713\n",
      "train acc :  0.828665\n",
      "validation acc :  0.82998\n",
      "degree=12, lambda=0.000, Training RMSE=0.716, Testing RMSE=0.716\n",
      "train acc :  0.82761\n",
      "validation acc :  0.8287\n",
      "degree=12, lambda=0.000, Training RMSE=0.722, Testing RMSE=0.721\n",
      "train acc :  0.82539\n",
      "validation acc :  0.82536\n",
      "degree=12, lambda=0.001, Training RMSE=0.730, Testing RMSE=0.730\n",
      "train acc :  0.821415\n",
      "validation acc :  0.82132\n",
      "degree=12, lambda=0.001, Training RMSE=0.741, Testing RMSE=0.741\n",
      "train acc :  0.8163\n",
      "validation acc :  0.81562\n",
      "degree=12, lambda=0.001, Training RMSE=0.756, Testing RMSE=0.756\n",
      "train acc :  0.807535\n",
      "validation acc :  0.8077\n",
      "degree=12, lambda=0.002, Training RMSE=0.774, Testing RMSE=0.773\n",
      "train acc :  0.794995\n",
      "validation acc :  0.79546\n",
      "degree=12, lambda=0.004, Training RMSE=0.794, Testing RMSE=0.793\n",
      "train acc :  0.78089\n",
      "validation acc :  0.78286\n",
      "degree=12, lambda=0.006, Training RMSE=0.814, Testing RMSE=0.812\n",
      "train acc :  0.769625\n",
      "validation acc :  0.77338\n",
      "degree=12, lambda=0.010, Training RMSE=0.831, Testing RMSE=0.829\n",
      "train acc :  0.76124\n",
      "validation acc :  0.76686\n",
      "Best params for Ridge regression : degree =  12 , lambda =  3.03919538231e-06 , accuracy =  0.83144\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEaCAYAAAASSuyNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBb4xxqQIC3xjjEkR/x9fyr0H1x/TRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4c5916c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_no_nan, degree_no_nan, lambda_no_nan = test_ridge_regression(\n",
    "    x_no_nan, y_train, x_no_nan_val, y_validation, degrees = np.linspace(8,12,4), lambdas=np.logspace(-8,-2, 30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:52: RuntimeWarning: invalid value encountered in greater\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:53: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Std: [ 1.0739  1.0389  1.149   1.1334  0.5516  0.6303  0.5892  1.0158  1.1171\n",
      "  1.1355  1.107   1.0015  0.5385  1.1449  0.9997  0.9995  1.1455  1.0002\n",
      "  0.9996  1.1345  0.9999  1.1064  1.0007  0.889   0.7718  0.7751  0.6213\n",
      "  0.5391  0.539   1.1289  1.0629  1.0554  0.5883  0.5594  0.5913  0.9623\n",
      "  1.0451  1.0026  1.0705  1.0481  1.0386  1.0774  1.0425  1.0828  0.5451\n",
      "  0.566   0.5472  0.5797  0.5941  0.5709  0.5386  0.5874] \n",
      "n_feat 52\n"
     ]
    }
   ],
   "source": [
    "y_test, x_test, ids_test, header = helper.load_csv_data(DATA_TEST)\n",
    "x_test[x_test == -999] = np.nan\n",
    "\n",
    "x_no_nan_test = x_test.copy()\n",
    "x_no_nan_test = add_features(x_no_nan_test, new_feats)\n",
    "x_no_nan_test = normalize_outliers_feed(x_no_nan_test, mean_train, std_train, max_train, distrib)\n",
    "x_no_nan_test = np.nan_to_num(x_no_nan_test)\n",
    "print('\\nStd:', np.std(x_no_nan_test, axis=0),'\\nn_feat', x_no_nan_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_opt = degree_no_nan\n",
    "weights_opt = weights_no_nan\n",
    "\n",
    "_phi_test = lib.build_poly(x_no_nan_test, degree_opt)\n",
    "y_pred = helper.predict_labels(weights_opt, _phi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved ...\n"
     ]
    }
   ],
   "source": [
    "helper.create_csv_submission(ids_test, y_pred, 'ridge_no_nan1.csv')\n",
    "print('Results saved ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
