{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,)\n"
     ]
    }
   ],
   "source": [
    "##Ridge regression \n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scripts.implementations as lib  # Add personal library\n",
    "import scripts.proj1_helpers as helper  # Add personal library\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "DATA_FOLDER = 'data'\n",
    "DATA_TRAIN = os.path.join(DATA_FOLDER, 'train.csv')\n",
    "DATA_TEST = os.path.join(DATA_FOLDER, 'test.csv')\n",
    "\n",
    "y, x, ids, header = helper.load_csv_data(DATA_TRAIN)\n",
    "y_train, x_train,  y_validation, x_validation = lib.sep_valid_train_data(x,y, 0.8);\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[x_train == -999] = np.nan\n",
    "x_validation[x_validation == -999] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 - DER_mass_MMC has range: [9.0440, 1192.0260]\n",
      "Feature 2 - DER_mass_transverse_met_lep has range: [0.0000, 595.8190]\n",
      "Feature 3 - DER_mass_vis has range: [6.4620, 1329.9130]\n",
      "Feature 4 - DER_pt_h has range: [0.0000, 1053.8070]\n",
      "Feature 5 - DER_deltaeta_jet_jet has range: [0.0000, 8.5030]\n",
      "Feature 6 - DER_mass_jet_jet has range: [13.6020, 4974.9790]\n",
      "Feature 7 - DER_prodeta_jet_jet has range: [-18.0660, 16.6900]\n",
      "Feature 8 - DER_deltar_tau_lep has range: [0.2080, 5.6840]\n",
      "Feature 9 - DER_pt_tot has range: [0.0000, 513.6590]\n",
      "Feature 10 - DER_sum_pt has range: [46.1040, 1852.4620]\n",
      "Feature 11 - DER_pt_ratio_lep_tau has range: [0.0470, 19.7730]\n",
      "Feature 12 - DER_met_phi_centrality has range: [-1.4140, 1.4140]\n",
      "Feature 13 - DER_lep_eta_centrality has range: [0.0000, 1.0000]\n",
      "Feature 14 - PRI_tau_pt has range: [20.0000, 622.8620]\n",
      "Feature 15 - PRI_tau_eta has range: [-2.4990, 2.4970]\n",
      "Feature 16 - PRI_tau_phi has range: [-3.1420, 3.1420]\n",
      "Feature 17 - PRI_lep_pt has range: [26.0000, 461.8960]\n",
      "Feature 18 - PRI_lep_eta has range: [-2.5050, 2.5020]\n",
      "Feature 19 - PRI_lep_phi has range: [-3.1420, 3.1420]\n",
      "Feature 20 - PRI_met has range: [0.1090, 951.3630]\n",
      "Feature 21 - PRI_met_phi has range: [-3.1420, 3.1420]\n",
      "Feature 22 - PRI_met_sumet has range: [13.6780, 2003.9760]\n",
      "Feature 23 - PRI_jet_num has range: [0.0000, 3.0000]\n",
      "Feature 24 - PRI_jet_leading_pt has range: [30.0000, 1120.5730]\n",
      "Feature 25 - PRI_jet_leading_eta has range: [-4.4990, 4.4990]\n",
      "Feature 26 - PRI_jet_leading_phi has range: [-3.1420, 3.1410]\n",
      "Feature 27 - PRI_jet_subleading_pt has range: [30.0010, 721.4560]\n",
      "Feature 28 - PRI_jet_subleading_eta has range: [-4.5000, 4.5000]\n",
      "Feature 29 - PRI_jet_subleading_phi has range: [-3.1420, 3.1420]\n",
      "Feature 30 - PRI_jet_all_pt has range: [0.0000, 1633.4330]\n"
     ]
    }
   ],
   "source": [
    "for i, feature in enumerate(x_train.T):\n",
    "    print('Feature {} - {} has range: [{:.4f}, {:.4f}]'.format(\n",
    "        i+1, header[i], np.nanmin(feature), np.nanmax(feature)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,60))\n",
    "\n",
    "for i, feature in enumerate(x_train.T):\n",
    "    plt.subplot(15, 2, i+1)\n",
    "    id_keep = ~np.isnan(feature)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.hist(feature[id_keep], bins=100, alpha=0.4, label='total')\n",
    "    plt.hist(feature[id_b], alpha=0.4, bins=100, label='back')\n",
    "    plt.hist(feature[id_s], alpha=0.4, bins=100, label='signal')\n",
    "    plt.title('Feature {} - {}'.format(i+1, header[i]))\n",
    "    plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,24))\n",
    "\n",
    "for i, feature in enumerate(x_train.T):\n",
    "    plt.subplot(10, 3, i+1)\n",
    "    id_keep = ~np.isnan(feature)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.boxplot([feature[id_keep], feature[id_b], feature[id_s]], whis=2.5, \n",
    "                vert=False, labels=['total', 'back', 'signal'])\n",
    "    plt.title('Feature {} - {}'.format(i+1, header[i]))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.ml import augmented_feat_angle\n",
    "\n",
    "id_angle_feat = np.array([15, 18, 20, 25, 28])\n",
    "id_left = [ i for i in range(x_train.shape[1]) if i not in id_angle_feat]\n",
    "\n",
    "# Augment features\n",
    "x_aug = augmented_feat_angle(x_train, id_angle_feat)\n",
    "x_aug[:, 0] = recenter_feature(x_aug[:, 0])\n",
    "x_aug[:, 1] = recenter_feature(x_aug[:, 1])\n",
    "x_aug[:, 2] = recenter_feature(x_aug[:, 2])\n",
    "x_aug[:, 3] = recenter_feature(x_aug[:, 3])\n",
    "x_aug[:, 4] = recenter_feature(x_aug[:, 4])\n",
    "x_aug[:, 5] = recenter_feature(x_aug[:, 5])\n",
    "x_aug[:, 6] = recenter_feature(x_aug[:, 6])\n",
    "x_aug[:, 7] = recenter_feature(x_aug[:, 7])\n",
    "x_aug[:, 8] = recenter_feature(x_aug[:, 8])\n",
    "x_aug[:, 9] = recenter_feature(x_aug[:, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,20))\n",
    "\n",
    "for i, feature in enumerate(x_aug.T):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    id_keep = ~np.isnan(feature)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.hist(feature[id_keep], bins=100, alpha=0.4, label='total')\n",
    "    plt.hist(feature[id_b], alpha=0.4, bins=100, label='back')\n",
    "    plt.hist(feature[id_s], alpha=0.4, bins=100, label='signal')\n",
    "    plt.title('Feature {} - {}'.format(int(i/2)+1, header[id_angle_feat[int(i/2)]]))\n",
    "    plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "for i, feature in enumerate(x_aug.T):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    id_keep = ~np.isnan(feature)\n",
    "    id_b = np.logical_and(y_train == -1, id_keep)\n",
    "    id_s = np.logical_and(y_train == 1, id_keep)\n",
    "    plt.boxplot([feature[id_keep], feature[id_b], feature[id_s]], whis=2.5, \n",
    "                vert=False, labels=['total', 'back', 'signal'])\n",
    "    plt.title('Feature {} - {}'.format(int(i/2)+1, header[id_angle_feat[int(i/2)]]))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test(train_errors, test_errors, lambdas, degree):\n",
    "    \"\"\"\n",
    "    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,\n",
    "    * lambda[0] = 1\n",
    "    * train_errors[0] = RMSE of a ridge regression on the train set\n",
    "    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set\n",
    "    \n",
    "    degree is just used for the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.semilogx(lambdas, train_errors, color='b', marker='*', label=\"Train error\")\n",
    "    plt.semilogx(lambdas, test_errors, color='r', marker='*', label=\"Test error\")\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Ridge regression for polynomial degree \" + str(degree))\n",
    "    leg = plt.legend(loc=1, shadow=True)\n",
    "    leg.draw_frame(False)\n",
    "    plt.savefig(\"ridge_regression\")\n",
    "    \n",
    "def test_ridge_regression(x, y, x_val, y_val, degrees, lambdas):\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_degree = 0\n",
    "    best_lambda = 0\n",
    "    best_rmse_tr = []\n",
    "    best_rmse_te = []\n",
    "    best_weights = []\n",
    "    for degree in degrees:\n",
    "        degree = int(degree)\n",
    "        #lambdas = np.logspace(-7, 2, 20)\n",
    "\n",
    "        # Split sets\n",
    "        #x_train, x_test, y_train, y_test = split_data(x, y, ratio, seed)\n",
    "\n",
    "        # Get ploynomial\n",
    "        phi_train = lib.build_poly(x, degree)\n",
    "        phi_test = lib.build_poly(x_val, degree)\n",
    "\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        update_rmse = False\n",
    "\n",
    "        for ind, lambda_ in enumerate(lambdas):\n",
    "\n",
    "            mse_tr, weights = lib.ridge_regression(y, phi_train, lambda_)\n",
    "            mse_te = lib.compute_loss(y_val, phi_test.dot(weights))\n",
    "            rmse_tr.append(np.sqrt(2*mse_tr))\n",
    "            rmse_te.append(np.sqrt(2*mse_te))\n",
    "\n",
    "            print(\"degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "                    d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "            print('train acc : ', lib.accuracy(y, phi_train.dot(weights)))\n",
    "            val_acc = lib.accuracy(y_val, phi_test.dot(weights))\n",
    "            print('validation acc : ', val_acc)\n",
    "\n",
    "            if(val_acc > best_acc):\n",
    "                best_acc = val_acc\n",
    "                best_degree = degree\n",
    "                best_lambda = lambda_\n",
    "                best_weights = weights\n",
    "                update_rmse = True\n",
    "        \n",
    "        if(update_rmse):\n",
    "            best_rmse_tr = rmse_tr\n",
    "            best_rmse_te = rmse_te\n",
    "\n",
    "        # Plot the best obtained results\n",
    "    plot_train_test(best_rmse_tr, best_rmse_te, lambdas, best_degree)\n",
    "\n",
    "    print('Best params for Ridge regression : degree = ',best_degree, ', lambda = ',best_lambda,', accuracy = ', best_acc)\n",
    "    \n",
    "    return best_weights, best_degree, best_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge with no_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distrib = [ 'p', 'i', 'p', 'p', 'i', 'p', 'g', 'g', 'p', 'p',\n",
    "#            'p', 'u', 'u', 'i', 'g', 'f', 'i', 'g', 'f', 'p',\n",
    "#            'f', 'p', 'd', 'i', 'g', 'f', 'i', 'g', 'f', 'p']\n",
    "\n",
    "distrib = ['g']*30\n",
    "\n",
    "def norm_poisson(feature, perc_threshold=0.01):\n",
    "    length = feature.shape[0];\n",
    "    idx_val = np.int(np.ceil(length*perc_threshold))\n",
    "    idx_outliers = np.argsort(feature)[-idx_val:]\n",
    "    maxval = feature[np.argsort(feature)[ -(idx_val+1) ]]\n",
    "    feature[idx_outliers] = maxval\n",
    "    mean = np.nanmean(feature)\n",
    "    std = np.nanstd(feature)\n",
    "    feature -= mean\n",
    "    feature /= std\n",
    "    return feature, mean, std, maxval\n",
    "\n",
    "def norm_poisson_feed(feature, mean_ref, std_ref, maxval):    \n",
    "    feature[feature > maxval] = maxval\n",
    "    feature -= mean_ref\n",
    "    feature /= std_ref\n",
    "    return feature\n",
    "\n",
    "def norm_gaussian(feature, n_std=2.5):\n",
    "    \n",
    "    feat_cent = feature-np.nanmean(feature)\n",
    "    std_thresh = np.nanstd(feat_cent, axis=0)\n",
    "    maxval = n_std*std_thresh\n",
    "    \n",
    "    mean_update = np.nanmean(feature[np.abs(feat_cent) < maxval])\n",
    "    std_update = np.nanstd(feature[np.abs(feat_cent) < maxval])\n",
    "    feat_final = feature-mean_update\n",
    "    feat_final[feat_final > maxval] = maxval\n",
    "    feat_final[feat_final < -maxval] = -maxval\n",
    "    feat_final /= std_update\n",
    "    \n",
    "    #feature[feature > maxval] = maxval\n",
    "    #feature[feature < -maxval] = -maxval\n",
    "    \n",
    "    #mean_update = np.nanmean(feature)\n",
    "    #std_update = np.nanstd(feature)\n",
    "    #feature = (feature-mean_update)/std_update\n",
    "        \n",
    "    #return feature, mean_update, std_update, maxval\n",
    "\n",
    "    return feat_final, mean_update, std_update, maxval\n",
    "\n",
    "def norm_gaussian_feed(feature, mean_ref, std_ref, maxval):    \n",
    "    feat_final = feature - mean_ref\n",
    "    feat_final[feat_final > maxval]  = maxval\n",
    "    feat_final[feat_final < -maxval] = -maxval\n",
    "    feat_final /= std_ref\n",
    "    return feat_final\n",
    "\n",
    "def normalize_outliers(x_in, dist_type):\n",
    "    # 1. Substract mean\n",
    "    # 2. Compute std and detect ouliers\n",
    "    # 3. Compute std and mean witout ouliers\n",
    "    mean_corr = []\n",
    "    std_corr = []\n",
    "    max_val_corr = []\n",
    "                \n",
    "    for i, feat in enumerate(x_in.T):\n",
    "        # Normalize according to distribution\n",
    "        if dist_type[i] == 'g' or dist_type[i] == 'i' or dist_type[i] == 'u' \\\n",
    "                or dist_type[i] == 'f' or dist_type[i] == 'd':\n",
    "            feat_new, mean_new, std_new, max_val_new = norm_gaussian(feat)\n",
    "        elif dist_type[i] == 'p':\n",
    "            feat_new, mean_new, std_new, max_val_new = norm_poisson(feat)\n",
    "        else:\n",
    "            feat_new, mean_new, std_new, max_val_new = (feat, 0, 1, np.inf)\n",
    "        # Affect new values\n",
    "        mean_corr.append(mean_new)\n",
    "        std_corr.append(std_new)\n",
    "        max_val_corr.append(max_val_new)\n",
    "        x_in[:, i] = feat_new\n",
    "    return x_in, mean_corr, std_corr, max_val_corr\n",
    "\n",
    "def normalize_outliers_feed(x_in, mean_ref, std_ref, max_ref, dist_type):\n",
    "    # 1. Substract mean\n",
    "    # 2. Compute std and detect ouliers\n",
    "    # 3. Compute std and mean witout ouliers\n",
    "    \n",
    "    for i, feat in enumerate(x_in.T):\n",
    "        # Normalize according to distribution\n",
    "        if dist_type[i] == 'g' or dist_type[i] == 'i' or dist_type[i] == 'u' \\\n",
    "                or dist_type[i] == 'f' or dist_type[i] == 'd':\n",
    "            feat_new = norm_gaussian_feed(feat, mean_ref[i], std_ref[i], max_ref[i])\n",
    "        elif dist_type[i] == 'p':\n",
    "            feat_new = norm_poisson_feed(feat, mean_ref[i], std_ref[i], max_ref[i])\n",
    "        else:\n",
    "            feat_new = feat\n",
    "        # Affect new value\n",
    "        x_in[:, i] = feat_new\n",
    "    return x_in\n",
    "\n",
    "### UNUSED \n",
    "def remove_useless(x_in, id_useless=[15, 18, 20, 25, 28]):\n",
    "    id_left = [ i for i in range(x_train.shape[1]) if i not in id_angle_feat]\n",
    "    return x_in[:, id_left]\n",
    "\n",
    "def recenter_feature(feature):\n",
    "    val_max = np.nanmax(feature)\n",
    "    val_min = np.nanmin(feature)\n",
    "    feature[feature < np.nanmean(feature)] += (val_max - val_min)\n",
    "    return feature\n",
    "### UNUSED \n",
    "\n",
    "# normalize features\n",
    "#x_no_nan = x_train.copy()\n",
    "#x_no_nan = (x_no_nan - np.nanmean(x_no_nan, axis=0))/np.nanstd(x_no_nan, axis=0)\n",
    "#x_no_nan = np.nan_to_num(x_no_nan)\n",
    "#print('\\nStd:', np.std(x_no_nan, axis=0))\n",
    "\n",
    "# normalize features\n",
    "#x_no_nan_val = x_validation.copy()\n",
    "#x_no_nan_val = (x_no_nan_val - np.nanmean(x_no_nan_val, axis=0))/np.nanstd(x_no_nan_val, axis=0)\n",
    "#x_no_nan_val = np.nan_to_num(x_no_nan_val)\n",
    "#print('\\nStd:', np.std(x_no_nan_val, axis=0))\n",
    "\n",
    "\n",
    "# normalize features\n",
    "#x_no_nan = x_train.copy()\n",
    "#x_no_nan = (x_no_nan - np.nanmedian(x_no_nan, axis=0))/mad(x_no_nan)\n",
    "#x_no_nan = np.nan_to_num(x_no_nan)\n",
    "#print('\\nStd:', mad(x_no_nan))\n",
    "\n",
    "# normalize features\n",
    "#x_no_nan_val = x_validation.copy()\n",
    "#x_no_nan_val = (x_no_nan_val - np.nanmedian(x_no_nan_val, axis=0))/mad(x_no_nan_val)\n",
    "#x_no_nan_val = np.nan_to_num(x_no_nan_val)\n",
    "#print('\\nStd:', mad(x_no_nan_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:31: RuntimeWarning: invalid value encountered in less\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:32: RuntimeWarning: invalid value encountered in less\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:34: RuntimeWarning: invalid value encountered in greater\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:35: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Std: [ 1.074   1.0408  1.1532  1.1364  0.5519  0.6304  0.5907  1.0192  1.1146\n",
      "  1.1341  1.1064  1.      0.5399  1.1454  1.      1.      1.1456  1.      1.\n",
      "  1.1392  1.      1.1057  1.      0.89    0.7758  0.7753  0.6201  0.5399\n",
      "  0.5399  1.1287]\n",
      "\n",
      "Std: [ 1.0809  1.0384  1.1548  1.134   0.5455  0.6189  0.5829  1.0176  1.106\n",
      "  1.1246  1.1044  1.0038  0.5337  1.1372  1.0017  1.0002  1.1371  1.0049\n",
      "  1.0041  1.1418  0.9975  1.103   0.9936  0.8915  0.7739  0.7704  0.6114\n",
      "  0.5313  0.5326  1.1198]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:51: RuntimeWarning: invalid value encountered in greater\n",
      "/home/abbet/anaconda3/envs/ml/lib/python3.5/site-packages/ipykernel/__main__.py:52: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "x_no_nan = x_train.copy()\n",
    "x_no_nan, mean_train, std_train, max_train = normalize_outliers(x_no_nan, distrib)\n",
    "#x_no_nan = remove_useless(x_no_nan)\n",
    "x_no_nan = np.nan_to_num(x_no_nan)\n",
    "print('\\nStd:', np.std(x_no_nan, axis=0))\n",
    "\n",
    "# normalize features\n",
    "x_no_nan_val = x_validation.copy()\n",
    "x_no_nan_val = normalize_outliers_feed(x_no_nan_val, mean_train, std_train, max_train, distrib)\n",
    "#x_no_nan_val = remove_useless(x_no_nan_val)\n",
    "x_no_nan_val = np.nan_to_num(x_no_nan_val)\n",
    "print('\\nStd:', np.std(x_no_nan_val, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=8, lambda=0.000, Training RMSE=0.730, Testing RMSE=0.729\n",
      "train acc :  0.82182\n",
      "validation acc :  0.8222\n",
      "degree=8, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.822095\n",
      "validation acc :  0.82248\n",
      "degree=8, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.822055\n",
      "validation acc :  0.8222\n",
      "degree=8, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.82208\n",
      "validation acc :  0.82224\n",
      "degree=8, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.822095\n",
      "validation acc :  0.82222\n",
      "degree=8, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.822075\n",
      "validation acc :  0.82222\n",
      "degree=8, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.82204\n",
      "validation acc :  0.82224\n",
      "degree=8, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.821995\n",
      "validation acc :  0.8223\n",
      "degree=8, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.822\n",
      "validation acc :  0.82218\n",
      "degree=8, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.821835\n",
      "validation acc :  0.82192\n",
      "degree=8, lambda=0.000, Training RMSE=0.730, Testing RMSE=0.729\n",
      "train acc :  0.82172\n",
      "validation acc :  0.82188\n",
      "degree=8, lambda=0.000, Training RMSE=0.731, Testing RMSE=0.730\n",
      "train acc :  0.821185\n",
      "validation acc :  0.82168\n",
      "degree=8, lambda=0.000, Training RMSE=0.739, Testing RMSE=0.737\n",
      "train acc :  0.817415\n",
      "validation acc :  0.81832\n",
      "degree=8, lambda=0.001, Training RMSE=0.755, Testing RMSE=0.753\n",
      "train acc :  0.807495\n",
      "validation acc :  0.80832\n",
      "degree=8, lambda=0.001, Training RMSE=0.781, Testing RMSE=0.778\n",
      "train acc :  0.786415\n",
      "validation acc :  0.79034\n",
      "degree=8, lambda=0.003, Training RMSE=0.820, Testing RMSE=0.817\n",
      "train acc :  0.76235\n",
      "validation acc :  0.76608\n",
      "degree=8, lambda=0.008, Training RMSE=0.860, Testing RMSE=0.856\n",
      "train acc :  0.742495\n",
      "validation acc :  0.74554\n",
      "degree=8, lambda=0.018, Training RMSE=0.887, Testing RMSE=0.884\n",
      "train acc :  0.72968\n",
      "validation acc :  0.73376\n",
      "degree=8, lambda=0.043, Training RMSE=0.907, Testing RMSE=0.905\n",
      "train acc :  0.724345\n",
      "validation acc :  0.72842\n",
      "degree=8, lambda=0.100, Training RMSE=0.933, Testing RMSE=0.932\n",
      "train acc :  0.72029\n",
      "validation acc :  0.7243\n",
      "degree=9, lambda=0.000, Training RMSE=0.737, Testing RMSE=0.737\n",
      "train acc :  0.81828\n",
      "validation acc :  0.8175\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.822375\n",
      "validation acc :  0.82214\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.728\n",
      "train acc :  0.82215\n",
      "validation acc :  0.82248\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.728\n",
      "train acc :  0.82224\n",
      "validation acc :  0.82248\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.728\n",
      "train acc :  0.8222\n",
      "validation acc :  0.82242\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.728\n",
      "train acc :  0.82227\n",
      "validation acc :  0.82244\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.728\n",
      "train acc :  0.82223\n",
      "validation acc :  0.82242\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.82219\n",
      "validation acc :  0.8223\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.82213\n",
      "validation acc :  0.82238\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.82207\n",
      "validation acc :  0.82204\n",
      "degree=9, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.821825\n",
      "validation acc :  0.82158\n",
      "degree=9, lambda=0.000, Training RMSE=0.731, Testing RMSE=0.730\n",
      "train acc :  0.82089\n",
      "validation acc :  0.82138\n",
      "degree=9, lambda=0.000, Training RMSE=0.738, Testing RMSE=0.737\n",
      "train acc :  0.8176\n",
      "validation acc :  0.81874\n",
      "degree=9, lambda=0.001, Training RMSE=0.753, Testing RMSE=0.751\n",
      "train acc :  0.8079\n",
      "validation acc :  0.80898\n",
      "degree=9, lambda=0.001, Training RMSE=0.782, Testing RMSE=0.779\n",
      "train acc :  0.785875\n",
      "validation acc :  0.78954\n",
      "degree=9, lambda=0.003, Training RMSE=0.817, Testing RMSE=0.815\n",
      "train acc :  0.76207\n",
      "validation acc :  0.76564\n",
      "degree=9, lambda=0.008, Training RMSE=0.849, Testing RMSE=0.846\n",
      "train acc :  0.74779\n",
      "validation acc :  0.75126\n",
      "degree=9, lambda=0.018, Training RMSE=0.882, Testing RMSE=0.879\n",
      "train acc :  0.733695\n",
      "validation acc :  0.73666\n",
      "degree=9, lambda=0.043, Training RMSE=0.907, Testing RMSE=0.905\n",
      "train acc :  0.722115\n",
      "validation acc :  0.72528\n",
      "degree=9, lambda=0.100, Training RMSE=0.927, Testing RMSE=0.926\n",
      "train acc :  0.720825\n",
      "validation acc :  0.72456\n",
      "degree=10, lambda=0.000, Training RMSE=0.791, Testing RMSE=0.790\n",
      "train acc :  0.7951\n",
      "validation acc :  0.79682\n",
      "degree=10, lambda=0.000, Training RMSE=0.733, Testing RMSE=0.733\n",
      "train acc :  0.82023\n",
      "validation acc :  0.81906\n",
      "degree=10, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.82193\n",
      "validation acc :  0.82238\n",
      "degree=10, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.729\n",
      "train acc :  0.822135\n",
      "validation acc :  0.82292\n",
      "degree=10, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.729\n",
      "train acc :  0.82225\n",
      "validation acc :  0.82296\n",
      "degree=10, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.729\n",
      "train acc :  0.82217\n",
      "validation acc :  0.82306\n",
      "degree=10, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.729\n",
      "train acc :  0.82223\n",
      "validation acc :  0.82304\n",
      "degree=10, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.729\n",
      "train acc :  0.82219\n",
      "validation acc :  0.82258\n",
      "degree=10, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.82216\n",
      "validation acc :  0.82236\n",
      "degree=10, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.822125\n",
      "validation acc :  0.82196\n",
      "degree=10, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.821715\n",
      "validation acc :  0.82208\n",
      "degree=10, lambda=0.000, Training RMSE=0.731, Testing RMSE=0.730\n",
      "train acc :  0.821135\n",
      "validation acc :  0.82144\n",
      "degree=10, lambda=0.000, Training RMSE=0.737, Testing RMSE=0.736\n",
      "train acc :  0.81773\n",
      "validation acc :  0.819\n",
      "degree=10, lambda=0.001, Training RMSE=0.752, Testing RMSE=0.750\n",
      "train acc :  0.80872\n",
      "validation acc :  0.80944\n",
      "degree=10, lambda=0.001, Training RMSE=0.781, Testing RMSE=0.779\n",
      "train acc :  0.786245\n",
      "validation acc :  0.78876\n",
      "degree=10, lambda=0.003, Training RMSE=0.815, Testing RMSE=0.812\n",
      "train acc :  0.763915\n",
      "validation acc :  0.7672\n",
      "degree=10, lambda=0.008, Training RMSE=0.846, Testing RMSE=0.843\n",
      "train acc :  0.74892\n",
      "validation acc :  0.7536\n",
      "degree=10, lambda=0.018, Training RMSE=0.877, Testing RMSE=0.873\n",
      "train acc :  0.736395\n",
      "validation acc :  0.74102\n",
      "degree=10, lambda=0.043, Training RMSE=0.899, Testing RMSE=0.896\n",
      "train acc :  0.7251\n",
      "validation acc :  0.72912\n",
      "degree=10, lambda=0.100, Training RMSE=0.914, Testing RMSE=0.911\n",
      "train acc :  0.721005\n",
      "validation acc :  0.72488\n",
      "degree=11, lambda=0.000, Training RMSE=0.771, Testing RMSE=0.771\n",
      "train acc :  0.801385\n",
      "validation acc :  0.80206\n",
      "degree=11, lambda=0.000, Training RMSE=0.735, Testing RMSE=0.735\n",
      "train acc :  0.821155\n",
      "validation acc :  0.82028\n",
      "degree=11, lambda=0.000, Training RMSE=0.731, Testing RMSE=0.731\n",
      "train acc :  0.82103\n",
      "validation acc :  0.82132\n",
      "degree=11, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.729\n",
      "train acc :  0.821905\n",
      "validation acc :  0.82224\n",
      "degree=11, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.728\n",
      "train acc :  0.822125\n",
      "validation acc :  0.82266\n",
      "degree=11, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.728\n",
      "train acc :  0.822065\n",
      "validation acc :  0.82266\n",
      "degree=11, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.728\n",
      "train acc :  0.8221\n",
      "validation acc :  0.82272\n",
      "degree=11, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.728\n",
      "train acc :  0.82216\n",
      "validation acc :  0.82248\n",
      "degree=11, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.728\n",
      "train acc :  0.82216\n",
      "validation acc :  0.82278\n",
      "degree=11, lambda=0.000, Training RMSE=0.728, Testing RMSE=0.728\n",
      "train acc :  0.82192\n",
      "validation acc :  0.82198\n",
      "degree=11, lambda=0.000, Training RMSE=0.729, Testing RMSE=0.729\n",
      "train acc :  0.82177\n",
      "validation acc :  0.82152\n",
      "degree=11, lambda=0.000, Training RMSE=0.731, Testing RMSE=0.730\n",
      "train acc :  0.821015\n",
      "validation acc :  0.82144\n",
      "degree=11, lambda=0.000, Training RMSE=0.737, Testing RMSE=0.735\n",
      "train acc :  0.81799\n",
      "validation acc :  0.81886\n",
      "degree=11, lambda=0.001, Training RMSE=0.753, Testing RMSE=0.751\n",
      "train acc :  0.808705\n",
      "validation acc :  0.8092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree=11, lambda=0.001, Training RMSE=0.779, Testing RMSE=0.777\n",
      "train acc :  0.787795\n",
      "validation acc :  0.7905\n",
      "degree=11, lambda=0.003, Training RMSE=0.811, Testing RMSE=0.808\n",
      "train acc :  0.764605\n",
      "validation acc :  0.76796\n",
      "degree=11, lambda=0.008, Training RMSE=0.845, Testing RMSE=0.842\n",
      "train acc :  0.748565\n",
      "validation acc :  0.75204\n",
      "degree=11, lambda=0.018, Training RMSE=0.870, Testing RMSE=0.868\n",
      "train acc :  0.73923\n",
      "validation acc :  0.74284\n",
      "degree=11, lambda=0.043, Training RMSE=0.890, Testing RMSE=0.887\n",
      "train acc :  0.731145\n",
      "validation acc :  0.73434\n",
      "degree=11, lambda=0.100, Training RMSE=0.909, Testing RMSE=0.906\n",
      "train acc :  0.721985\n",
      "validation acc :  0.72576\n",
      "Best params for Ridge regression : degree =  10 , lambda =  6.95192796178e-07 , accuracy =  0.82306\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEaCAYAAAA7YdFPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSepikEiSuhgkkqQuBokkqct33Kkkv9SrJF0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff98adecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights_no_nan, degree_no_nan, lambda_no_nan = test_ridge_regression(\n",
    "    x_no_nan, y_train, x_no_nan_val, y_validation, degrees = np.linspace(8,11,4), lambdas=np.logspace(-8,-1, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, x_test, ids_test, header = helper.load_csv_data(DATA_TEST)\n",
    "x_test[x_test == -999] = np.nan\n",
    "\n",
    "x_no_nan_test = x_test.copy()\n",
    "x_no_nan_test = normalize_outliers_feed(x_no_nan_test, mean_train, std_train)\n",
    "x_no_nan_test = remove_useless(x_no_nan_test)\n",
    "# x_no_nan_test = (x_no_nan_test - np.nanmedian(x_no_nan_test, axis=0))/mad(x_no_nan_test)\n",
    "x_no_nan_test = np.nan_to_num(x_no_nan_test)\n",
    "print('\\nStd:', np.std(x_no_nan_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_opt = degree_no_nan\n",
    "weights_opt = weights_no_nan\n",
    "\n",
    "_phi_test = lib.build_poly(x_no_nan_test, degree_opt)\n",
    "y_pred = helper.predict_labels(weights_opt, _phi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.create_csv_submission(ids_test, y_pred, 'ridge_no_nan1.csv')\n",
    "print('Results saved ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_no_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
