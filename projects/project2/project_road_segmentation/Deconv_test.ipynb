{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeconvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os,sys\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def load_image(infilename):\n",
    "    data = mpimg.imread(infilename)\n",
    "    return data\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * 255).round().astype(np.uint8)\n",
    "    return rimg\n",
    "\n",
    "# Concatenate an image and its groundtruth\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def normalize_data(data):\n",
    "    # Data pre-processing, Normalize each image with itself\n",
    "    n = data.shape[0]\n",
    "    for i in range(n):\n",
    "        xx = data[i,:,:]\n",
    "        xx -= np.mean(xx) # Centering in 0\n",
    "        xx /= np.linalg.norm(xx) # Normalizing to 1\n",
    "        data[i] = xx # Affect value\n",
    "    return data\n",
    "def one_hot_convert(vector, num_classes=None):\n",
    "    \"\"\" (From https://stackoverflow.com/questions/29831489/numpy-1-hot-array)\n",
    "    Converts an input 1-D vector of integers into an output\n",
    "    2-D array of one-hot vectors, where an i'th input value\n",
    "    of j will set a '1' in the i'th row, j'th column of the\n",
    "    output array.\n",
    "\n",
    "    Example:\n",
    "        v = np.array((1, 0, 4))\n",
    "        one_hot_v = convertToOneHot(v)\n",
    "        print one_hot_v\n",
    "\n",
    "        [[0 1 0 0 0]\n",
    "         [1 0 0 0 0]\n",
    "         [0 0 0 0 1]]\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(vector, np.ndarray)\n",
    "    assert len(vector) > 0\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = np.max(vector)+1\n",
    "    else:\n",
    "        assert num_classes > 0\n",
    "        assert num_classes >= np.max(vector)\n",
    "\n",
    "    result = np.zeros(shape=(len(vector), num_classes))\n",
    "    result[np.arange(len(vector)), vector] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 8, 9]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.asarray([[1,2,3],[4,5,6],[7,8,9]])\n",
    "a[2:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 images\n",
      "satImage_001.png\n",
      "Loading 20 images\n",
      "satImage_001.png\n"
     ]
    }
   ],
   "source": [
    "# Loaded a set of images\n",
    "root_dir = \"training/\"\n",
    "\n",
    "image_dir = root_dir + \"images/\"\n",
    "files = os.listdir(image_dir)\n",
    "n = min(20, len(files)) # Load maximum 20 images\n",
    "#n = len(files)\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "imgs = [load_image(image_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "gt_dir = root_dir + \"groundtruth/\"\n",
    "print(\"Loading \" + str(n) + \" images\")\n",
    "gt_imgs = [load_image(gt_dir + files[i]) for i in range(n)]\n",
    "print(files[0])\n",
    "\n",
    "#n = int(len(files)*0.8) # Only use 10 images for training\n",
    "n=10\n",
    "np_imgs = normalize_data(np.asarray(imgs))\n",
    "train_imgs = np_imgs[:n]\n",
    "val_imgs = np_imgs[n:]\n",
    "np_gt = np.ceil(np.asarray(gt_imgs))\n",
    "train_gt = np_gt[:n]\n",
    "val_gt = np_gt[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpool(net, mask, stride):\n",
    "  assert mask is not None\n",
    "  with tf.name_scope('UnPool2D'):\n",
    "    ksize = [1, stride, stride, 1]\n",
    "    input_shape = net.get_shape().as_list()\n",
    "    #  calculation new shape\n",
    "    output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n",
    "    # calculation indices for batch, height, width and feature maps\n",
    "    one_like_mask = tf.ones_like(mask)\n",
    "    batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int64), shape=[input_shape[0], 1, 1, 1])\n",
    "    b = one_like_mask * batch_range\n",
    "    y = mask // (output_shape[2] * output_shape[3])\n",
    "    x = mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n",
    "    feature_range = tf.range(output_shape[3], dtype=tf.int64)\n",
    "    f = one_like_mask * feature_range\n",
    "    # transpose indices & reshape update values to one dimension\n",
    "    updates_size = tf.size(net)\n",
    "    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n",
    "    values = tf.reshape(net, [updates_size])\n",
    "    ret = tf.scatter_nd(indices, values, output_shape)\n",
    "    return ret\n",
    "\n",
    "def unpool_naive(value, name='unpool'):\n",
    "        \"\"\"N-dimensional version of the unpooling operation from\n",
    "        https://www.robots.ox.ac.uk/~vgg/rg/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf\n",
    "\n",
    "        :param value: A Tensor of shape [b, d0, d1, ..., dn, ch]\n",
    "        :return: A Tensor of shape [b, 2*d0, 2*d1, ..., 2*dn, ch]\n",
    "        # x will become [[x,0], [0, 0]] in any cases\n",
    "        # From : https://github.com/tensorflow/tensorflow/issues/2169\n",
    "        \"\"\"\n",
    "        with tf.name_scope(name) as scope:\n",
    "            sh = value.get_shape().as_list()\n",
    "            dim = len(sh[1:-1])\n",
    "            out = (tf.reshape(value, [-1] + sh[-dim:]))\n",
    "            for i in range(dim, 0, -1):\n",
    "                out = tf.concat( [out, tf.zeros_like(out, dtype=tf.float32)],i)\n",
    "            out_size = [-1] + [s * 2 for s in sh[1:-1]] + [sh[-1]]\n",
    "            out = tf.reshape(out, out_size, name=scope)\n",
    "        return out\n",
    "    \n",
    "def get_bilinear_filter(filter_shape, upscale_factor):\n",
    "    \"\"\"http://cv-tricks.com/image-segmentation/transpose-convolution-in-tensorflow/\"\"\"\n",
    "    ##filter_shape is [width, height, num_in_channels, num_out_channels]\n",
    "    kernel_size = filter_shape[1]\n",
    "    ### Centre location of the filter for which value is calculated\n",
    "    if kernel_size % 2 == 1:\n",
    "        centre_location = upscale_factor - 1\n",
    "    else:\n",
    "        centre_location = upscale_factor - 0.5\n",
    " \n",
    "    bilinear = np.zeros([filter_shape[0], filter_shape[1]])\n",
    "    for x in range(filter_shape[0]):\n",
    "        for y in range(filter_shape[1]):\n",
    "            ##Interpolation Calculation\n",
    "            value = (1 - abs((x - centre_location)/ upscale_factor)) * (1 - abs((y - centre_location)/ upscale_factor))\n",
    "            bilinear[x, y] = value\n",
    "    weights = np.zeros(filter_shape)\n",
    "    for i in range(filter_shape[2]):\n",
    "        weights[:, :, i, i] = bilinear\n",
    "    init = tf.constant_initializer(value=weights,\n",
    "                                       dtype=tf.float32)\n",
    " \n",
    "    bilinear_weights = tf.get_variable(name=\"deconv_bilinear_filter\", initializer=init,\n",
    "                               shape=weights.shape)\n",
    "    return bilinear_weights\n",
    "    \n",
    "def upsample_layer(bottom, n_channels, name, upscale_factor):\n",
    "    \"\"\"http://cv-tricks.com/image-segmentation/transpose-convolution-in-tensorflow/\"\"\"\n",
    "    kernel_size = 2*upscale_factor - upscale_factor%2\n",
    "    stride = upscale_factor   \n",
    "    strides = [1, stride, stride, 1]\n",
    "    with tf.variable_scope(name):\n",
    "        # Shape of the bottom tensor\n",
    "        in_shape = tf.shape(bottom)\n",
    " \n",
    "        h = ((in_shape[1] - 1) * stride) + 1\n",
    "        w = ((in_shape[2] - 1) * stride) + 1\n",
    "        new_shape = [in_shape[0], h, w, n_channels]\n",
    "        output_shape = tf.stack(new_shape)\n",
    " \n",
    "        filter_shape = [kernel_size, kernel_size, n_channels, n_channels]\n",
    " \n",
    "        weights = get_bilinear_filter(filter_shape,upscale_factor)\n",
    "        deconv = tf.nn.conv2d_transpose(bottom, weights, output_shape,\n",
    "                                            strides=strides, padding='SAME')\n",
    " \n",
    "    return deconv\n",
    "\n",
    "def deconv_layer(x, W_shape, b_shape, padding='SAME'):\n",
    "    \"\"\"https://github.com/fabianbormann/Tensorflow-DeconvNet-Segmentation/blob/master/DeconvNetPipeline.py\n",
    "    W_shape must be [width height in_channel out_channel]\"\"\"\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal(W_shape,stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[b_shape]))\n",
    "    x_shape = tf.shape(x)\n",
    "    out_shape = tf.stack([x_shape[0], x_shape[1], x_shape[2], W_shape[2]])\n",
    "\n",
    "    return tf.nn.conv2d_transpose(x, W, out_shape, [1, 1, 1, 1], padding=padding) + b\n",
    "\n",
    "def weighted_pixelwise_crossentropy(y_true, y_pred, class_weights):\n",
    "    \"\"\"computes pixelwise crossentropy\"\"\"\n",
    "    temp = tf.clip_by_value(y_pred, 1e-8, 1. - 1e-8)\n",
    "    loss = - tf.reduce_sum(tf.multiply(y_true * tf.log(temp), class_weights))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 size (?, 400, 400, 16)\n",
      "pool1 size (?, 200, 200, 16)\n",
      "conv2 size (?, 200, 200, 32)\n",
      "pool2 size (?, 100, 100, 32)\n",
      "conv3 size (?, 100, 100, 64)\n",
      "pool3 size (?, 50, 50, 64)\n",
      "conv4 size (?, 50, 50, 64)\n",
      "pool4 size (?, 25, 25, 64)\n",
      "unpool 1 size (?, 50, 50, 64)\n",
      "deconv_1 size  (?, 50, 50, 64)\n",
      "unpool2 size  (?, 100, 100, 64)\n",
      "deconv_2 size (?, 100, 100, 32)\n",
      "unpool3 size  (?, 200, 200, 32)\n",
      "deconv_3 size (?, 200, 200, 16)\n",
      "unpool4 size  (?, 400, 400, 16)\n",
      "score size (?, 400, 400, 2)\n",
      "Tensor(\"Reshape:0\", shape=(?, 2), dtype=float32)\n",
      "Tensor(\"Placeholder_1:0\", shape=(?, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Define neural network\n",
    "\n",
    "learning_rate = 1e0\n",
    "reg = 1e-3\n",
    "n_filters = 16\n",
    "kernel_size = 5\n",
    "\n",
    "tf_data = tf.placeholder(tf.float32,[None, 400,400,3])\n",
    "tf_labels = tf.placeholder(tf.int32,[None,2])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=reg)\n",
    "\n",
    "class_weights = tf.constant([[1.0,3.0]])\n",
    "weights = tf.reduce_sum(class_weights * tf.cast(tf_labels, tf.float32), axis=1)\n",
    "\n",
    "#Conv layer 1\n",
    "conv_1 = tf.layers.conv2d(inputs=tf_data, filters=n_filters, kernel_size=kernel_size, kernel_regularizer=regularizer, bias_regularizer=regularizer, padding='same')\n",
    "#pool layer 1\n",
    "print(\"conv1 size\", conv_1.shape)\n",
    "\n",
    "pool_1 = tf.contrib.layers.max_pool2d(inputs = conv_1, kernel_size=2, stride=2)\n",
    "#pool_1, mask_1 = tf.nn.max_pool_with_argmax(input=conv_1, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "print(\"pool1 size\", pool_1.shape)\n",
    "\n",
    "#conv layer 2\n",
    "\n",
    "conv_2 = tf.layers.conv2d(inputs = pool_1, filters = n_filters*2, kernel_size = kernel_size, kernel_regularizer=regularizer, bias_regularizer=regularizer, padding='same')\n",
    "print(\"conv2 size\", conv_2.shape)\n",
    "\n",
    "#Pool layer 2\n",
    "\n",
    "pool_2 = tf.contrib.layers.max_pool2d(inputs = conv_2, kernel_size=2, stride=2)\n",
    "#pool_2, mask_2 = tf.nn.max_pool_with_argmax(input=conv_2, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "print(\"pool2 size\", pool_2.shape)\n",
    "\n",
    "# Relu\n",
    "\n",
    "relu_1 = tf.nn.relu(features=pool_2)\n",
    "\n",
    "#Conv layer 3\n",
    "conv_3 = tf.layers.conv2d(inputs=relu_1, filters=n_filters*4, kernel_size=kernel_size, kernel_regularizer=regularizer, bias_regularizer=regularizer, padding='same')\n",
    "#pool layer 3\n",
    "print(\"conv3 size\", conv_3.shape)\n",
    "\n",
    "pool_3 = tf.contrib.layers.max_pool2d(inputs = conv_3, kernel_size=2, stride=2)\n",
    "#pool_3, mask_3 = tf.nn.max_pool_with_argmax(input=conv_3, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "print(\"pool3 size\", pool_3.shape)\n",
    "\n",
    "#conv layer 4\n",
    "\n",
    "conv_4 = tf.layers.conv2d(inputs = pool_3, filters = n_filters*4, kernel_size = kernel_size, kernel_regularizer=regularizer, bias_regularizer=regularizer, padding='same')\n",
    "print(\"conv4 size\", conv_4.shape)\n",
    "\n",
    "#Pool layer 4\n",
    "\n",
    "pool_4 = tf.contrib.layers.max_pool2d(inputs = conv_4, kernel_size=2, stride=2)\n",
    "#pool_4, mask_4 = tf.nn.max_pool_with_argmax(input=conv_4, ksize=(1,2,2,1), strides=(1,2,2,1), padding='VALID')\n",
    "\n",
    "print(\"pool4 size\", pool_4.shape)\n",
    "\n",
    "# Relu 2\n",
    "\n",
    "relu_2 = tf.nn.relu(features=pool_4)\n",
    "\n",
    "#FC layer (as conv)\n",
    "#fc_1 = tf.layers.conv2d(inputs= relu_2, kernel_size=(25,25), filters = 500, kernel_regularizer=regularizer,bias_regularizer=regularizer) #compute the shape based on the expected input (here 200 => 15x15)\n",
    "#print(\"fc1 size \", fc_1.shape)\n",
    "#FC Layer 2 (as conv)\n",
    "#fc_2 = tf.layers.conv2d(inputs=fc_1, kernel_size = 1, filters=2, kernel_regularizer=regularizer,bias_regularizer=regularizer)\n",
    "#print(\"fc2 size \", fc_2.shape)\n",
    "#fc_1 = tf.contrib.layers.fully_connected(inputs=tf.reshape(conv_out,[-1, (mfcc_num-kernel_size+1)*n_filters]), num_outputs=2, weights_regularizer=regularizer, biases_regularizer=regularizer)\n",
    "#fc_1 = tf.layers.dropout(inputs=fc_1,rate=keep_prob)\n",
    "\n",
    "# Deconv FC layer\n",
    "#deconv_fc = deconv_layer(fc_2, [25,25,100,2],100)\n",
    "#deconv_fc = tf.layers.conv2d_transpose(inputs=fc_1, kernel_size=(25,25), filters = n_filters*4, kernel_regularizer=regularizer,bias_regularizer=regularizer)\n",
    "#deconv_fc = upsample_layer(bottom=fc_2, n_channels=64, name=\"deconv_fc\", upscale_factor=25)\n",
    "#print(\"deconv fc size \", deconv_fc.shape)\n",
    "\n",
    "# Deconv relu\n",
    "\n",
    "#relu_3 = tf.nn.relu(features=deconv_fc)\n",
    "# Unpool 1\n",
    "unpool_1 = unpool_naive(relu_2)\n",
    "print(\"unpool 1 size\", unpool_1.shape)\n",
    "\n",
    "# Deconv 1\n",
    "\n",
    "deconv_1 = tf.layers.conv2d_transpose(inputs=unpool_1, filters=n_filters*4 ,kernel_size=kernel_size, kernel_regularizer=regularizer,bias_regularizer=regularizer,padding='SAME')\n",
    "print(\"deconv_1 size \",deconv_1.shape)\n",
    "\n",
    "# Unpool 2\n",
    "\n",
    "unpool_2 = unpool_naive(deconv_1)\n",
    "print(\"unpool2 size \", unpool_2.shape)\n",
    "\n",
    "#Deconv 2\n",
    "\n",
    "deconv_2 = tf.layers.conv2d_transpose(inputs=unpool_2, filters=n_filters*2,kernel_size=kernel_size, kernel_regularizer=regularizer,bias_regularizer=regularizer,padding='SAME')\n",
    "print(\"deconv_2 size\", deconv_2.shape)\n",
    "\n",
    "#unpool 3 \n",
    "\n",
    "unpool_3 = unpool_naive(deconv_2)\n",
    "print(\"unpool3 size \", unpool_3.shape)\n",
    "\n",
    "#Deconv 3\n",
    "\n",
    "deconv_3 = tf.layers.conv2d_transpose(inputs=unpool_3, filters=n_filters,kernel_size=kernel_size, kernel_regularizer=regularizer,bias_regularizer=regularizer,padding='SAME')\n",
    "print(\"deconv_3 size\", deconv_3.shape)\n",
    "\n",
    "#unpool 4\n",
    "\n",
    "unpool_4 = unpool_naive(deconv_3)\n",
    "print(\"unpool4 size \", unpool_4.shape)\n",
    "\n",
    "#Deconv 4\n",
    "\n",
    "#deconv_4 = tf.layers.conv2d_transpose(inputs=unpool_4, filters=3,kernel_size=kernel_size, kernel_regularizer=regularizer,bias_regularizer=regularizer,padding='SAME')\n",
    "#print(\"deconv_4 size\", deconv_4.shape)\n",
    "\n",
    "# Score\n",
    "\n",
    "score_layer = tf.layers.conv2d_transpose(inputs=unpool_4, filters=2, kernel_size=1, kernel_regularizer=regularizer,bias_regularizer=regularizer, padding='SAME')\n",
    "print(\"score size\", score_layer.shape)\n",
    "\n",
    "logits = tf.reshape(score_layer, (-1,2))\n",
    "\n",
    "print(logits)\n",
    "print(tf_labels)\n",
    "#loss\n",
    "#cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_labels, logits=logits)\n",
    "#cross_entropy = tf.losses.softmax_cross_entropy(tf_labels, logits,weights=weights)\n",
    "#cross_entropy = tf.losses.sigmoid_cross_entropy(tf_labels, logits, weights=class_weights)\n",
    "cross_entropy = weighted_pixelwise_crossentropy(tf.cast(tf_labels, dtype=tf.float32), logits, class_weights)\n",
    "#Regularization\n",
    "reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "loss = reg_term + cross_entropy\n",
    "\n",
    "#Optimizer\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "preds = tf.argmax(logits,axis=1,output_type=tf.int32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, tf.argmax(tf_labels,axis=1, output_type=tf.int32)), tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on image 7\n",
      "4.07449e+06\n",
      "1.60687e-06\n",
      "training on image 4\n",
      "527735.0\n",
      "3.31609\n",
      "training on image 8\n",
      "415108.0\n",
      "25.7737\n",
      "training on image 6\n",
      "390091.0\n",
      "75.897\n",
      "training on image 9\n",
      "372427.0\n",
      "154.193\n",
      "training on image 0\n",
      "343561.0\n",
      "257.906\n",
      "training on image 2\n",
      "344059.0\n",
      "383.136\n",
      "training on image 5\n",
      "343710.0\n",
      "525.634\n",
      "training on image 1\n",
      "342107.0\n",
      "681.297\n",
      "training on image 3\n",
      "327886.0\n",
      "846.36\n",
      "(1600000,)\n",
      "(1600000,)\n",
      "(10, 400, 400)\n",
      "epoch  1 , val f1 :  0.500416277578 , train f1 :  0.492865112369\n",
      "val_loss :  3.0621e+06 , train_loss :  3.31668e+06\n",
      "training on image 3\n",
      "325067.0\n",
      "1017.45\n",
      "training on image 6\n",
      "329065.0\n",
      "1191.66\n",
      "training on image 9\n",
      "326284.0\n",
      "1366.52\n",
      "training on image 7\n",
      "315268.0\n",
      "1539.96\n",
      "training on image 0\n",
      "312542.0\n",
      "1710.31\n",
      "training on image 8\n",
      "329525.0\n",
      "1876.21\n",
      "training on image 2\n",
      "322691.0\n",
      "2036.62\n",
      "training on image 4\n",
      "317258.0\n",
      "2190.75\n",
      "training on image 1\n",
      "321384.0\n",
      "2338.02\n",
      "training on image 5\n",
      "323503.0\n",
      "2478.05\n",
      "(1600000,)\n",
      "(1600000,)\n",
      "(10, 400, 400)\n",
      "epoch  2 , val f1 :  0.500403679655 , train f1 :  0.49278605868\n",
      "val_loss :  2.88592e+06 , train_loss :  3.15859e+06\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 2\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    train_idx = np.random.permutation(np.arange(n))\n",
    "    for idx in train_idx:\n",
    "        print(\"training on image {:d}\".format(idx))\n",
    "        cur_img = train_imgs[idx]\n",
    "\n",
    "        batch_x = np.expand_dims(cur_img,axis=0)\n",
    "        batch_y = one_hot_convert(np.reshape(train_gt[idx],(400*400)).astype(int),2)\n",
    "        \n",
    "        _, train_acc, train_loss, train_cross_entropy, train_reg_term = sess.run([train_step, accuracy, loss, cross_entropy, reg_term], feed_dict={tf_data : batch_x, tf_labels : batch_y})\n",
    "        print(train_cross_entropy)\n",
    "        print(train_reg_term)\n",
    "    if epoch % 1 ==0:\n",
    "        train_pred, train_acc, train_loss = sess.run([preds, accuracy, loss], feed_dict={tf_data : train_imgs, tf_labels : one_hot_convert(np.reshape(train_gt,n*400*400).astype(int),2)})\n",
    "        val_pred, val_acc, val_loss = sess.run([preds, accuracy, loss], feed_dict={tf_data : val_imgs, tf_labels : one_hot_convert(np.reshape(val_gt,n*400*400).astype(int),2)})\n",
    "        \n",
    "        print(train_pred.shape)\n",
    "        print(val_pred.shape)\n",
    "        print(train_gt.shape)\n",
    "        f1_train = f1_score(np.reshape(train_gt,n*400*400), train_pred, average='macro') \n",
    "        f1_val = f1_score(np.reshape(val_gt,n*400*400), val_pred, average='macro')\n",
    "        print(\"epoch \", epoch+1,\", val f1 : \", f1_val, \", train f1 : \", f1_train)\n",
    "        print(\"val_loss : \", val_loss, \", train_loss : \", train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 400, 400)\n",
      "(10, 400, 400)\n"
     ]
    }
   ],
   "source": [
    "print(train_gt.shape)\n",
    "print(val_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 1 ..., 0 1 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 1 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[1 0 1 ..., 0 1 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 1 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[1 0 1 ..., 0 1 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 1 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " ..., \n",
      " [[1 0 1 ..., 0 1 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 1 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[1 0 1 ..., 0 1 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 1 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]\n",
      "\n",
      " [[1 0 1 ..., 0 1 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 1 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [1 0 1 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]]\n",
      "[[[ 1.  1.  1. ...,  1.  1.  1.]\n",
      "  [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "  [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "  [ 1.  1.  1. ...,  0.  0.  0.]\n",
      "  [ 1.  1.  1. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  1.  1.  1.]\n",
      "  [ 0.  0.  0. ...,  1.  1.  1.]\n",
      "  [ 0.  0.  0. ...,  1.  1.  1.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.reshape(val_pred, val_gt.shape))\n",
    "print(val_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.7904368360284373"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.mean(np_gt[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x279d1470>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADxdJREFUeJzt3VusXGd5xvH/gzEJKamIC7VsJyiJ6lRKEdkgK0UCVSmI\n2k2rBm4iUhX5IpK5oAjUSq0pUgsXSLTi0JuCZEpUq+XQiIMSobSW46ZCSJAjxnUSSAwkShzHFieR\nCCmQ5O3FrE0G4733t2f27Jk1+/+TRrNmHWa+L571ZM1a315vqgpJWsmLpt0ASf1gWEhqYlhIamJY\nSGpiWEhqYlhIajKxsEiyJ8l3kpxIsn9SnyNpfWQS4yySbAIeAt4CPA7cDdxQVQ+s+YdJWheTOrK4\nGjhRVd+rqp8Dnweum9BnSVoHL57Q++4AHht6/Tjw+0ut/Iotm+rSSzZPqCmahoeOXTDtJugsT/Hj\nH1TVK0fdflJhsaIk+4B9AK/a8WLuOnTJtJoyEbu3L6zJ+xx64uiavM9y7Vmrz2j9PE3H7fWFR8fZ\nflJhcRIY3vsv7ub9UlUdAA4A7LrqfP9AZYrcsdViUmFxN7AzyWUMQuLtwJ8vtfJDxy7wCyvNuImE\nRVU9m+QvgUPAJuCmqrp/Ep8laX1M7JxFVd0G3Dap95e0vhzBOcMmceJRGpVhIamJYSGpiWEhqYlh\nIamJYSGpiWEhqYlhIamJYSGpiWEhqYlhIamJYSGpiWGxQRx64qh/a6KxGBaSmhgWkpoYFjNs9/YF\n7yCmmWFYSGpiWEhqMtZt9ZI8AjwFPAc8W1W7kmwB/hO4FHgEuL6qfjxeMyVN21ocWfxhVS1U1a7u\n9X7gSFXtBI50ryX13CR+hlwHHOymDwJvncBnSFpn44ZFAbcnuberMAawtapOddNPAlvH/AxJM2Dc\nUgBvrKqTSX4bOJzk28MLq6qSnLPa2HD5wvOxLqY068Y6sqiqk93zGeDLDKqnn06yDaB7PrPEtgeq\naldV7drMeeM0Q9I6GDkskvxGkgsXp4E/Ao4DtwJ7u9X2AreM20hJ0zfOz5CtwJeTLL7PZ6vqv5Pc\nDdyc5EbgUeD68ZspadpGDouq+h5w1Tnm/xB48ziNkjR7HMEpqYlhsYHs3r7gPS00MsNCUhPDQlIT\nw0JSE8NCUhPDQlITw0JSE8NixnmpU7PCsJDUxLCQ1MSwkNTEsJDUxLCQ1MSwkNTEsJDUxLDYQByz\noXEYFhuMhZY1KsNCUpMVwyLJTUnOJDk+NG9LksNJHu6eLxpa9r4kJ5J8J8nuSTV8o9i9fcGjAc2E\nliOLfwP2nDXvnPVMk1wJvB34vW6bTyTZtGatlTQ1K4ZFVX0V+NFZs5eqZ3od8Pmqeqaqvg+cYFB4\nSFLPjXrOYql6pjuAx4bWe7yb92uS7EtyT5J7fsEzIzZD0noZ+wRnVRWDAsmr3c7yhVKPjBoWS9Uz\nPQlcMrTexd08ST03algsVc/0VuDtSc5LchmwE7hrvCZKmgUrli9M8jngGuAVSR4H/gH4MOeoZ1pV\n9ye5GXgAeBZ4V1U9N6G2S1pHK4ZFVd2wxKJz1jOtqg8BHxqnUZJmjyM4JTUxLCQ1WfFniDTLZuUv\naTfCkHzDYoNZy51rI+wgrWYltJazadt42/cqLGbhH8Qd5AWz8O+h9dOrsJgF7iDaqGYiLK54zc84\ndMidUJplXg2R1MSwkNTEsJDUZCbOWUizaBaufM3SCXXDQr/CHWS2rO2/x4mxtjYsZshKX4y12omW\n+5xZ2FFnIbD062YiLB46dsFUvyCzsIMs59ATR9f0v89y7+eOqqXMRFhMmzuItDKvhkhqYlhIamJY\nSGoyavnCDyQ5meRo97h2aJnlC6U5NGr5QoCPV9VC97gNLF8ozbNRyxcuxfKFPeDVH41inHMW705y\nrPuZslhF3fKF0pwaNSw+CVwOLACngI+u9g0sX9jOIwHNgpHCoqpOV9VzVfU88Cle+Klh+cIJmPUR\nptoYRgqLxTqnnbcBi1dKLF8ozalRyxdek2SBQfX0R4B3guULpXk2avnCTy+zvuULpTnkCM4e2L19\nwfMWmjrDQlITw0JSE8NCUhPDQlITw0JSE8NCUhPDQlITw2IDcsyGRmFYbFAGhlbLsJDUxLCQ1MSw\nkNTEsJDUxLCQ1MSwkNTEsJDUxLDoid3bF7zLt6aqpXzhJUnuSPJAkvuTvKebvyXJ4SQPd88XDW1j\nCUNpzrQcWTwL/HVVXQm8HnhXV6ZwP3CkqnYCR7rXljCU5lRL+cJTVXVfN/0U8CCDKmPXAQe71Q4C\nb+2mLWEozaFVnbNIcinwWuBOYGtVneoWPQls7aabShhavlDql+awSPIy4IvAe6vqp8PLqqoY1BBp\nZvlCqV+awiLJZgZB8Zmq+lI3+/RiZbLu+Uw33xKG0hxquRoSBkWFHqyqjw0tuhXY203vBW4Zmm8J\nQ2nOrFiRDHgD8A7g/5Is3gTh74APAzcnuRF4FLgeLGHYF47Z0Gq1lC/8GpAlFr95iW0sYSjNGUdw\nSmpiWEhqYlhIamJYSGpiWEhqYlhIamJY9Ii1PjRNhoWkJoaFpCaGhaQmhoWkJoaFpCaGhaQmhsUG\n5WVYrZZhIamJYbGBeXSh1TAsJDVpua2e5pC31dNqjVO+8ANJTiY52j2uHdrG8oUz7tATR/0ZolVp\nObJYLF94X5ILgXuTHO6WfbyqPjK88lnlC7cDtye5wpv2Sv02TvnCpVi+UJpD45QvBHh3kmNJbhqq\not5UvlBSv4xTvvCTwOXAAnAK+OhqPthap1K/jFy+sKpOV9VzVfU88Cle+KnRVL7QWqert3v7glcx\nNDUjly9crHPaeRtwvJu2fKE0h8YpX3hDkgUG1dMfAd4Jli+U5tU45QtvW2YbyxdKc8bh3pKaONxb\nmjGTGlm7advK6yzHsFgnszq0erhdXmnRcmYiLK54zc84dGg2dyZYm51oLXfEtQqe9Q6HWQ1MtZmJ\nsJh18/oln9d+aTI8wSmpiWEhqYlhIamJYSGpiWEhqYlhIamJYSGpiWEhqYlhIamJYSGpiWEhqYlh\nIamJYSGpScsNe89PcleSb3XlCz/Yzd+S5HCSh7vni4a2sXyhNGdajiyeAd5UVVcxqBGyJ8nrgf3A\nkaraCRzpXp9dvnAP8IkkmybReEnrp+WGvQU83b3c3D2KQZnCa7r5B4H/Bf6WofKFwPeTLJYv/Ppa\nNlzqs+nclezEWFs33fymOzK4F/gd4F+q6s4kW6vqVLfKk8DWbnoH8I2hzS1f2FmrL0if7pTlDXbm\nR1NYdHU/FpK8HPhykleftbyS1Go+OMk+YB/Aq3b8ajPm9V6Qs7bjrEd75vXfciNa1W31quonSe5g\ncC7idJJtVXWqq052plutuXwhcABg11Xnrypo+motdpy13MHdkbUaLVdDXtkdUZDkpcBbgG8zKFO4\nt1ttL3BLN73q8oUPHbvgl3U8/QJLs6nlyGIbcLA7b/Ei4Oaq+kqSrwM3J7kReBS4HixfKM2rlqsh\nx4DXnmP+D4E3L7GN5QulOeMITklNDIsN7NATR2fuCo1ml2EhqYlhIamJYSGpiWEhqYlhIamJYSGp\niWEhqYlhIamJYSGpiWEhqYlhIamJYSGpiWEhqYlhIamJYdEz3npQ02JYSGoyTvnCDyQ5meRo97h2\naBvLF0pzpuWGvYvlC59Oshn4WpL/6pZ9vKo+MrzyWeULtwO3J7nCm/ZK/bbikUUNnKt84VJ+Wb6w\nqr7PoGba1WO3VNJUNZ2zSLIpyVEGhYQOV9Wd3aJ3JzmW5KahKuo7gMeGNrd8oTQHmsKiqp6rqgUG\n1cWu7soXfhK4nEFl9VPAR1fzwUn2JbknyT2/4JlVNlvSelvV1ZCq+glwB7Cnqk53IfI88Cle+KnR\nXL6wqnZV1a7NnDda6yWtm5HLF3b1TRe9DTjeTa+6fKGk2TdO+cJ/T7LA4GTnI8A7wfKF0rwap3zh\nO5bZxvKF0pxxBKekJoaFpCaGhaQmhoWkJoaFpCaGhaQmhoWkJoaFpCaGhaQmhoWkJoaFpCaGhaQm\nhoWkJoaFpCaGhaQmhoWkJoaFpCaGxQZn3VS1MiwkNWkOi67Q0DeTfKV7vSXJ4SQPd88XDa1rrVNp\nzqzmyOI9wINDr/cDR6pqJ3Cke312rdM9wCe6O4NL6rHW8oUXA38C/OvQ7OuAg930QeCtQ/OtdSrN\nmZa6IQD/DPwNcOHQvK1VdaqbfhLY2k3vAL4xtN45a50m2Qfs614+fXt94YfADxrb0yevYAL92rRt\n5XXanBh1w4n0a0bMa99+d5yNVwyLJH8KnKmqe5Ncc651qqqSLFdZ/VzbHAAODH3OPVW1azXv0Qf2\nq3/mtW9J7hln+5YjizcAf5bkWuB84DeT/AdwOsm2qjrVlTI8063fVOtUUr+seM6iqt5XVRdX1aUM\nTlz+T1X9BYOapnu71fYCt3TT1jqV5lDrOYtz+TBwc5IbgUeB62GsWqcHVl6ll+xX/8xr38bqV6pW\ndapB0gblCE5JTaYeFkn2dCM9TyTZP+32rFaSm5KcSXJ8aF7vR7cmuSTJHUkeSHJ/kvd083vdtyTn\nJ7krybe6fn2wm9/rfi2a6EjrqpraA9gEfBe4HHgJ8C3gymm2aYQ+/AHwOuD40Lx/AvZ30/uBf+ym\nr+z6eB5wWdf3TdPuwxL92ga8rpu+EHioa3+v+wYEeFk3vRm4E3h93/s11L+/Aj4LfGWtv4vTPrK4\nGjhRVd+rqp8Dn2cwArQ3quqrwI/Omt370a1Vdaqq7uumn2Iw1H8HPe9bDTzdvdzcPYqe9wsmP9J6\n2mGxA3hs6PU5R3v20HKjW3vX3ySXAq9l8H/h3vetO1Q/ymBs0OGqmot+8cJI6+eH5q1Zv6YdFnOv\nBsd8vb3klORlwBeB91bVT4eX9bVvVfVcVS0wGDB4dZJXn7W8d/0aHmm91Drj9mvaYTGvoz1Pd6Na\n6fPo1iSbGQTFZ6rqS93suegbQFX9BLiDwV9H971fiyOtH2Hwc/5NwyOtYfx+TTss7gZ2JrksyUsY\njBC9dcptWgu9H92aJMCngQer6mNDi3rdtySvTPLybvqlwFuAb9PzftV6jLSegbO31zI40/5d4P3T\nbs8I7f8ccAr4BYPffTcCv8XgHh8PA7cDW4bWf3/X1+8Afzzt9i/TrzcyOGQ9BhztHtf2vW/Aa4Bv\ndv06Dvx9N7/X/Tqrj9fwwtWQNeuXIzglNZn2zxBJPWFYSGpiWEhqYlhIamJYSGpiWEhqYlhIamJY\nSGry/+CKdHCz8Xy1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cfdf4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(val_gt[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2761b278>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIoSShRAixP8DrdUwPmP1u0MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2794f7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = np.reshape(val_pred, val_gt.shape)\n",
    "plt.imshow(test[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
